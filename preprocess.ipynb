{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from deep_translator import GoogleTranslator\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "import string\n",
    "import pandas as pd\n",
    "import email\n",
    "from langdetect import detect\n",
    "import warnings\n",
    "from bs4 import MarkupResemblesLocatorWarning\n",
    "warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logger(txt):\n",
    "    log_file = \"log.txt\"\n",
    "    with open(log_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(txt)\n",
    "        f.write(\"\\n\")\n",
    "        f.close()\n",
    "\n",
    "class DataPreProcessor():\n",
    "    def __init__(self):\n",
    "        self.translator = GoogleTranslator(source='auto', target='en')\n",
    "\n",
    "    def html_remover(self, doc):\n",
    "        \"\"\"\n",
    "        Removes html tags in a given text\n",
    "        Input: doc\n",
    "        Output: String\n",
    "        \"\"\"\n",
    "        txt = doc[\"pipe_text\"]\n",
    "        soup=BeautifulSoup(txt,'html.parser')\n",
    "        a=soup.get_text()\n",
    "        doc[\"pipe_text\"] = a\n",
    "        return doc\n",
    "\n",
    "    # char limit 5000 currently 100000\n",
    "\n",
    "    def illegal_char_remover(self, doc):\n",
    "        \"\"\"\n",
    "        Removes some illegal characters that are not in unicode (represented in hex or bytes).\n",
    "        \"\"\"\n",
    "        txt = doc[\"pipe_text\"]\n",
    "        txt_encoded = txt.encode(\"unicode_escape\")\n",
    "        txt_encoded_cleaned = re.sub(b'\\\\\\\\x[a-f0-9][a-f0-9]', b'', txt_encoded)\n",
    "        txt_cleaned = txt_encoded_cleaned.decode(\"unicode_escape\")\n",
    "        doc[\"pipe_text\"] = txt_cleaned\n",
    "        return doc\n",
    "\n",
    "    #Additions by Josiah\n",
    "\n",
    "    def replace_tokens(self, doc):\n",
    "        '''\n",
    "        Replaces links, money, ip, numbers\n",
    "        '''\n",
    "        text = doc[\"pipe_text\"]\n",
    "        # Replace links with token [link]\n",
    "        text = re.sub(\"http\\S+\", \" [link] \", text)\n",
    "        # Replace money with token [MONEY]\n",
    "        text = re.sub(r\"[$]\\d+[.,]*\\d*\", \" [money] \", text)\n",
    "        text = re.sub(r\"\\d+[.,]*\\d*[$]\", \" [money] \", text)\n",
    "        # Replace ip addresses with token [ip]\n",
    "        text = re.sub(r\"^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$\", \" [ip] \", text)\n",
    "        # Replace emails with token [email]\n",
    "        text = re.sub(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b\", \" [email] \", text)\n",
    "        # Remove words with a number in the middle, like s8d or 7a8s8r. Does not remove office365 or 250k\n",
    "        text = re.sub(r\"\\d*(([a-zA-Z]+)(\\d+))+[a-zA-Z]+\\d*\", \" \", text)\n",
    "        # Replace numbers with a special token [NUM]\n",
    "        text = re.sub(r\"[\\d]+\", \" [num] \", text)\n",
    "        # Standardise whitespace\n",
    "        text = re.sub('[\\s]+',\" \", text)\n",
    "        # lowercase text\n",
    "        text = text.lower()\n",
    "        doc[\"pipe_text\"] = text\n",
    "        return doc\n",
    "    \n",
    "    def remove_nonenglish(self, doc):\n",
    "        '''\n",
    "        Removes characters from other languages\n",
    "        '''\n",
    "        text = doc[\"pipe_text\"]\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\]!\"$%&\\'()*+,./:;=#@?[\\\\^_`{|}~\\-[\\u00C0-\\u01DA]+', \" \", text) #Remove characters from other languages, except diacritics\n",
    "        text = unidecode(text) # strip accents\n",
    "        doc[\"pipe_text\"] = text\n",
    "        return doc\n",
    "    \n",
    "    def remove_punctuations(self, doc):\n",
    "        text = doc[\"pipe_text\"]\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        doc[\"pipe_text\"] = text\n",
    "        return doc\n",
    "        \n",
    "    def join_spaces(self, text):\n",
    "        '''\n",
    "        Joins single letters that are separated by a single space, e.g w a n t  m o r e -> want more\n",
    "        '''\n",
    "        split = text.split(\" \")\n",
    "        ans = []\n",
    "        temp = []\n",
    "        for part in split:\n",
    "            if len(part) == 1:\n",
    "                temp.append(part)\n",
    "            else:\n",
    "                if len(temp)>0:\n",
    "                    word = ''.join(temp)\n",
    "                    ans.append(word)\n",
    "                temp = []\n",
    "                if len(part) != 0:\n",
    "                    ans.append(part)\n",
    "        word = ''.join(temp)\n",
    "        ans.append(word)\n",
    "        ans = \" \".join(ans)\n",
    "        return ans\n",
    "\n",
    "    def clean_unicode(self, doc):\n",
    "        '''\n",
    "        Attempts to remove nonsensical characters and convert letterlike symbols back to letters.\n",
    "        '''\n",
    "        text = doc[\"pipe_text\"]\n",
    "        words = []\n",
    "        text = str(text)\n",
    "        text = text.replace(\"\\n\", \"  \")\n",
    "        text = text.replace(chr(160),\" \") # Replace weird space\n",
    "        text = re.sub(r\"[\\u2800-\\u28ff]+\", \"\", text)\n",
    "        text = self.join_spaces(text)\n",
    "        for word in text.split(\" \"):\n",
    "            if len(word) == 0:\n",
    "                continue\n",
    "            if unidecode(word) == \"\":\n",
    "                continue\n",
    "            code = [ord(char) for char in word]\n",
    "            if max(code)>65535: # Outside Basic multilangual plane\n",
    "                word = unidecode(word)\n",
    "            words.append(word)\n",
    "        text = \" \".join(words)\n",
    "        doc[\"pipe_text\"] = text\n",
    "        return doc\n",
    "\n",
    "    def detect_lang(self, doc):\n",
    "        '''\n",
    "        Tries to detect the language of text, reducing number of text that has to be passed through translator.\n",
    "        Note that it is not accurate at detecting the actual language, just weeds out English text.\n",
    "        '''\n",
    "        text = doc[\"pipe_text\"]\n",
    "        text = re.sub(\"[link]\", \"\", text)\n",
    "        text = re.sub(r\"[!'#$%&\\\"()*+,-./:;<=>?@[\\\\\\]^_`{|}~]+\", \" \", text)\n",
    "        text = re.sub(r\"[0-9]+\", \" \", text)\n",
    "        if len(text) == 0:\n",
    "            doc[\"lang\"] = None\n",
    "        try:\n",
    "            lang = detect(text)\n",
    "        except:\n",
    "            text = unidecode(text)\n",
    "            try:\n",
    "                lang = detect(text)\n",
    "            except:\n",
    "                print(text)\n",
    "                doc[\"lang\"] = None\n",
    "                return doc\n",
    "        doc[\"lang\"] = lang\n",
    "        return doc\n",
    "\n",
    "    def translate(self, doc, char_limit=50000):\n",
    "        \"\"\"\n",
    "        Can only deal with one string (not a list)\n",
    "        \"\"\"\n",
    "        text = doc[\"pipe_text\"]\n",
    "        if doc.get(\"lang\") == \"en\":\n",
    "            doc[\"translated\"] = text\n",
    "            doc[\"pipe_text\"] = text\n",
    "            return doc\n",
    "        # Limit the translation to 50000 chars. If larger, dont translate.\n",
    "        if len(text) <= char_limit:\n",
    "\n",
    "            if len(text) <= 1000:\n",
    "                try:\n",
    "                    translated = self.translator.translate(text)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    translated = text\n",
    "                if translated==None:\n",
    "                    translated = \"\"\n",
    "                doc[\"translated\"] = translated\n",
    "                doc[\"pipe_text\"] = doc[\"translated\"]\n",
    "                return doc\n",
    "            \n",
    "            # Split text into chunks that the translation engine can handle. We limit this to 4000 char as of now\n",
    "            words = doc[\"pipe_text\"].split(\" \")\n",
    "            chunks = []\n",
    "            char_count = 0\n",
    "            temp_chunk = []\n",
    "            for word in words:\n",
    "                char_count += (len(word) + 1)\n",
    "                if char_count <= 1000:\n",
    "                    temp_chunk.append(word)\n",
    "                else:\n",
    "                    chunks.append(\" \".join(temp_chunk))\n",
    "                    char_count = len(word)\n",
    "                    temp_chunk = [word]\n",
    "\n",
    "            if len(temp_chunk) > 0:\n",
    "                chunks.append(\" \".join(temp_chunk))\n",
    "\n",
    "            new_pipe_text = []\n",
    "            for chunk in chunks:\n",
    "                if len(chunk) > 1000:\n",
    "                    translated_text = chunk\n",
    "                else:\n",
    "                    try:\n",
    "                        translated_text = self.translator.translate(chunk)\n",
    "                    except Exception as e:\n",
    "                        logger(f\"long_translate failed once \\n {chunk}\")\n",
    "                        try:\n",
    "                            translated_text = self.translator.translate(chunk)\n",
    "                        except Exception as e:\n",
    "                            logger(f\"long_translate failed twice \\n {chunk}\")\n",
    "                            translated_text = chunk\n",
    "                    if not translated_text:\n",
    "                        translated_text = ''\n",
    "                new_pipe_text.append(translated_text)\n",
    "\n",
    "            full_translated = ' '.join(new_pipe_text)\n",
    "        else:\n",
    "            full_translated = text\n",
    "        if full_translated == None:\n",
    "            full_translated = \"\"\n",
    "        doc[\"translated\"] = full_translated\n",
    "        doc[\"pipe_text\"] = full_translated\n",
    "        return doc\n",
    "    \n",
    " \n",
    "    def preprocess(self, text, first_pipe, second_pipe, doc_char_limit=5000):\n",
    "        \"\"\"\n",
    "        The returned doc might have the following fields, depending on the pipes\n",
    "        - original_text\n",
    "        - translated\n",
    "        - sents\n",
    "        - pipes is a list of lists of various pipe components \n",
    "            E.g. [[html_remover, illegal_char_remover, translator], [url_remover, consec_newliine_remover]]\n",
    "        \"\"\"\n",
    "\n",
    "        # Limit the number of characters if not there will be a memory error\n",
    "        doc = {\n",
    "            \"original_text\": text,\n",
    "            \"pipe_text\": text,\n",
    "        }\n",
    "\n",
    "        # Sentencizer has to be the last pipe.\n",
    "        pipe_component_to_func = {\n",
    "            \"html_remover\": self.html_remover,\n",
    "            \"illegal_char_remover\": self.illegal_char_remover,\n",
    "            \"translator\": self.translate,\n",
    "            \"clean_unicode\": self.clean_unicode,\n",
    "            \"detect_lang\": self.detect_lang,\n",
    "        }\n",
    "        for pipe_component in first_pipe:\n",
    "            doc = pipe_component_to_func[pipe_component](doc)\n",
    "        if not doc.get(\"translated\"):\n",
    "            doc[\"translated\"] = doc[\"pipe_text\"]\n",
    "        # Truncate pipe text and then do the remaining operations\n",
    "        doc[\"first_pipe_text\"] = doc[\"pipe_text\"]\n",
    "        doc[\"pipe_text\"] = doc[\"pipe_text\"][:doc_char_limit]\n",
    "        second_pipe_component_to_func = {\n",
    "            \"remove_nonenglish\": self.remove_nonenglish,\n",
    "            \"remove_punctuation\": self.remove_punctuations,\n",
    "            \"replace_tokens\": self.replace_tokens,\n",
    "        }\n",
    "\n",
    "        for pipe_component in second_pipe:\n",
    "            doc = second_pipe_component_to_func[pipe_component](doc)\n",
    "        doc[\"second_pipe_text\"] = doc[\"pipe_text\"]\n",
    "        doc.pop(\"pipe_text\")\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_emails_raw = pd.read_pickle(\"Data/ch_emails_raw.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = ch_emails_raw.iloc[:100].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Josia\\AppData\\Local\\Temp\\ipykernel_26328\\539062817.py:12: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup=BeautifulSoup(txt,'html.parser')\n",
      "C:\\Users\\Josia\\AppData\\Local\\Temp\\ipykernel_26328\\539062817.py:12: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup=BeautifulSoup(txt,'html.parser')\n",
      "C:\\Users\\Josia\\AppData\\Local\\Temp\\ipykernel_26328\\3283322295.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp[\"doc\"] = temp.message.apply(lambda x: data_preprocessor.preprocess(x, first_preprocess_pipe, second_preprocess_pipe))\n"
     ]
    }
   ],
   "source": [
    "data_preprocessor = DataPreProcessor()\n",
    "first_preprocess_pipe = [\"html_remover\", \"clean_unicode\", \"detect_lang\", \"translator\"]\n",
    "second_preprocess_pipe = [\"remove_nonenglish\", \"replace_tokens\", \"remove_punctuation\"]\n",
    "temp[\"doc\"] = temp.message.apply(lambda x: data_preprocessor.preprocess(x, first_preprocess_pipe, second_preprocess_pipe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_message = temp.iloc[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original_text': '讲的是孔子后人的故事。一个老领导回到家乡，跟儿子感情不和，跟贪财的孙子孔为本和睦。\\n老领导的弟弟魏宗万是赶马车的。\\n有个洋妞大概是考察民俗的，在他们家过年。\\n孔为本总想出国，被爷爷教育了。\\n最后，一家人基本和解。\\n顺便问另一类电影，北京青年电影制片厂的。中越战背景。一军人被介绍了一个对象，去相亲。女方是军队医院的护士，犹豫不决，总是在回忆战场上负伤的男友，好像还没死。最后\\n男方表示理解，归队了。\\n',\n",
       " 'lang': 'zh-cn',\n",
       " 'translated': \"It tells the story of the descendants of Confucius. An old leader returned to his hometown and was at odds with his son and at peace with his money-grubbing grandson Kong Weiben. Wei Zongwan, the old leader's younger brother, drives the carriage. There was a foreign girl who was probably investigating folk customs and was celebrating the New Year at their house. Kong Weiben always wanted to go abroad and was educated by his grandfather. In the end, the family basically reconciled. By the way, I want to ask about another type of film, produced by Beijing Youth Film Studio. Background of the Sino-Vietnam War. A soldier was introduced to a date and went on a blind date. The woman is a nurse in a military hospital. She is hesitant and always thinking about her boyfriend who was injured on the battlefield and seems to be still alive. Finally, the man expressed his understanding and returned to the team.\",\n",
       " 'first_pipe_text': \"It tells the story of the descendants of Confucius. An old leader returned to his hometown and was at odds with his son and at peace with his money-grubbing grandson Kong Weiben. Wei Zongwan, the old leader's younger brother, drives the carriage. There was a foreign girl who was probably investigating folk customs and was celebrating the New Year at their house. Kong Weiben always wanted to go abroad and was educated by his grandfather. In the end, the family basically reconciled. By the way, I want to ask about another type of film, produced by Beijing Youth Film Studio. Background of the Sino-Vietnam War. A soldier was introduced to a date and went on a blind date. The woman is a nurse in a military hospital. She is hesitant and always thinking about her boyfriend who was injured on the battlefield and seems to be still alive. Finally, the man expressed his understanding and returned to the team.\",\n",
       " 'second_pipe_text': 'it tells the story of the descendants of confucius an old leader returned to his hometown and was at odds with his son and at peace with his moneygrubbing grandson kong weiben wei zongwan the old leaders younger brother drives the carriage there was a foreign girl who was probably investigating folk customs and was celebrating the new year at their house kong weiben always wanted to go abroad and was educated by his grandfather in the end the family basically reconciled by the way i want to ask about another type of film produced by beijing youth film studio background of the sinovietnam war a soldier was introduced to a date and went on a blind date the woman is a nurse in a military hospital she is hesitant and always thinking about her boyfriend who was injured on the battlefield and seems to be still alive finally the man expressed his understanding and returned to the team'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_preprocessor.preprocess(temp_message, first_preprocess_pipe, second_preprocess_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50445"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ch_emails_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = pd.read_pickle(\"Data/sample.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "data_preprocessor = DataPreProcessor()\n",
    "first_preprocess_pipe = [\"html_remover\", \"clean_unicode\", \"detect_lang\", \"translator\"]\n",
    "second_preprocess_pipe = [\"remove_nonenglish\", \"replace_tokens\", \"remove_punctuation\"]\n",
    "sample_df[\"doc\"] = sample_df.message.apply(lambda x: data_preprocessor.preprocess(x, first_preprocess_pipe, second_preprocess_pipe))\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken to process 2000 records is 2566.5277490615845 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"time taken to process {len(sample_df)} records is {end-start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>doc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full_subpath</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>trec06c/data/205/284</th>\n",
       "      <td>rerererererererererererererererererererererere...</td>\n",
       "      <td>{'original_text': 'rererererererererererererer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trec06c/data/116/017</th>\n",
       "      <td>己阅\\n     标  题: Re: 哇,女生版怎么混的只剩女生了?\\n     \\n   ...</td>\n",
       "      <td>{'original_text': '己阅\n",
       "     标  题: Re: 哇,女生版怎么混的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trec06p/data/053/084</th>\n",
       "      <td>@\\nFDB.dJ.FNBæ.K...</td>\n",
       "      <td>{'original_text': '@\n",
       "FDB.dJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trec06p/data/084/129</th>\n",
       "      <td>\\nHello Taro Sato,\\n\\n     I am a student in o...</td>\n",
       "      <td>{'original_text': '\n",
       "Hello Taro Sato,\n",
       "\n",
       "     I a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trec06p/data/057/268</th>\n",
       "      <td>I'm inclined to agree.  These things are not e...</td>\n",
       "      <td>{'original_text': 'I'm inclined to agree.  The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trec06p/data/025/000</th>\n",
       "      <td>\\nWorld Top10 Branded Watches at 90% off \\nthe...</td>\n",
       "      <td>{'original_text': '\n",
       "World Top10 Branded Watche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trec06c/data/106/061</th>\n",
       "      <td>广州市中化贸易有限公司 \\n尊...</td>\n",
       "      <td>{'original_text': '                           ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trec06p/data/116/076</th>\n",
       "      <td>To complete the inventory process, we have to ...</td>\n",
       "      <td>{'original_text': 'To complete the inventory p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trec06c/data/011/162</th>\n",
       "      <td>yei8xsg=?=\\nX-Priority: 3\\nX-Originating-IP: [...</td>\n",
       "      <td>{'original_text': 'yei8xsg=?=\n",
       "X-Priority: 3\n",
       "X-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trec06c/data/135/163</th>\n",
       "      <td>有些情况和我很类似\\n我bf就是觉得找了gf就是放在家里\\n别的不需要了\\n我觉得两个人在一...</td>\n",
       "      <td>{'original_text': '有些情况和我很类似\n",
       "我bf就是觉得找了gf就是放在家里...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                message  \\\n",
       "full_subpath                                                              \n",
       "trec06c/data/205/284  rerererererererererererererererererererererere...   \n",
       "trec06c/data/116/017  己阅\\n     标  题: Re: 哇,女生版怎么混的只剩女生了?\\n     \\n   ...   \n",
       "trec06p/data/053/084  @\\nFDB.dJ.FNBæ.K...   \n",
       "trec06p/data/084/129  \\nHello Taro Sato,\\n\\n     I am a student in o...   \n",
       "trec06p/data/057/268  I'm inclined to agree.  These things are not e...   \n",
       "...                                                                 ...   \n",
       "trec06p/data/025/000  \\nWorld Top10 Branded Watches at 90% off \\nthe...   \n",
       "trec06c/data/106/061                                 广州市中化贸易有限公司 \\n尊...   \n",
       "trec06p/data/116/076  To complete the inventory process, we have to ...   \n",
       "trec06c/data/011/162  yei8xsg=?=\\nX-Priority: 3\\nX-Originating-IP: [...   \n",
       "trec06c/data/135/163  有些情况和我很类似\\n我bf就是觉得找了gf就是放在家里\\n别的不需要了\\n我觉得两个人在一...   \n",
       "\n",
       "                                                                    doc  \n",
       "full_subpath                                                             \n",
       "trec06c/data/205/284  {'original_text': 'rererererererererererererer...  \n",
       "trec06c/data/116/017  {'original_text': '己阅\n",
       "     标  题: Re: 哇,女生版怎么混的...  \n",
       "trec06p/data/053/084  {'original_text': '@\n",
       "FDB.dJ...  \n",
       "trec06p/data/084/129  {'original_text': '\n",
       "Hello Taro Sato,\n",
       "\n",
       "     I a...  \n",
       "trec06p/data/057/268  {'original_text': 'I'm inclined to agree.  The...  \n",
       "...                                                                 ...  \n",
       "trec06p/data/025/000  {'original_text': '\n",
       "World Top10 Branded Watche...  \n",
       "trec06c/data/106/061  {'original_text': '                           ...  \n",
       "trec06p/data/116/076  {'original_text': 'To complete the inventory p...  \n",
       "trec06c/data/011/162  {'original_text': 'yei8xsg=?=\n",
       "X-Priority: 3\n",
       "X-...  \n",
       "trec06c/data/135/163  {'original_text': '有些情况和我很类似\n",
       "我bf就是觉得找了gf就是放在家里...  \n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
