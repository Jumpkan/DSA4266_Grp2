{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3080\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name())\n",
    "    print(torch.cuda.current_device())\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizerFast, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "import random\n",
    "import heapq\n",
    "path = '/app/Data/'\n",
    "model_path = 'google-bert/bert-base-uncased'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrow_file_name = \"data-00000-of-00001\"\n",
    "def load_data(path,tokenized=False):\n",
    "    if tokenized:\n",
    "        full_ds = load_dataset('arrow',data_files={\n",
    "            'train':path+f'/tokenized/train_ds/{arrow_file_name}.arrow',\n",
    "            'test':path+f'/tokenized/test_ds/{arrow_file_name}.arrow',\n",
    "            'val':path+f'/tokenized/val_ds/{arrow_file_name}.arrow'\n",
    "        })\n",
    "    else:\n",
    "        full_ds = load_dataset('arrow',data_files={\n",
    "            'train':path+f'/train_ds/{arrow_file_name}.arrow',\n",
    "            'test':path+f'/test_ds/{arrow_file_name}.arrow',\n",
    "            'validation':path+f'/val_ds/{arrow_file_name}.arrow'\n",
    "        })\n",
    "    return full_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Val-Test split, run only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(id,example):\n",
    "    text = example['processed']\n",
    "    label = example['class']\n",
    "    tokens = tokenizer.encode(text)\n",
    "    out = []\n",
    "    if len(tokens) <= 512:\n",
    "        out.append([tokens,label,id])\n",
    "        return out\n",
    "    else:\n",
    "        cls_token = tokens[0]\n",
    "        sep_token = tokens[-1]\n",
    "        tokens = tokens[1:-1] # remove CLS and SEP tokens\n",
    "        chunks = [tokens[i:i+500] for i in range(0,len(tokens),500)]\n",
    "        for c in chunks: # add back CLS and SEP tokens\n",
    "            c.insert(0,cls_token)\n",
    "            c.append(sep_token)\n",
    "            out.append([c,label,id])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten(text):\n",
    "    words = text.split(\" \")\n",
    "    curr = None\n",
    "    final = []\n",
    "    for word in words:\n",
    "        if word != curr:\n",
    "            final.append(word)\n",
    "        curr = word\n",
    "    return \" \".join(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function_2(id,example):\n",
    "    text = example['processed']\n",
    "    text = shorten(text)\n",
    "    label = example['class']\n",
    "    tokens = tokenizer.encode(text)\n",
    "    cls_token = tokens[0]\n",
    "    sep_token = tokens[-1]\n",
    "    first_chunk = tokens[1: 511]\n",
    "    c = []\n",
    "    c.append(cls_token)\n",
    "    c.extend(first_chunk)\n",
    "    c.append(sep_token)\n",
    "    return([c, label, id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    new_df = {'text':[],'label':[],'raw_text':[]}\n",
    "    ids = []\n",
    "    mapping = {\"ham\":0,\"spam\":1}\n",
    "    for i,row in tqdm(df.iterrows()):\n",
    "        new_df[\"raw_text\"].append(row['processed'])\n",
    "        tokens, label, idx = preprocess_function_2(i,row)\n",
    "        new_df['text'].append(tokens)\n",
    "        new_df['label'].append(mapping[label])\n",
    "        ids.append(idx)\n",
    "    final_df = pd.DataFrame(new_df,index=ids)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (574 > 512). Running this sequence through the model will result in indexing errors\n",
      "26078it [00:35, 744.49it/s]\n",
      "6520it [00:09, 680.85it/s]\n",
      "8150it [00:11, 694.33it/s]\n"
     ]
    }
   ],
   "source": [
    "seed = random.seed(37)\n",
    "df = pd.read_pickle(\"/app/Data/full_df.pkl\")\n",
    "df = df.drop_duplicates(subset=['processed'])\n",
    "X = df['processed']\n",
    "y = df['class']\n",
    "X_tr, X_test, y_tr, y_test = train_test_split(X,y,test_size=0.2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tr,y_tr,test_size=0.2)\n",
    "train_df = pd.DataFrame({'processed':X_train,'class':y_train})\n",
    "val_df = pd.DataFrame({'processed':X_val,'class':y_val})\n",
    "test_df = pd.DataFrame({'processed':X_test,'class':y_test})\n",
    "train_df = preprocess(train_df)\n",
    "val_df = preprocess(val_df)\n",
    "test_df = preprocess(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(df.processed)\n",
    "ids = df.index\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_df = pd.DataFrame({\"embedding\": [embeddings[i] for i in range(len(ids))]}, index=list(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_df.to_pickle(\"/app/Data/embedding.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_df = pd.read_pickle(\"/app/Data/embedding.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "if torch.cuda.is_available():\n",
    "    jax_device = jax.devices(\"gpu\")[0]\n",
    "else:\n",
    "    jax_device = jax.devices(\"cpu\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(x,y):\n",
    "    '''\n",
    "    Computes cosine similarity between b and each row of a\n",
    "    x: 2d np vector\n",
    "    y: 1d np vector\n",
    "    '''\n",
    "    dot_product = jnp.dot(x, y)\n",
    "    \n",
    "    # Compute the magnitudes of x and y\n",
    "    x_norm = jnp.linalg.norm(x)\n",
    "    y_norm = jnp.linalg.norm(y)\n",
    "    \n",
    "    # Compute the cosine similarity\n",
    "    similarity = dot_product / (x_norm * y_norm)\n",
    "    \n",
    "    return similarity\n",
    "cosine_sim_jit = jax.jit(device=jax_device, fun=cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post(object):\n",
    "    def __init__(self, embedding, pos):\n",
    "        self.embedding = embedding\n",
    "        self.closest_dst = -1 # Cosine similarity is a value from -1 to 1, with similar posts having value close to 1\n",
    "        self.pos = pos # Position in self.posts\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.closest_dst < other.closest_dst\n",
    "\n",
    "class Undersample:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.post_ids = list(df.index)\n",
    "        self.distances = jnp.array([-1] * len(self.post_ids))\n",
    "        self.embeddings = np.row_stack(list(df.embedding))\n",
    "        self.embeddings = jax.device_put(jnp.array(self.embeddings), device=jax_device)\n",
    "        self.selected_ids = []\n",
    "        self.recent = None\n",
    "    \n",
    "    def select_furthest(self):\n",
    "        if len(self.selected_ids) == 0:\n",
    "            # Pick random starting point\n",
    "            recent_pos = random.randint(0, len(self.post_ids)-1)\n",
    "            self.selected_ids.append(self.post_ids[recent_pos])\n",
    "            self.recent = self.embeddings[recent_pos]\n",
    "            return \n",
    "        dist_to_recent = cosine_sim_jit(self.embeddings, self.recent)\n",
    "        self.distances = jnp.maximum(self.distances, dist_to_recent)\n",
    "        recent_pos = jnp.argmin(self.distances)\n",
    "        self.selected_ids.append(self.post_ids[recent_pos])\n",
    "        self.recent = self.embeddings[recent_pos]\n",
    "        return\n",
    "\n",
    "    def select_n(self, n):\n",
    "        if n>len(self.post_ids):\n",
    "            return self.df\n",
    "        for i in tqdm(range(n)):\n",
    "            self.select_furthest()\n",
    "        return self.df[self.df.index.isin(self.selected_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>raw_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19745</td>\n",
       "      <td>19745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6333</td>\n",
       "      <td>6333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text  raw_text\n",
       "label                 \n",
       "0      19745     19745\n",
       "1       6333      6333"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_spam = train_df[train_df.label==1].copy()\n",
    "train_ham = train_df[train_df.label==0].copy()\n",
    "train_ham_embed = pd.merge(train_ham, embedding_df, left_index=True, right_index=True)\n",
    "train_df.groupby([\"label\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6333/6333 [00:18<00:00, 340.94it/s]\n"
     ]
    }
   ],
   "source": [
    "undersampler = Undersample(train_ham_embed)\n",
    "train_ham_sampled = undersampler.select_n(len(train_spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new = pd.concat([train_spam[[\"text\", \"label\"]], train_ham_sampled[[\"text\", \"label\"]]])\n",
    "val_df = val_df[[\"text\", \"label\"]]\n",
    "test_df = test_df[[\"text\", \"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 12666/12666 [00:00<00:00, 333143.46 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8150/8150 [00:00<00:00, 295279.12 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 6520/6520 [00:00<00:00, 263457.25 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds = Dataset.from_pandas(train_new,split='train')\n",
    "val_ds = Dataset.from_pandas(val_df,split='validation')\n",
    "test_ds = Dataset.from_pandas(test_df,split='test')\n",
    "train_ds.save_to_disk(path+'/train_ds')\n",
    "test_ds.save_to_disk(path+'/test_ds')\n",
    "val_ds.save_to_disk(path+'/val_ds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 12666 examples [00:00, 339743.77 examples/s]\n",
      "Generating test split: 8150 examples [00:00, 378082.55 examples/s]\n",
      "Generating validation split: 6520 examples [00:00, 334792.58 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds = load_data(path)\n",
    "train_ds = ds['train']\n",
    "test_ds = ds['test']\n",
    "val_ds = ds['validation']\n",
    "train_ds = train_ds.remove_columns(['__index_level_0__'])\n",
    "test_ds = test_ds.remove_columns(['__index_level_0__'])\n",
    "val_ds = val_ds.remove_columns(['__index_level_0__'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding of tokens and getting attention maps via BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_and_tokenize_function(example):\n",
    "    tokens = example['text']\n",
    "    text = tokenizer.decode(tokens,skip_special_tokens=True)\n",
    "    return tokenizer(text,padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 12666/12666 [00:28<00:00, 440.51 examples/s]\n",
      "Map: 100%|██████████| 8150/8150 [00:19<00:00, 417.80 examples/s]\n",
      "Map: 100%|██████████| 6520/6520 [00:15<00:00, 413.05 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_ds = train_ds.map(decode_and_tokenize_function)\n",
    "tokenized_test_ds = test_ds.map(decode_and_tokenize_function)\n",
    "tokenized_val_ds = val_ds.map(decode_and_tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 12666/12666 [00:00<00:00, 100646.51 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 6520/6520 [00:00<00:00, 172687.94 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8150/8150 [00:00<00:00, 151182.52 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_ds.save_to_disk(path+'/tokenized/train_ds')\n",
    "tokenized_val_ds.save_to_disk(path+'/tokenized/val_ds')\n",
    "tokenized_test_ds.save_to_disk(path+'/tokenized/test_ds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
