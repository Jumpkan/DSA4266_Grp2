{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "# import wordninja\n",
    "import requests\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Torch cannot work properly in jupyter notebook\n",
    "# import os\n",
    "# count = 0 \n",
    "# if count == 0:\n",
    "#     os.chdir(\"test_dir\")\n",
    "#     count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/DSA4266_Grp2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_pickle(DF_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "## CONFIG\n",
    "\n",
    "DF_PATH = \"Data/full_df_2.pkl\"\n",
    "X_NAME = 'clean_msg'\n",
    "Y_NAME = 'class'\n",
    "EMBEDDINGS_FOLDER = 'embeddings_2'\n",
    "\n",
    "#### For preprocessing\n",
    "MAXLEN_PER_SENT = 150\n",
    "ALL_TOKEN_MAX_WORDS = 5000\n",
    "INPUT_LENGTH = 150\n",
    "UNDERSAMPLE = True\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 125\n",
    "NUM_CLASSES = 2\n",
    "HIDDEN_SIZE = 75\n",
    "LEARNING_RATE = 0.001\n",
    "VERBOSE = True\n",
    "NUM_LAYERS = 3\n",
    "DROPOUT = 0.2\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(DEVICE)\n",
    "torch.manual_seed(13)\n",
    "torch.set_default_device(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Semantic Dictionaries\n",
    "\n",
    "def get_synonyms_conceptnet(word):\n",
    "    synonyms = []\n",
    "    url = f'http://api.conceptnet.io/c/en/{word}?filter=/c/en'\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    for edge in data['edges']:\n",
    "        if edge['rel']['label'] == 'Synonym' and edge['start']['language'] == 'en' and edge['end']['language'] == 'en':\n",
    "            start = edge['start']['label']\n",
    "            end = edge['end']['label']\n",
    "            synonyms.append(end if start == word else start)\n",
    "\n",
    "    if synonyms != []:\n",
    "        synonym = random.choice(synonyms)\n",
    "    else:\n",
    "        synonym = synonyms\n",
    "    return synonym\n",
    "\n",
    "def get_synonyms_wordnet(word):\n",
    "    synonyms = []\n",
    "    synsets = wordnet.synsets(word)\n",
    "    for synset in synsets:\n",
    "        synonyms.extend([lemma.name() for lemma in synset.lemmas() if lemma.name() != word])\n",
    "\n",
    "    if synonyms != []:\n",
    "        synonym = random.choice(synonyms)\n",
    "    else:\n",
    "        synonym = synonyms\n",
    "    return synonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPrep():\n",
    "    def __init__(self, subset = None, text_prep = 'lem', token_max_words = ALL_TOKEN_MAX_WORDS, maxlen_per_sent = MAXLEN_PER_SENT, undersample = UNDERSAMPLE):\n",
    "        \"\"\"\n",
    "        subset: X[:subset]\n",
    "        \"\"\"\n",
    "        self.df = pd.read_pickle(DF_PATH)\n",
    "        self.subset = subset\n",
    "        self.maxlen_per_sent = maxlen_per_sent\n",
    "\n",
    "\n",
    "        self.remove_duplicates()\n",
    "        print('Dupes removed')\n",
    "        self.X = self.df[X_NAME]\n",
    "        self.y = self.df[Y_NAME].apply(lambda x: 1 if x == 'spam' else 0)\n",
    "        self.token_max_words = token_max_words\n",
    "\n",
    "        if self.subset:\n",
    "            self.X = self.X[:self.subset]\n",
    "            self.y = self.y[:self.subset]\n",
    "        \n",
    "        print('Tokenizing..')\n",
    "        self.tokenize()\n",
    "        print('Finished Tokenizing')\n",
    "\n",
    "        print('Initialising word2vec')\n",
    "        self.word_to_vec_map = self.word2vec()\n",
    "\n",
    "        print('lemm/stemm')\n",
    "        if text_prep == 'lem':\n",
    "            self.X = self.lemming()\n",
    "        if text_prep == 'stem':\n",
    "            self.X = self.stemming()\n",
    "\n",
    "        print('Embedding...')\n",
    "        self.pre_embed()\n",
    "        path = f'{EMBEDDINGS_FOLDER}/emb_matrix_x{self.subset}_tok_{self.maxlen_per_sent}_len{self.token_max_words}.pkl'\n",
    "        if os.path.exists(path):\n",
    "            self.emb_matrix = pd.read_pickle(path)\n",
    "        else:\n",
    "            self.emb_matrix = self.tok_embedding_mat(alternative = [get_synonyms_conceptnet, get_synonyms_wordnet])\n",
    "            print('Finished embedding')\n",
    "\n",
    "        print('Padding')\n",
    "        X_pad = self.pad()\n",
    "        print('Finished padding')\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X_pad, self.y, test_size=0.20, random_state=42)\n",
    "        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(self.X_train, self.y_train, test_size = 0.2, random_state=42 )\n",
    "\n",
    "        if undersample:\n",
    "            print('Undersampling..')\n",
    "            print(Counter(self.y_train))\n",
    "            self.X_train, self.y_train = self.undersample()\n",
    "            print(Counter(self.y_train))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def remove_duplicates(self):\n",
    "    \n",
    "        ## First remove all those X values with differing binary y values\n",
    "        occurrences = self.df.groupby([X_NAME, Y_NAME]).size().reset_index(name='count')\n",
    "        duplicates = occurrences[occurrences.duplicated(subset=X_NAME, keep=False)]\n",
    "        for index, row in duplicates.iterrows():\n",
    "            x_value = row[X_NAME]\n",
    "            max_count = occurrences[(occurrences[X_NAME] == x_value)].max()['count']\n",
    "            occurrences.drop(occurrences[(occurrences[X_NAME] == x_value) & (occurrences['count'] != max_count)].index, inplace=True)\n",
    "\n",
    "        ## Remove duplicates\n",
    "        self.df = occurrences.drop_duplicates(subset = X_NAME).reset_index(drop = True)\n",
    "    \n",
    "    def tokenize(self, join = False):\n",
    "        def tokenize_helper(text, join = False):\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            tokens = word_tokenize(text)\n",
    "            tokens = [word.lower() for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "            if join:\n",
    "                tokens = ' '.join([''.join(c for c in word if c not in string.punctuation) for word in tokens if word])\n",
    "        \n",
    "            return tokens\n",
    "        \n",
    "        self.X = self.X.apply(lambda x: tokenize_helper(x, join))\n",
    "\n",
    "    ## Embedders\n",
    "        \n",
    "    def word2vec(self):\n",
    "        from gensim.models.word2vec import Word2Vec\n",
    "        import gensim.downloader as api\n",
    "\n",
    "        word_to_vec_map = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "        return word_to_vec_map\n",
    "    \n",
    "    \n",
    "    ## Stemming/ Lemmetization\n",
    "\n",
    "    def stemming(self):\n",
    "        ps = PorterStemmer()\n",
    "\n",
    "        def stem(row):\n",
    "            print(row)\n",
    "            stemmed = []\n",
    "            for word in row:\n",
    "                stemmed += [ps.stem(word)]\n",
    "            print('STEMMED:', stemmed)\n",
    "\n",
    "            return stemmed\n",
    "\n",
    "        return self.X.apply(stem)\n",
    "    \n",
    "\n",
    "    def lemming(self):\n",
    "\n",
    "        def lem(row):\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            lemmed = [lemmatizer.lemmatize(word) for word in row]\n",
    "            # print(row)\n",
    "            # print(lemmed,\"\\n\")\n",
    "            return lemmed\n",
    "\n",
    "        return self.X.apply(lem)\n",
    "    \n",
    "    def pre_embed(self):\n",
    "        self.tokenizer = text.Tokenizer(num_words=self.token_max_words)\n",
    "        self.tokenizer.fit_on_texts(self.X)\n",
    "\n",
    "        self.sequences = self.tokenizer.texts_to_sequences(self.X)\n",
    "\n",
    "        self.word_index = self.tokenizer.word_index\n",
    "        self.vocab_len = len(self.word_index) + 1\n",
    "        self.embed_vector_len = self.word_to_vec_map['moon'].shape[0]\n",
    "    \n",
    "    def tok_embedding_mat(self, alternative):\n",
    "        \"\"\"\n",
    "        embedder: word2vec\n",
    "        alternative: list of callable to find synonyms from, inorder of precedence\n",
    "        \"\"\"\n",
    "        synonyms = {} #Dict to store synonyms\n",
    "\n",
    "        emb_matrix = np.zeros((self.vocab_len, self.embed_vector_len))\n",
    "\n",
    "\n",
    "        for word, index in tqdm.tqdm(self.word_index.items(), total = len(self.word_index)):\n",
    "            try: # Try to find in word2vec\n",
    "                embedding_vector = self.word_to_vec_map[word]\n",
    "                emb_matrix[index-1, :] = embedding_vector\n",
    "            except: # Word2vec dont have, find in own synonym dict\n",
    "                synonym = synonyms.get(word, None) \n",
    "                if (synonym) and (synonym in self.word_to_vec_map.index_to_key):\n",
    "                    emb_matrix[index-1,:] = self.word_to_vec_map[synonym]\n",
    "                else: # If word2vec, own synonym dict dont have, find from dictionaries\n",
    "                    for dictionary in alternative:\n",
    "                        try: \n",
    "                            synonym = dictionary(word)\n",
    "                            if synonym:\n",
    "                                # print(f'Found synonym: {synonym} for word: {word}')\n",
    "                                embedding_vector = self.word_to_vec_map[synonym] \n",
    "                                emb_matrix[index-1, :] = embedding_vector\n",
    "                                synonyms[word] = synonym\n",
    "                        except:\n",
    "                            continue\n",
    "        self.syn = synonyms\n",
    "        \n",
    "        try:\n",
    "            pd.to_pickle(emb_matrix, f\"{EMBEDDINGS_FOLDER}/emb_matrix_x{self.subset}_tok_{self.maxlen_per_sent}_len{self.token_max_words}.pkl\")\n",
    "        except:\n",
    "            print('Saved unsuccessfully')\n",
    "            return emb_matrix\n",
    "\n",
    "        return emb_matrix\n",
    "\n",
    "\n",
    "    def pad(self):\n",
    "        X_pad = pad_sequences(self.sequences, maxlen = self.maxlen_per_sent)\n",
    "        return X_pad\n",
    "\n",
    "    def undersample(self):\n",
    "        undersampler = RandomUnderSampler(random_state=42)\n",
    "        X_resampled, y_resampled = undersampler.fit_resample(self.X_train, self.y_train)\n",
    "\n",
    "        return X_resampled, y_resampled\n",
    "\n",
    "\n",
    "class Train(DataPrep):\n",
    "    def __init__(self, subset = None, text_prep = 'lem', token_max_words = 5000, maxlen_per_sent = 150, undersample = True):\n",
    "        super().__init__(subset, text_prep, token_max_words, maxlen_per_sent, undersample)\n",
    "        \n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        self.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "        self.X_train_tensor = torch.as_tensor(self.X_train, dtype = torch.float)\n",
    "        self.y_train_tensor = torch.as_tensor(self.y_train, dtype = torch.int8)\n",
    "\n",
    "\n",
    "    def lstm(self, nodes):\n",
    "\n",
    "        self.model = Sequential().to(device = self.device)\n",
    "        self.model.add(Embedding(input_dim= self.vocab_len, output_dim= self.embed_vector_len, input_shape = (self.maxlen_per_sent,), trainable=False, embeddings_initializer = initializers.Constant(self.emb_matrix)))\n",
    "        self.model.add(LSTM(nodes))\n",
    "        self.model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "        self.model.compile(optimizer='adam',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "        # Train model\n",
    "        self.model.fit(self.X_train, self.y_train, epochs=10, batch_size=1, verbose=1)  \n",
    "    \n",
    "    # def lstm_op(self):\n",
    "    #     import math\n",
    "\n",
    "    #     def objective(trial):\n",
    "    #         units = trial.suggest_categorical(\"units\", [32, 64, 128, 256])\n",
    "    #         units2 = units//2\n",
    "    #         epochs = trial.suggest_categorical(\"epochs\", [10, 20, 30])\n",
    "    #         batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "    #         dropout = trial.suggest_float(\"dropout\", low = 0.1, high = 0.5)\n",
    "            \n",
    "    #         self.model = Sequential()\n",
    "    #         self.model.add(Embedding(input_dim= self.vocab_len, output_dim= self.embed_vector_len, input_shape = (self.maxlen_per_sent,), trainable=False, embeddings_initializer = initializers.Constant(self.emb_matrix)))\n",
    "    #         self.model.add(LSTM(units))\n",
    "    #         self.model.add(Dropout(dropout))\n",
    "    #         self.model.add(Dense(units2))\n",
    "    #         self.model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "    #         self.model.compile(optimizer='adam',\n",
    "    #                         loss='binary_crossentropy',\n",
    "    #                         metrics=['accuracy'])\n",
    "\n",
    "    #         self.model.fit(self.X_train, self.y_train, epochs= epochs, batch_size= batch_size, verbose=1)  \n",
    "    #         _, accuracy = self.model.evaluate(self.X_test, self.y_test, verbose=0)\n",
    "\n",
    "    #         return accuracy\n",
    "\n",
    "    #     study = optuna.create_study(direction=\"maximize\")\n",
    "    #     study.optimize(objective, n_trials=10)\n",
    "\n",
    "    #     self.best_trial = study.best_trial\n",
    "    #     self.best_params = self.best_trial.params\n",
    "    #     self.best_accuracy = self.best_trial.value\n",
    "\n",
    "    #     print(\"Best hyperparameters:\", self.best_params)\n",
    "    #     print(\"Best accuracy:\", self.best_accuracy)\n",
    "\n",
    "\n",
    "    def predict(self, verbose = False):\n",
    "\n",
    "        loss, accuracy = self.model.evaluate(self.X_test, self.y_test)\n",
    "        print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = self.model.predict(self.X_test)\n",
    "\n",
    "        y_hat = [1 if i> 0.5 else 0 for i in predictions]\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Classification Report:\")\n",
    "            print(classification_report(self.y_test, y_hat))\n",
    "\n",
    "            print(\"Confusion Matrix:\")\n",
    "            print(confusion_matrix(self.y_test, y_hat))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dupes removed\n",
      "Tokenizing..\n",
      "Finished Tokenizing\n",
      "Initialising word2vec\n",
      "lemm/stemm\n",
      "Embedding...\n",
      "Padding\n",
      "Finished padding\n",
      "Undersampling..\n",
      "Counter({0: 19480, 1: 5672})\n",
      "Counter({0: 5672, 1: 5672})\n"
     ]
    }
   ],
   "source": [
    "test = Train()\n",
    "## the test.X_train is not embedded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., 1501, 1126,   25],\n",
       "       [   0,    0,    0, ...,  205, 2790, 4265],\n",
       "       [   0,    0,    0, ...,  274,  581, 4183],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  867,   73,   28],\n",
       "       [   0,    0,    0, ..., 1565, 1977, 2873],\n",
       "       [   0,    0,    0, ...,   24,   73,   39]], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor shape: torch.Size([11344, 150])\n",
      "Embedded output shape: torch.Size([11344, 150, 150])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0607,  0.3815,  0.3934,  ..., -0.2935,  0.4256, -0.1617],\n",
       "         [ 1.0607,  0.3815,  0.3934,  ..., -0.2935,  0.4256, -0.1617],\n",
       "         [ 1.0607,  0.3815,  0.3934,  ..., -0.2935,  0.4256, -0.1617],\n",
       "         ...,\n",
       "         [ 0.3236, -0.4857, -0.6464,  ...,  0.8669,  1.8450, -0.7716],\n",
       "         [ 1.6670,  1.1885, -0.0464,  ...,  2.1215,  0.0046, -0.5499],\n",
       "         [ 0.6553, -1.5342,  0.5770,  ...,  0.2437, -1.3944,  0.1840]],\n",
       "\n",
       "        [[ 1.0607,  0.3815,  0.3934,  ..., -0.2935,  0.4256, -0.1617],\n",
       "         [ 1.0607,  0.3815,  0.3934,  ..., -0.2935,  0.4256, -0.1617],\n",
       "         [ 1.0607,  0.3815,  0.3934,  ..., -0.2935,  0.4256, -0.1617],\n",
       "         ...,\n",
       "         [-2.3411, -1.0062, -0.8563,  ..., -0.9615,  1.3050, -0.1689],\n",
       "         [-1.3110,  0.6002,  1.7048,  ..., -0.2773, -2.0583,  1.4835],\n",
       "         [-0.4248, -0.2866,  1.7164,  ...,  1.1965, -0.3353, -1.8403]],\n",
       "\n",
       "        [[ 1.0607,  0.3815,  0.3934,  ..., -0.2935,  0.4256, -0.1617],\n",
       "         [ 1.0607,  0.3815,  0.3934,  ..., -0.2935,  0.4256, -0.1617],\n",
       "         [ 1.0607,  0.3815,  0.3934,  ..., -0.2935,  0.4256, -0.1617],\n",
       "         ...,\n",
       "         [-1.6711,  0.8811,  1.0756,  ...,  0.7442, -1.1589, -0.8047],\n",
       "         [-1.6058,  0.8132, -1.0971,  ...,  2.1831, -0.8262,  1.7611],\n",
       "         [ 0.9355,  0.6156,  0.0322,  ...,  0.4033, -0.0864, -2.2573]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.0607,  0.3815,  0.3934,  ..., -0.2935,  0.4256, -0.1617],\n",
       "         [ 1.0607,  0.3815,  0.3934,  ..., -0.2935,  0.4256, -0.1617],\n",
       "         [ 1.0607,  0.3815,  0.3934,  ..., -0.2935,  0.4256, -0.1617],\n",
       "         ...,\n",
       "         [ 0.2367, -0.0270, -1.2817,  ...,  1.0179,  0.4781,  0.3639],\n",
       "         [ 0.1700, -0.1574, -0.6845,  ...,  0.3744, -0.7781,  0.0264],\n",
       "         [ 1.1377,  2.2310,  0.7642,  ...,  1.1401,  0.8995,  1.4556]],\n",
       "\n",
       "        [[ 1.0607,  0.3815,  0.3934,  ..., -0.2935,  0.4256, -0.1617],\n",
       "         [ 1.0607,  0.3815,  0.3934,  ..., -0.2935,  0.4256, -0.1617],\n",
       "         [ 1.0607,  0.3815,  0.3934,  ..., -0.2935,  0.4256, -0.1617],\n",
       "         ...,\n",
       "         [ 1.4249, -1.0619, -0.7695,  ...,  1.5525, -1.0030, -0.7707],\n",
       "         [ 0.1891, -1.0061,  1.0321,  ...,  1.1743, -0.5704, -0.5580],\n",
       "         [ 0.7875, -0.8443,  0.7406,  ...,  1.5146, -0.7507,  1.5915]],\n",
       "\n",
       "        [[ 1.0607,  0.3815,  0.3934,  ..., -0.2935,  0.4256, -0.1617],\n",
       "         [ 1.0607,  0.3815,  0.3934,  ..., -0.2935,  0.4256, -0.1617],\n",
       "         [ 1.0607,  0.3815,  0.3934,  ..., -0.2935,  0.4256, -0.1617],\n",
       "         ...,\n",
       "         [-0.2254, -1.0659,  1.2681,  ..., -0.3285,  0.8312,  0.6983],\n",
       "         [ 0.1700, -0.1574, -0.6845,  ...,  0.3744, -0.7781,  0.0264],\n",
       "         [ 0.9200, -0.0584, -1.3690,  ..., -0.9284, -0.6020,  0.3040]]],\n",
       "       device='cuda:0', grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        return embedded\n",
    "        \n",
    "\n",
    "embedding_layer = TokenEmbedding(vocab_size = ALL_TOKEN_MAX_WORDS, embed_size = MAXLEN_PER_SENT)\n",
    "input_tensor_train = torch.as_tensor(test.X_train, dtype=torch.int64)\n",
    "input_tensor_test = torch.as_tensor(test.X_test, dtype = torch.int64)\n",
    "input_tensor_val = torch.as_tensor(test.X_val, dtype = torch.int64)\n",
    "\n",
    "X_train = embedding_layer(input_tensor_train)\n",
    "X_test = embedding_layer(input_tensor_test)\n",
    "X_val = embedding_layer(input_tensor_val)\n",
    "\n",
    "print(\"Input tensor shape:\", input_tensor_train.shape)\n",
    "print(\"Embedded output shape:\", X_train.shape)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11344])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "\n",
    "y_train_tensor = torch.as_tensor(test.y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.as_tensor(test.y_test.to_numpy(dtype=np.single))\n",
    "y_val_tensor = torch.as_tensor(test.y_val.to_numpy(dtype=np.single))\n",
    "\n",
    "train_data = Data.TensorDataset(X_train, y_train_tensor)\n",
    "test_data = Data.TensorDataset(X_test, y_test_tensor)\n",
    "val_data = Data.TensorDataset(X_val, y_val_tensor)\n",
    "\n",
    "train_loader = Data.DataLoader(dataset=train_data,\n",
    "                               batch_size =BATCH_SIZE,\n",
    "                               shuffle=False)\n",
    "\n",
    "test_loader = Data.DataLoader(dataset=test_data,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=False)\n",
    "\n",
    "val_loader = Data.DataLoader(dataset = val_data,\n",
    "                            batch_size = BATCH_SIZE,\n",
    "                            shuffle = False)\n",
    "\n",
    "y_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, dropout = dropout, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # self.hidden = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        ##  (num layers, batch_size, hidden_size)\n",
    "        hidden_states = torch.zeros(self.num_layers, X.size(0), self.hidden_size, device = DEVICE)\n",
    "        cell_states = torch.zeros(self.num_layers, X.size(0), self.hidden_size, device = DEVICE)\n",
    "\n",
    "        out, _ = self.lstm(X, (hidden_states, cell_states))\n",
    "        out = self.output_layer(out[:, -1, :])\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "    def init_hidden(self, batch_size, device='cpu'):\n",
    "        # Initializes hidden state\n",
    "        # The hidden state is a tuple of (h_0, c_0) for LSTMs\n",
    "        # h_0: Initial hidden state for each element in the batch\n",
    "        # c_0: Initial cell state for each element in the batch\n",
    "        h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        c_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        return (h_0, c_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (lstm): LSTM(150, 75, num_layers=3, batch_first=True, dropout=0.2)\n",
      "  (output_layer): Linear(in_features=75, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lstm = LSTMModel(input_size = INPUT_LENGTH, hidden_size = HIDDEN_SIZE, num_layers = NUM_LAYERS, dropout= DROPOUT, output_size = 1).to(DEVICE)\n",
    "print(lstm)\n",
    "loss_func = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_true,y_pred):\n",
    "    correct = torch.eq(y_true,y_pred).sum().item()\n",
    "    accuracy = (correct / len(y_pred)) * 100\n",
    "    return accuracy\n",
    "\n",
    "def precision_fn(y_true,y_pred):\n",
    "    true_positive = ((y_pred == 1.0) & (y_true == 1.0)).sum().item()\n",
    "    predicted_positive = (y_pred==1.0).sum().item()\n",
    "    return true_positive/(predicted_positive + 1e-7)\n",
    "\n",
    "def recall_fn(y_true,y_pred):\n",
    "    true_positive = ((y_pred == 1.0) & (y_true == 1.0)).sum().item()\n",
    "    actual_positive = (y_true==1.0).sum().item()\n",
    "    return true_positive/(actual_positive + 1e-7)\n",
    "\n",
    "def f1_score(y_true,y_pred):\n",
    "    prec = precision_fn(y_true,y_pred)\n",
    "    recall = recall_fn(y_true,y_pred)\n",
    "    return 2 * (prec * recall) / (prec + recall + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model,loss_fn,xb,yb, opt=None):\n",
    "    yb_pred = model(xb)\n",
    "\n",
    "    # print(yb_pred)\n",
    "    # print(yb)\n",
    "    loss = loss_fn(yb_pred.squeeze(),yb)\n",
    "\n",
    "    yb_bin = (yb_pred.squeeze() > 0.5).float()\n",
    "    \n",
    "    accuracy = accuracy_fn(yb,yb_bin)\n",
    "    precision = precision_fn(yb,yb_bin)\n",
    "    recall = recall_fn(yb,yb_bin)\n",
    "    f1 = f1_score(yb,yb_bin)\n",
    "\n",
    "    if opt is not None:\n",
    "        opt.zero_grad()\n",
    "        loss.backward(retain_graph = True)\n",
    "        opt.step()\n",
    "        \n",
    "\n",
    "    return loss.item(), accuracy, precision, recall, f1, len(xb)\n",
    "    \n",
    "\n",
    "def train(num_epochs, model, train_dataloader, val_dataloader, opt = None):\n",
    "    total_steps = len(train_dataloader)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch, (X_train_batch, y_train_batch) in tqdm.tqdm(enumerate(train_dataloader), total = len(train_loader), desc = f\"Epochs {epoch+1}/{num_epochs}\"):\n",
    "            X_train_batch = X_train_batch.to(DEVICE)\n",
    "            y_train_batch = y_train_batch.to(DEVICE)\n",
    "            loss_batch(model, loss_func, X_train_batch, y_train_batch, opt)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            losses, accuracy, precision,recall, f1_scores, nums = zip(*[loss_batch(model,loss_func,xb,yb) for xb,yb in val_dataloader]\n",
    "            )\n",
    "\n",
    "        val_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)\n",
    "        val_accuracy = np.sum(np.multiply(accuracy,nums)) / np.sum(nums)\n",
    "        val_precision = np.sum(np.multiply(precision,nums)) / np.sum(nums)\n",
    "        val_recall = np.sum(np.multiply(recall,nums)) / np.sum(nums)\n",
    "        val_f1_score = np.sum(np.multiply(f1_scores,nums)) / np.sum(nums)\n",
    "\n",
    "        print(f\"Epoch: {epoch+1}\") \n",
    "        print(f\"Validation Loss: {val_loss}\")\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.4f}% | Validation Recall: {val_recall:.4f} | Validation Precision: {val_precision:.4f} | Validation F1 Score: {val_f1_score:.4f}\")   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 1/10: 100%|██████████| 91/91 [02:24<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Validation Loss: 0.8202700692361216\n",
      "Validation Accuracy: 21.3388% | Validation Recall: 1.0000 | Validation Precision: 0.2134 | Validation F1 Score: 0.3501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 2/10: 100%|██████████| 91/91 [02:23<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Validation Loss: 0.7988980048353711\n",
      "Validation Accuracy: 21.3388% | Validation Recall: 1.0000 | Validation Precision: 0.2134 | Validation F1 Score: 0.3501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 3/10: 100%|██████████| 91/91 [02:24<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Validation Loss: 0.8229302201158842\n",
      "Validation Accuracy: 21.3388% | Validation Recall: 1.0000 | Validation Precision: 0.2134 | Validation F1 Score: 0.3501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 4/10:  74%|███████▎  | 67/91 [01:47<00:38,  1.59s/it]"
     ]
    }
   ],
   "source": [
    "trained_model = train(EPOCHS, lstm, train_loader, val_loader, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(trained_model, 'model/model_epoch_10_bs_8.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load('model/trained_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_dl, model, loss_fn):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        losses, accuracy, precision, recall, f1_scores, nums = zip(\n",
    "            *[loss_batch(model,loss_fn,xb,yb) for xb,yb in test_dl]\n",
    "        )\n",
    "    test_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)\n",
    "    test_acc = np.sum(np.multiply(accuracy,nums)) / np.sum(nums)\n",
    "    test_precision = np.sum(np.multiply(precision,nums)) / np.sum(nums)\n",
    "    test_recall = np.sum(np.multiply(recall,nums)) / np.sum(nums)\n",
    "    test_f1_score = np.sum(np.multiply(f1_scores,nums)) / np.sum(nums)\n",
    "    print(f\"Test Error: \\nAccuracy: {test_acc}%, Test loss: {test_loss}\")\n",
    "    print(f\"Recall: {test_recall}, Precision: {test_precision} , F1 Score: {test_f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      "Accuracy: 65.93308739346139%, Test loss: 0.6543582614907901\n",
      "Recall: 0.765403486274822, Precision: 0.3636737000274603 , F1 Score: 0.4897227652088349\n"
     ]
    }
   ],
   "source": [
    "test(test_loader, lstm, loss_func)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
