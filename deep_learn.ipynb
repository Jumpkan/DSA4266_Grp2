{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zoe Lua\\DSA4266_Grp2\\functions.py:111: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  result = re.sub('coroll\\.', 'coroll', result)\n",
      "c:\\Users\\Zoe Lua\\DSA4266_Grp2\\functions.py:112: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  result = re.sub('pt\\.', 'pt', result)\n",
      "c:\\Users\\Zoe Lua\\anaconda3\\envs\\spam\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Zoe\n",
      "[nltk_data]     Lua\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from functions import *\n",
    "import wordninja\n",
    "import requests\n",
    "import random\n",
    "\n",
    "# Torch cannot work properly in jupyter notebook\n",
    "import os\n",
    "count = 0 \n",
    "if count == 0:\n",
    "    os.chdir(\"test_dir\")\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Zoe Lua\\\\DSA4266_Grp2\\\\test_dir'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONFIG\n",
    "\n",
    "df_path = \"../Data/full_df.pkl\"\n",
    "X_name = 'processed'\n",
    "y_name = 'class'\n",
    "\n",
    "#### For preprocessing\n",
    "all_maxlen_per_sent = [150]\n",
    "all_token_max_words = [5000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Semantic Dictionaries\n",
    "\n",
    "def get_synonyms_conceptnet(word):\n",
    "    synonyms = []\n",
    "    url = f'http://api.conceptnet.io/c/en/{word}?filter=/c/en'\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    for edge in data['edges']:\n",
    "        if edge['rel']['label'] == 'Synonym' and edge['start']['language'] == 'en' and edge['end']['language'] == 'en':\n",
    "            start = edge['start']['label']\n",
    "            end = edge['end']['label']\n",
    "            synonyms.append(end if start == word else start)\n",
    "\n",
    "    if synonyms != []:\n",
    "        synonym = random.choice(synonyms)\n",
    "    else:\n",
    "        synonym = synonyms\n",
    "    return synonym\n",
    "\n",
    "def get_synonyms_wordnet(word):\n",
    "    synonyms = []\n",
    "    synsets = wordnet.synsets(word)\n",
    "    for synset in synsets:\n",
    "        synonyms.extend([lemma.name() for lemma in synset.lemmas() if lemma.name() != word])\n",
    "\n",
    "    if synonyms != []:\n",
    "        synonym = random.choice(synonyms)\n",
    "    else:\n",
    "        synonym = synonyms\n",
    "    return synonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPrep():\n",
    "    def __init__(self, subset = None, text_prep = 'lem', token_max_words = 5000, maxlen_per_sent = 150, undersample = True):\n",
    "        \"\"\"\n",
    "        subset: X[:subset]\n",
    "        \"\"\"\n",
    "        self.df = pd.read_pickle(df_path)\n",
    "        self.subset = subset\n",
    "        self.maxlen_per_sent = maxlen_per_sent\n",
    "\n",
    "        self.remove_duplicates()\n",
    "        print('Dupes removed')\n",
    "        self.X = self.df[X_name]\n",
    "        self.y = self.df[y_name].apply(lambda x: 1 if x == 'spam' else 0)\n",
    "        self.token_max_words = token_max_words\n",
    "\n",
    "        if self.subset:\n",
    "            self.X = self.X[:self.subset]\n",
    "            self.y = self.y[:self.subset]\n",
    "        \n",
    "        print('Tokenizing..')\n",
    "        self.tokenize()\n",
    "        print('Finished Tokenizing')\n",
    "\n",
    "        print('Initialising word2vec')\n",
    "        self.word_to_vec_map = self.word2vec()\n",
    "\n",
    "        print('lemm/stemm')\n",
    "        if text_prep == 'lem':\n",
    "            self.X = self.lemming()\n",
    "        if text_prep == 'stem':\n",
    "            self.X = self.stemming()\n",
    "\n",
    "        print('Embedding...')\n",
    "        self.emb_matrix = self.tok_embedding_mat(alternative = [get_synonyms_conceptnet, get_synonyms_wordnet])\n",
    "        print('Finished embedding')\n",
    "\n",
    "        print('Padding')\n",
    "        X_pad = self.pad()\n",
    "        print('Finished padding')\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X_pad, self.y, test_size=0.33, random_state=42)\n",
    "\n",
    "        if undersample:\n",
    "            print('Undersampling..')\n",
    "            print(Counter(self.y_train))\n",
    "            self.X_train, self.y_train = self.undersample(self.X_train, self.y_train)\n",
    "            print(Counter(self.y_train))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def remove_duplicates(self):\n",
    "    \n",
    "        ## First remove all those X values with differing binary y values\n",
    "        occurrences = self.df.groupby([X_name, y_name]).size().reset_index(name='count')\n",
    "        duplicates = occurrences[occurrences.duplicated(subset=X_name, keep=False)]\n",
    "        for index, row in duplicates.iterrows():\n",
    "            x_value = row[X_name]\n",
    "            max_count = occurrences[(occurrences[X_name] == x_value)].max()['count']\n",
    "            occurrences.drop(occurrences[(occurrences[X_name] == x_value) & (occurrences['count'] != max_count)].index, inplace=True)\n",
    "\n",
    "        ## Remove duplicates\n",
    "        self.df = occurrences.drop_duplicates(subset = X_name).reset_index(drop = True)\n",
    "    \n",
    "    def tokenize(self, join = False):\n",
    "        def tokenize_helper(text, join = False):\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            tokens = word_tokenize(text)\n",
    "            tokens = [word.lower() for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "            if join:\n",
    "                tokens = ' '.join([''.join(c for c in word if c not in string.punctuation) for word in tokens if word])\n",
    "        \n",
    "            return tokens\n",
    "        \n",
    "        self.X = self.X.apply(lambda x: tokenize_helper(x, join))\n",
    "\n",
    "    ## Embedders\n",
    "        \n",
    "    def word2vec(self):\n",
    "        from gensim.models.word2vec import Word2Vec\n",
    "        import gensim.downloader as api\n",
    "\n",
    "        word_to_vec_map = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "        return word_to_vec_map\n",
    "    \n",
    "    \n",
    "    ## Stemming/ Lemmetization\n",
    "\n",
    "    def stemming(self):\n",
    "        ps = PorterStemmer()\n",
    "\n",
    "        def stem(row):\n",
    "            print(row)\n",
    "            stemmed = []\n",
    "            for word in row:\n",
    "                stemmed += [ps.stem(word)]\n",
    "            print('STEMMED:', stemmed)\n",
    "\n",
    "            return stemmed\n",
    "\n",
    "        return self.X.apply(stem)\n",
    "    \n",
    "\n",
    "    def lemming(self):\n",
    "\n",
    "        def lem(row):\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            lemmed = [lemmatizer.lemmatize(word) for word in row]\n",
    "            # print(row)\n",
    "            # print(lemmed,\"\\n\")\n",
    "            return lemmed\n",
    "\n",
    "        return self.X.apply(lem)\n",
    "    \n",
    "    def tok_embedding_mat(self, alternative):\n",
    "        \"\"\"\n",
    "        embedder: word2vec\n",
    "        alternative: list of callable to find synonyms from, inorder of precedence\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        self.tokenizer = text.Tokenizer(num_words=self.token_max_words)\n",
    "        self.tokenizer.fit_on_texts(self.X)\n",
    "\n",
    "        self.sequences = self.tokenizer.texts_to_sequences(self.X)\n",
    "\n",
    "        self.word_index = self.tokenizer.word_index\n",
    "        self.vocab_len = len(self.word_index) + 1\n",
    "        self.embed_vector_len = self.word_to_vec_map['moon'].shape[0]\n",
    "\n",
    "        emb_matrix = np.zeros((self.vocab_len, self.embed_vector_len))\n",
    "\n",
    "\n",
    "        for word, index in tqdm.tqdm(self.word_index.items(), total = len(self.word_index)):\n",
    "            try:\n",
    "                embedding_vector = self.word_to_vec_map[word]\n",
    "                emb_matrix[index-1, :] = embedding_vector\n",
    "            except:\n",
    "                for dictionary in alternative:\n",
    "                    try: \n",
    "                        synonym = dictionary(word)\n",
    "                        if synonym:\n",
    "                            # print(f'Found synonym: {synonym} for word: {word}')\n",
    "                            embedding_vector = self.word_to_vec_map[synonym] \n",
    "                            emb_matrix[index-1, :] = embedding_vector\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "        pd.to_pickle(emb_matrix, f\"../embeddings/emb_matrix_x{self.subset}_tok_{self.maxlen_per_sent}_len{self.token_max_words}.pkl\")\n",
    "\n",
    "        return emb_matrix\n",
    "\n",
    "\n",
    "    def pad(self):\n",
    "        X_pad = pad_sequences(self.sequences, maxlen = self.maxlen_per_sent)\n",
    "        return X_pad\n",
    "\n",
    "    def undersample(self):\n",
    "        undersampler = RandomUnderSampler(random_state=42)\n",
    "        X_resampled, y_resampled = undersampler.fit_resample(self.X_train, self.y_train)\n",
    "\n",
    "        return X_resampled, y_resampled\n",
    "\n",
    "\n",
    "class Train(DataPrep):\n",
    "    def __init__(self, nodes = 256, subset = None, text_prep = 'lem', token_max_words = 5000, maxlen_per_sent = 150, undersample = True):\n",
    "        super().__init__(subset, text_prep, token_max_words, maxlen_per_sent, undersample)\n",
    "\n",
    "        self.nodes = nodes\n",
    "\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Embedding(input_dim= self.vocab_len, output_dim= self.embed_vector_len, input_shape = (self.maxlen_per_sent,), trainable=False, embeddings_initializer = initializers.Constant(self.emb_matrix)))\n",
    "        self.model.add(LSTM(self.nodes))\n",
    "        self.model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "        self.model.compile(optimizer='adam',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "        # Train model\n",
    "        self.model.fit(self.X_train, self.y_train, epochs=10, batch_size=1, verbose=1)  \n",
    "\n",
    "    def predict(self, verbose = False):\n",
    "\n",
    "        loss, accuracy = self.model.evaluate(self.X_test, self.y_test)\n",
    "        print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = self.model.predict(self.X_test)\n",
    "\n",
    "        y_hat = [1 if i> 0.5 else 0 for i in predictions]\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Classification Report:\")\n",
    "            print(classification_report(self.y_test, y_hat))\n",
    "\n",
    "            print(\"Confusion Matrix:\")\n",
    "            print(confusion_matrix(self.y_test, y_hat))\n",
    "\n",
    "\n",
    "class optimize():\n",
    "    pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dupes removed\n",
      "Tokenizing..\n",
      "Finished Tokenizing\n",
      "Initialising word2vec\n",
      "lemm/stemm\n",
      "Embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11219/11219 [2:45:27<00:00,  1.13it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished embedding\n",
      "Padding\n",
      "Finished padding\n",
      "Undersampling..\n",
      "Counter({0: 196, 1: 139})\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DataPrep.undersample() takes 1 positional argument but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test \u001b[38;5;241m=\u001b[39m Train(subset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 169\u001b[0m, in \u001b[0;36mTrain.__init__\u001b[1;34m(self, nodes, subset, text_prep, token_max_words, maxlen_per_sent, undersample)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m, subset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, text_prep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlem\u001b[39m\u001b[38;5;124m'\u001b[39m, token_max_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m, maxlen_per_sent \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m150\u001b[39m, undersample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 169\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(subset, text_prep, token_max_words, maxlen_per_sent, undersample)\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes \u001b[38;5;241m=\u001b[39m nodes\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m Sequential()\n",
      "Cell \u001b[1;32mIn[5], line 46\u001b[0m, in \u001b[0;36mDataPrep.__init__\u001b[1;34m(self, subset, text_prep, token_max_words, maxlen_per_sent, undersample)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUndersampling..\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(Counter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_train))\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mundersample(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_train)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(Counter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_train))\n",
      "\u001b[1;31mTypeError\u001b[0m: DataPrep.undersample() takes 1 positional argument but 3 were given"
     ]
    }
   ],
   "source": [
    "test = Train(subset = 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 133ms/step - accuracy: 0.8667 - loss: 0.6358\n",
      "Test Accuracy: 0.8666666746139526\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 182ms/step\n"
     ]
    }
   ],
   "source": [
    "test.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to optimize\n",
    "    units = trial.suggest_categorical(\"units\", [32, 64, 128])\n",
    "    epochs = trial.suggest_categorical(\"epochs\", [10, 20, 30])\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "    \n",
    "    model = Sequential()\n",
    "    self.model.add(Embedding(input_dim= self.vocab_len, output_dim= self.embed_vector_len, input_shape = (self.maxlen_per_sent,), trainable=False, embeddings_initializer = initializers.Constant(self.emb_matrix)))\n",
    "    self.model.add(LSTM(self.nodes))\n",
    "    self.model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "    self.model.compile(optimizer='adam',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "    self.model.fit(self.X_train, self.y_train, epochs=10, batch_size=1, verbose=1)  \n",
    "\n",
    "    # Evaluate the model\n",
    "    _, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Define data and other constants\n",
    "timesteps = ...\n",
    "features = ...\n",
    "num_classes = ...\n",
    "\n",
    "# Create study object and optimize hyperparameters\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Get best hyperparameters and results\n",
    "best_trial = study.best_trial\n",
    "best_params = best_trial.params\n",
    "best_accuracy = best_trial.value\n",
    "\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "print(\"Best accuracy:\", best_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To try:\n",
    "\n",
    "- GloVe + LSTM + Ray Tune (Hyperpara Tune)\n",
    "<!-- - GloVe + LSTM  -->\n",
    "- subword tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Embedders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import KeyedVectors\n",
    "# words_to_index = tokenizer.word_index\n",
    "\n",
    "# def read_glove_vector(glove_vec):\n",
    "#   with open(glove_vec, 'r', encoding='UTF-8') as f:\n",
    "#     words = set()\n",
    "#     word_to_vec_map = {}\n",
    "#     for line in f:\n",
    "#       w_line = line.split()\n",
    "#       curr_word = w_line[0]\n",
    "#       word_to_vec_map[curr_word] = np.array(w_line[1:], dtype=np.float64)\n",
    "\n",
    "#   return word_to_vec_map\n",
    "\n",
    "# def glove_embed(path = '../GloVe/glove.6B.50d.txt', words_to_index):\n",
    "#   \"\"\"\n",
    "#   path: Path to glove txt file\n",
    "#   words_to_index: tokenizer.word_index\n",
    "#   \"\"\"\n",
    "#   word_to_vec_map = read_glove_vector(path)\n",
    "#   vocab_len = len(words_to_index)\n",
    "#   embed_vector_len = word_to_vec_map['moon'].shape[0]\n",
    "\n",
    "#   emb_matrix = np.zeros((vocab_len, embed_vector_len))\n",
    "\n",
    "#   for word, index in words_to_index.items():\n",
    "#     embedding_vector = word_to_vec_map.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#       emb_matrix[index, :] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept Net\n",
    "Takes really long to load too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import random\n",
    "\n",
    "def get_synonyms_conceptnet(word):\n",
    "    synonyms = []\n",
    "    url = f'http://api.conceptnet.io/c/en/{word}?filter=/c/en'\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    for edge in data['edges']:\n",
    "        if edge['rel']['label'] == 'Synonym' and edge['start']['language'] == 'en' and edge['end']['language'] == 'en':\n",
    "            start = edge['start']['label']\n",
    "            end = edge['end']['label']\n",
    "            synonyms.append(end if start == word else start)\n",
    "\n",
    "    if synonyms != []:\n",
    "        synonym = random.choice(synonyms)\n",
    "    else:\n",
    "        synonym = synonyms\n",
    "    return synonym\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# word = 'happy'\n",
    "# synonyms = get_synonyms_conceptnet(word)\n",
    "# print(synonyms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj = requests.get(\"http://api.conceptnet.io/c/en/example\").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming & Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem import PorterStemmer\n",
    "# nltk.download(\"punkt\")\n",
    "\n",
    "# ps = PorterStemmer()\n",
    "\n",
    "# def stem(row):\n",
    "#     print(row)\n",
    "#     stemmed = []\n",
    "#     for word in row:\n",
    "#         stemmed += [ps.stem(word)]\n",
    "#     print('STEMMED:', stemmed)\n",
    "\n",
    "#     return stemmed\n",
    "\n",
    "# X.apply(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lem(row):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmed = [lemmatizer.lemmatize(word) for word in row]\n",
    "    # print(row)\n",
    "    # print(lemmed,\"\\n\")\n",
    "    return lemmed\n",
    "\n",
    "lemmed_X = X.apply(lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def tok_embedding_mat(X, y, alternative, token_max_words = 5000 ):\n",
    "    \"\"\"\n",
    "    embedder: word2vec or GloVe\n",
    "    alternative: list of callable to find synonyms from, inorder of precedence\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    tokenizer = Tokenizer(num_words=token_max_words)\n",
    "    tokenizer.fit_on_texts(X)\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_len = len(word_index) + 1\n",
    "    embed_vector_len = word_to_vec_map['moon'].shape[0]\n",
    "\n",
    "    emb_matrix = np.zeros((vocab_len, embed_vector_len))\n",
    "\n",
    "\n",
    "    for word, index in word_index.items():\n",
    "      try:\n",
    "        embedding_vector = word_to_vec_map[word]\n",
    "        emb_matrix[index-1, :] = embedding_vector\n",
    "      except:\n",
    "        for dictionary in alternative:\n",
    "            try:\n",
    "              synonym = dictionary(word)\n",
    "              if synonym:\n",
    "                  print(f'Found synonym: {synonym} for word: {word}')\n",
    "                  embedding_vector = word_to_vec_map[synonym] \n",
    "                  emb_matrix[index-1, :] = embedding_vector\n",
    "                  break\n",
    "            except:\n",
    "               continue\n",
    "\n",
    "    return tokenizer, emb_matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found synonym: canton for word: guangzhou\n",
      "Found synonym: Guangdong province for word: guangdong\n",
      "Found synonym: Guangdong for word: guangdong\n",
      "Found synonym: swine fever for word: csf\n",
      "Found synonym: Nanking for word: nanjing\n",
      "Found synonym: Humboldt for word: humboldt\n",
      "Found synonym: Hangchow for word: hangzhou\n",
      "Found synonym: Hangchow for word: hangzhou\n",
      "Found synonym: key performance indicator for word: kpi\n",
      "Found synonym: Chungking for word: chongqing\n",
      "Found synonym: Tientsin for word: tianjin\n",
      "Found synonym: Mukden for word: shenyang\n",
      "Found synonym: Wuhan for word: wuhan\n",
      "Found synonym: Yunnan province for word: yunnan\n",
      "Found synonym: Yunnan for word: yunnan\n",
      "Found synonym: Taipeh for word: taipei\n",
      "Found synonym: chang jiang for word: yangtze\n",
      "Found synonym: Yangtze_River for word: yangtze\n",
      "Found synonym: Hunan province for word: hunan\n",
      "Found synonym: Hunan_province for word: hunan\n",
      "Found synonym: dalinian for word: dalian\n",
      "Found synonym: Dalian for word: dalian\n",
      "Found synonym: glutamine for word: gln\n",
      "Found synonym: Sichuan for word: sichuan\n",
      "Found synonym: Richelieu for word: richelieu\n",
      "Found synonym: gimp for word: gump\n",
      "Found synonym: ha erbin for word: harbin\n",
      "Found synonym: ABB for word: abb\n",
      "Found synonym: Taiyuan for word: taiyuan\n",
      "Found synonym: Hebei province for word: hebei\n",
      "Found synonym: Hopei for word: hebei\n",
      "Found synonym: Xinjiang Uighur Autonomous Region for word: xinjiang\n",
      "Found synonym: Xinjiang for word: xinjiang\n",
      "Found synonym: nantong for word: nantong\n",
      "Found synonym: inquiry for word: enquiry\n",
      "Found synonym: lennox gastaut syndrome for word: lgs\n",
      "Found synonym: Zhuang for word: zhuang\n",
      "Found synonym: macao for word: macau\n",
      "Found synonym: Macao for word: macau\n",
      "Found synonym: Changan for word: changan\n",
      "Found synonym: Dusseldorf for word: dusseldorf\n",
      "Found synonym: Guomindang for word: kuomintang\n",
      "Found synonym: amway for word: amway\n",
      "Found synonym: Delphi for word: delphi\n",
      "Found synonym: Tao for word: taoist\n",
      "Found synonym: luchow for word: hefei\n",
      "Found synonym: Kansu for word: gansu\n",
      "Found synonym: Gansu for word: gansu\n",
      "Found synonym: Hemingway for word: hemingway\n",
      "Found synonym: ningbonese for word: ningbo\n",
      "Found synonym: haikou for word: haikou\n",
      "Found synonym: jin for word: jinjiang\n",
      "Found synonym: Nagoya for word: nagoya\n",
      "Found synonym: system operator for word: sysop\n",
      "Found synonym: Outer_Mongolia for word: mongolia\n",
      "Found synonym: british virgin islands for word: bvi\n",
      "Found synonym: guangyuan for word: guangyuan\n",
      "Found synonym: crucify for word: duning\n",
      "Found synonym: genus Silvia for word: silvia\n",
      "Found synonym: genus_Silvia for word: silvia\n",
      "Found synonym: cenancestor for word: lca\n",
      "Found synonym: Circe for word: circe\n",
      "Found synonym: scientificness for word: scientificity\n",
      "Found synonym: teragramme for word: teragram\n",
      "Found synonym: Sagittarius the Archer for word: sagittarius\n",
      "Found synonym: Archer for word: sagittarius\n",
      "Found synonym: Tangshan for word: tangshan\n",
      "Found synonym: safe for word: riskfree\n",
      "Found synonym: macau for word: macao\n",
      "Found synonym: Macau for word: macao\n",
      "Found synonym: Osaka for word: osaka\n",
      "Found synonym: Kunlan Shan for word: kunlun\n",
      "Found synonym: Kunlun_Mountains for word: kunlun\n",
      "Found synonym: Nan-chang for word: nanchang\n",
      "Found synonym: Nan-chang for word: nanchang\n",
      "Found synonym: Nurnberg for word: nuremberg\n",
      "Found synonym: genus Erinaceus for word: erinaceus\n",
      "Found synonym: Erinaceus for word: erinaceus\n",
      "Found synonym: daqing for word: daqing\n",
      "Found synonym: zhuji for word: zhuji\n",
      "Found synonym: Confucianism for word: confucianism\n",
      "Found synonym: Daoism for word: taoism\n",
      "Found synonym: Gogh for word: gogh\n",
      "Found synonym: yichang for word: yichang\n",
      "Found synonym: Pareto for word: pareto\n",
      "Found synonym: off for word: cancelled\n",
      "Found synonym: Nan-ning for word: nanning\n",
      "Found synonym: Nanning for word: nanning\n",
      "Found synonym: Socrates for word: socrates\n",
      "Found synonym: Confucius for word: confucius\n",
      "Found synonym: w cdma for word: wcdma\n",
      "Found synonym: black turtle for word: xuanwu\n",
      "Found synonym: sinistroverse for word: rtl\n",
      "Found synonym: Confucian for word: confucian\n",
      "Found synonym: propwash deflection unit for word: pdu\n",
      "Found synonym: Kiowa for word: kiowa\n",
      "Found synonym: stepwell for word: baoli\n",
      "Found synonym: Albanian for word: albanian\n",
      "Found synonym: Smyrna for word: izmir\n",
      "Found synonym: Saipan for word: saipan\n",
      "Found synonym: Okinawa campaign for word: okinawa\n",
      "Found synonym: Okinawa for word: okinawa\n",
      "Found synonym: baby doctor for word: paediatrician\n",
      "Found synonym: pediatrist for word: paediatrician\n",
      "Found synonym: Lanchou for word: lanzhou\n",
      "Found synonym: Lanzhou for word: lanzhou\n",
      "Found synonym: Scandinavian Peninsula for word: scandinavia\n",
      "Found synonym: Scandinavian_Peninsula for word: scandinavia\n",
      "Found synonym: pku for word: beida\n",
      "Found synonym: Pasadena for word: pasadena\n",
      "Found synonym: Mount Parnassus for word: parnassus\n",
      "Found synonym: Parnassus for word: parnassus\n",
      "Found synonym: field for word: theatre\n",
      "Found synonym: Noyes for word: noyes\n",
      "Found synonym: vichy water for word: vichy\n",
      "Found synonym: Vichy for word: vichy\n",
      "Found synonym: tsat for word: huihui\n",
      "Found synonym: Gemini the Twins for word: gemini\n",
      "Found synonym: Twin for word: gemini\n",
      "Found synonym: ghost for word: spectre\n",
      "Found synonym: Pythia for word: pythia\n",
      "Found synonym: Pyle for word: pyle\n",
      "Found synonym: Planck for word: planck\n",
      "Found synonym: Marseilles for word: marseille\n",
      "Found synonym: Palace of Versailles for word: versailles\n",
      "Found synonym: Versailles for word: versailles\n",
      "Found synonym: yap for word: yiping\n",
      "Found synonym: Sodom for word: sodom\n",
      "Found synonym: turn_over for word: handes\n",
      "Found synonym: Pisces the Fishes for word: pisces\n",
      "Found synonym: Pisces_the_Fishes for word: pisces\n",
      "Found synonym: Hungarian capital for word: budapest\n",
      "Found synonym: Hungarian_capital for word: budapest\n",
      "Found synonym: Markov for word: markov\n",
      "Found synonym: quality of service for word: qos\n",
      "Found synonym: gastric for word: stomachic\n",
      "Found synonym: Poseidon for word: poseidon\n",
      "Found synonym: Sheraton for word: sheraton\n",
      "Found synonym: check out for word: cheque\n",
      "Found synonym: check for word: cheque\n",
      "Found synonym: Manchu for word: manchu\n",
      "Found synonym: heidelberg for word: heidelberg\n",
      "Found synonym: sanming for word: sanming\n",
      "Found synonym: Andersen for word: andersen\n",
      "Found synonym: Faust for word: faust\n",
      "Found synonym: sun for word: suning\n",
      "Found synonym: bean for word: javabean\n",
      "Found synonym: lushan for word: lushan\n",
      "Found synonym: leshan for word: leshan\n",
      "Found synonym: Loyang for word: luoyang\n",
      "Found synonym: turpan for word: turpan\n",
      "Found synonym: Oceanica for word: oceania\n",
      "Found synonym: pattaya for word: pattaya\n",
      "Found synonym: taizhong for word: taichung\n",
      "Found synonym: Taichung for word: taichung\n",
      "Found synonym: genus Sophora for word: sophora\n",
      "Found synonym: Sophora for word: sophora\n",
      "Found synonym: rhypophobia for word: mysophobia\n",
      "Found synonym: Veblen for word: veblen\n",
      "Found synonym: edmond for word: edmond\n",
      "Found synonym: weihai for word: weihai\n",
      "Found synonym: Yafo for word: yafo\n",
      "Found synonym: gridlers for word: hanjie\n",
      "Found synonym: quechan for word: yuma\n",
      "Found synonym: Yuma for word: yuma\n",
      "Found synonym: genus Ixia for word: ixia\n",
      "Found synonym: Ixia for word: ixia\n",
      "Found synonym: mugwump for word: fencesitter\n",
      "Found synonym: mugwump for word: fencesitter\n",
      "Found synonym: Sicilia for word: sicily\n",
      "Found synonym: pratas for word: dongsha\n",
      "Found synonym: genus Polygonum for word: polygonum\n",
      "Found synonym: Polygonum for word: polygonum\n",
      "Found synonym: desoxyribonucleic for word: deoxyribonucleic\n",
      "Found synonym: job for word: jobing\n",
      "Found synonym: Weimar for word: weimar\n",
      "Found synonym: Republic of Guatemala for word: guatemala\n",
      "Found synonym: Guatemala for word: guatemala\n",
      "Found synonym: laboursaving for word: laborsaving\n",
      "Found synonym: laboursaving for word: laborsaving\n",
      "Found synonym: genus Houttuynia for word: houttuynia\n",
      "Found synonym: Houttuynia for word: houttuynia\n",
      "Found synonym: miai for word: miai\n",
      "Found synonym: temporomandibular joint dysfunction for word: tmd\n",
      "Found synonym: Battle of Guadalcanal for word: guadalcanal\n",
      "Found synonym: Battle_of_Guadalcanal for word: guadalcanal\n",
      "Found synonym: bank of canada for word: boc\n",
      "Found synonym: Kyushu for word: kyushu\n",
      "Found synonym: yi for word: dongyi\n",
      "Found synonym: wan for word: wanning\n",
      "Found synonym: rivers and lakes for word: jianghu\n",
      "Found synonym: prioritize for word: prioritising\n",
      "Found synonym: Aladdin for word: aladdin\n",
      "Found synonym: cowherd for word: cowherder\n",
      "Found synonym: Fulbright for word: fulbright\n",
      "Found synonym: Pusan for word: pusan\n",
      "Found synonym: lyc photon for word: lyc\n",
      "Found synonym: ectropion for word: ectropion\n",
      "Found synonym: center for word: centre\n",
      "Found synonym: doubleword for word: dword\n",
      "Found synonym: anemone for word: windflower\n",
      "Found synonym: marines for word: usmc\n",
      "Found synonym: Bonn for word: bonn\n",
      "Found synonym: beihai for word: beihai\n",
      "Found synonym: rust for word: ruster\n",
      "Found synonym: Kurosawa for word: kurosawa\n",
      "Found synonym: kaw for word: kansa\n",
      "Found synonym: Cantonese for word: cantonese\n",
      "Found synonym: Augustine for word: augustine\n",
      "Found synonym: catia for word: catia\n",
      "Found synonym: Incheon for word: incheon\n",
      "Found synonym: panzhihua for word: panzhihua\n",
      "Found synonym: Ruanda for word: rwanda\n",
      "Found synonym: Yuman for word: yuman\n",
      "Found synonym: huzhou for word: huzhou\n",
      "Found synonym: baselessly for word: unfoundedly\n",
      "Found synonym: Ernst for word: ernst\n"
     ]
    }
   ],
   "source": [
    "tokenizer, emb_matrix = tok_embedding_mat(lemmed_X, df['class'], [get_synonyms_conceptnet, get_synonyms_wordnet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.23339844,  0.0189209 , -0.10302734, ..., -0.20214844,\n",
       "        -0.18652344,  0.22070312],\n",
       "       [-0.03564453, -0.13378906, -0.07324219, ...,  0.02954102,\n",
       "        -0.08496094, -0.22363281],\n",
       "       [-0.04736328,  0.1875    ,  0.0022583 , ..., -0.0035553 ,\n",
       "        -0.0625    , -0.05566406],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.15917969,  0.06787109,  0.01477051, ..., -0.03295898,\n",
       "         0.03662109,  0.08984375],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(emb_matrix, \"../embeddings/emb_matrix_x_2000.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences  = tokenizer.texts_to_sequences(X)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_len = len(word_index) + 1\n",
    "embed_vector_len = word_to_vec_map['moon'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pad = pad_sequences(sequences, maxlen=150)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83      1\n",
       "938     0\n",
       "1045    0\n",
       "391     1\n",
       "1057    0\n",
       "       ..\n",
       "1158    0\n",
       "1331    0\n",
       "882     1\n",
       "1498    0\n",
       "1154    1\n",
       "Name: class, Length: 1340, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zoe Lua\\anaconda3\\envs\\spam\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:81: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1340/1340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m285s\u001b[0m 211ms/step - accuracy: 0.8360 - loss: 0.3885\n",
      "Epoch 2/10\n",
      "\u001b[1m1340/1340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m248s\u001b[0m 185ms/step - accuracy: 0.9879 - loss: 0.0399\n",
      "Epoch 3/10\n",
      "\u001b[1m1340/1340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 191ms/step - accuracy: 0.9974 - loss: 0.0095\n",
      "Epoch 4/10\n",
      "\u001b[1m1340/1340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 227ms/step - accuracy: 1.0000 - loss: 0.0014\n",
      "Epoch 5/10\n",
      "\u001b[1m1340/1340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 233ms/step - accuracy: 1.0000 - loss: 1.3424e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m1340/1340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m284s\u001b[0m 212ms/step - accuracy: 1.0000 - loss: 3.6908e-05\n",
      "Epoch 7/10\n",
      "\u001b[1m1340/1340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 218ms/step - accuracy: 1.0000 - loss: 1.6421e-05\n",
      "Epoch 8/10\n",
      "\u001b[1m1340/1340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 205ms/step - accuracy: 1.0000 - loss: 6.8050e-06\n",
      "Epoch 9/10\n",
      "\u001b[1m1340/1340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 199ms/step - accuracy: 1.0000 - loss: 3.6600e-06\n",
      "Epoch 10/10\n",
      "\u001b[1m1340/1340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 200ms/step - accuracy: 1.0000 - loss: 1.9460e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x28265412960>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim= vocab_len, output_dim= embed_vector_len, input_shape = (150,), trainable=False, embeddings_initializer = initializers.Constant(emb_matrix)))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.9501 - loss: 0.3285\n",
      "Test Accuracy: 0.9651514887809753\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97       451\n",
      "           1       0.94      0.95      0.95       209\n",
      "\n",
      "    accuracy                           0.97       660\n",
      "   macro avg       0.96      0.96      0.96       660\n",
      "weighted avg       0.97      0.97      0.97       660\n",
      "\n",
      "Confusion Matrix:\n",
      "[[438  13]\n",
      " [ 10 199]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "y_hat = [1 if i> 0.5 else 0 for i in predictions]\n",
    "\n",
    "## Matrix\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_hat))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do:\n",
    "- Find ideal tokenizer MAX_WORDS\n",
    "- Find ideal padding length/dimensions\n",
    "- Find ideal LSTM Nodes\n",
    "- Find idea lepochs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
