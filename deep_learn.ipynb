{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zoe Lua\\anaconda3\\envs\\spam\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Zoe\n",
      "[nltk_data]     Lua\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Zoe\n",
      "[nltk_data]     Lua\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Zoe\n",
      "[nltk_data]     Lua\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from functions import *\n",
    "import wordninja\n",
    "import requests\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Torch cannot work properly in jupyter notebook\n",
    "# import os\n",
    "# count = 0 \n",
    "# if count == 0:\n",
    "#     os.chdir(\"test_dir\")\n",
    "#     count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Zoe Lua\\\\DSA4266_Grp2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONFIG\n",
    "\n",
    "df_path = \"Data/full_df.pkl\"\n",
    "X_name = 'processed'\n",
    "y_name = 'class'\n",
    "\n",
    "#### For preprocessing\n",
    "all_maxlen_per_sent = [150]\n",
    "all_token_max_words = [5000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Semantic Dictionaries\n",
    "\n",
    "def get_synonyms_conceptnet(word):\n",
    "    synonyms = []\n",
    "    url = f'http://api.conceptnet.io/c/en/{word}?filter=/c/en'\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    for edge in data['edges']:\n",
    "        if edge['rel']['label'] == 'Synonym' and edge['start']['language'] == 'en' and edge['end']['language'] == 'en':\n",
    "            start = edge['start']['label']\n",
    "            end = edge['end']['label']\n",
    "            synonyms.append(end if start == word else start)\n",
    "\n",
    "    if synonyms != []:\n",
    "        synonym = random.choice(synonyms)\n",
    "    else:\n",
    "        synonym = synonyms\n",
    "    return synonym\n",
    "\n",
    "def get_synonyms_wordnet(word):\n",
    "    synonyms = []\n",
    "    synsets = wordnet.synsets(word)\n",
    "    for synset in synsets:\n",
    "        synonyms.extend([lemma.name() for lemma in synset.lemmas() if lemma.name() != word])\n",
    "\n",
    "    if synonyms != []:\n",
    "        synonym = random.choice(synonyms)\n",
    "    else:\n",
    "        synonym = synonyms\n",
    "    return synonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:35: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<>:35: SyntaxWarning: invalid escape sequence '\\e'\n",
      "C:\\Users\\Zoe Lua\\AppData\\Local\\Temp\\ipykernel_17752\\555480725.py:35: SyntaxWarning: invalid escape sequence '\\e'\n",
      "  path = f'embeddings\\emb_matrix_x{self.subset}_tok_{self.maxlen_per_sent}_len{self.token_max_words}.pkl'\n"
     ]
    }
   ],
   "source": [
    "class DataPrep():\n",
    "    def __init__(self, subset = None, text_prep = 'lem', token_max_words = 5000, maxlen_per_sent = 150, undersample = True):\n",
    "        \"\"\"\n",
    "        subset: X[:subset]\n",
    "        \"\"\"\n",
    "        self.df = pd.read_pickle(df_path)\n",
    "        self.subset = subset\n",
    "        self.maxlen_per_sent = maxlen_per_sent\n",
    "\n",
    "        self.remove_duplicates()\n",
    "        print('Dupes removed')\n",
    "        self.X = self.df[X_name]\n",
    "        self.y = self.df[y_name].apply(lambda x: 1 if x == 'spam' else 0)\n",
    "        self.token_max_words = token_max_words\n",
    "\n",
    "        if self.subset:\n",
    "            self.X = self.X[:self.subset]\n",
    "            self.y = self.y[:self.subset]\n",
    "        \n",
    "        print('Tokenizing..')\n",
    "        self.tokenize()\n",
    "        print('Finished Tokenizing')\n",
    "\n",
    "        print('Initialising word2vec')\n",
    "        self.word_to_vec_map = self.word2vec()\n",
    "\n",
    "        print('lemm/stemm')\n",
    "        if text_prep == 'lem':\n",
    "            self.X = self.lemming()\n",
    "        if text_prep == 'stem':\n",
    "            self.X = self.stemming()\n",
    "\n",
    "        print('Embedding...')\n",
    "        self.pre_embed()\n",
    "        path = f'embeddings\\emb_matrix_x{self.subset}_tok_{self.maxlen_per_sent}_len{self.token_max_words}.pkl'\n",
    "        if os.path.exists(path):\n",
    "            self.emb_matrix = pd.read_pickle(path)\n",
    "        else:\n",
    "            self.emb_matrix = self.tok_embedding_mat(alternative = [get_synonyms_conceptnet, get_synonyms_wordnet])\n",
    "            print('Finished embedding')\n",
    "\n",
    "        print('Padding')\n",
    "        X_pad = self.pad()\n",
    "        print('Finished padding')\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X_pad, self.y, test_size=0.33, random_state=42)\n",
    "\n",
    "        if undersample:\n",
    "            print('Undersampling..')\n",
    "            print(Counter(self.y_train))\n",
    "            self.X_train, self.y_train = self.undersample()\n",
    "            print(Counter(self.y_train))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def remove_duplicates(self):\n",
    "    \n",
    "        ## First remove all those X values with differing binary y values\n",
    "        occurrences = self.df.groupby([X_name, y_name]).size().reset_index(name='count')\n",
    "        duplicates = occurrences[occurrences.duplicated(subset=X_name, keep=False)]\n",
    "        for index, row in duplicates.iterrows():\n",
    "            x_value = row[X_name]\n",
    "            max_count = occurrences[(occurrences[X_name] == x_value)].max()['count']\n",
    "            occurrences.drop(occurrences[(occurrences[X_name] == x_value) & (occurrences['count'] != max_count)].index, inplace=True)\n",
    "\n",
    "        ## Remove duplicates\n",
    "        self.df = occurrences.drop_duplicates(subset = X_name).reset_index(drop = True)\n",
    "    \n",
    "    def tokenize(self, join = False):\n",
    "        def tokenize_helper(text, join = False):\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            tokens = word_tokenize(text)\n",
    "            tokens = [word.lower() for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "            if join:\n",
    "                tokens = ' '.join([''.join(c for c in word if c not in string.punctuation) for word in tokens if word])\n",
    "        \n",
    "            return tokens\n",
    "        \n",
    "        self.X = self.X.apply(lambda x: tokenize_helper(x, join))\n",
    "\n",
    "    ## Embedders\n",
    "        \n",
    "    def word2vec(self):\n",
    "        from gensim.models.word2vec import Word2Vec\n",
    "        import gensim.downloader as api\n",
    "\n",
    "        word_to_vec_map = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "        return word_to_vec_map\n",
    "    \n",
    "    \n",
    "    ## Stemming/ Lemmetization\n",
    "\n",
    "    def stemming(self):\n",
    "        ps = PorterStemmer()\n",
    "\n",
    "        def stem(row):\n",
    "            print(row)\n",
    "            stemmed = []\n",
    "            for word in row:\n",
    "                stemmed += [ps.stem(word)]\n",
    "            print('STEMMED:', stemmed)\n",
    "\n",
    "            return stemmed\n",
    "\n",
    "        return self.X.apply(stem)\n",
    "    \n",
    "\n",
    "    def lemming(self):\n",
    "\n",
    "        def lem(row):\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            lemmed = [lemmatizer.lemmatize(word) for word in row]\n",
    "            # print(row)\n",
    "            # print(lemmed,\"\\n\")\n",
    "            return lemmed\n",
    "\n",
    "        return self.X.apply(lem)\n",
    "    \n",
    "    def pre_embed(self):\n",
    "        self.tokenizer = text.Tokenizer(num_words=self.token_max_words)\n",
    "        self.tokenizer.fit_on_texts(self.X)\n",
    "\n",
    "        self.sequences = self.tokenizer.texts_to_sequences(self.X)\n",
    "\n",
    "        self.word_index = self.tokenizer.word_index\n",
    "        self.vocab_len = len(self.word_index) + 1\n",
    "        self.embed_vector_len = self.word_to_vec_map['moon'].shape[0]\n",
    "    \n",
    "    def tok_embedding_mat(self, alternative):\n",
    "        \"\"\"\n",
    "        embedder: word2vec\n",
    "        alternative: list of callable to find synonyms from, inorder of precedence\n",
    "        \"\"\"\n",
    "\n",
    "        emb_matrix = np.zeros((self.vocab_len, self.embed_vector_len))\n",
    "\n",
    "\n",
    "        for word, index in tqdm.tqdm(self.word_index.items(), total = len(self.word_index)):\n",
    "            try:\n",
    "                embedding_vector = self.word_to_vec_map[word]\n",
    "                emb_matrix[index-1, :] = embedding_vector\n",
    "            except:\n",
    "                for dictionary in alternative:\n",
    "                    try: \n",
    "                        synonym = dictionary(word)\n",
    "                        if synonym:\n",
    "                            # print(f'Found synonym: {synonym} for word: {word}')\n",
    "                            embedding_vector = self.word_to_vec_map[synonym] \n",
    "                            emb_matrix[index-1, :] = embedding_vector\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "        pd.to_pickle(emb_matrix, f\"../embeddings/emb_matrix_x{self.subset}_tok_{self.maxlen_per_sent}_len{self.token_max_words}.pkl\")\n",
    "\n",
    "        return emb_matrix\n",
    "\n",
    "\n",
    "    def pad(self):\n",
    "        X_pad = pad_sequences(self.sequences, maxlen = self.maxlen_per_sent)\n",
    "        return X_pad\n",
    "\n",
    "    def undersample(self):\n",
    "        undersampler = RandomUnderSampler(random_state=42)\n",
    "        X_resampled, y_resampled = undersampler.fit_resample(self.X_train, self.y_train)\n",
    "\n",
    "        return X_resampled, y_resampled\n",
    "\n",
    "\n",
    "class Train(DataPrep):\n",
    "    def __init__(self, subset = None, text_prep = 'lem', token_max_words = 5000, maxlen_per_sent = 150, undersample = True):\n",
    "        super().__init__(subset, text_prep, token_max_words, maxlen_per_sent, undersample)\n",
    "\n",
    "    def lstm(self, nodes):\n",
    "        \"\"\"\n",
    "        Single layer LSTM\n",
    "        \"\"\"\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Embedding(input_dim= self.vocab_len, output_dim= self.embed_vector_len, input_shape = (self.maxlen_per_sent,), trainable=False, embeddings_initializer = initializers.Constant(self.emb_matrix)))\n",
    "        self.model.add(LSTM(512))\n",
    "        self.model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "        self.model.compile(optimizer='adam',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "        # Train model\n",
    "        self.model.fit(self.X_train, self.y_train, epochs=10, batch_size=1, verbose=1)  \n",
    "    \n",
    "    def lstm_op(self):\n",
    "        import math\n",
    "\n",
    "        def objective(trial):\n",
    "            units = trial.suggest_categorical(\"units\", [32, 64, 128, 256])\n",
    "            units2 = round(math.sqrt(units))\n",
    "            epochs = trial.suggest_categorical(\"epochs\", [10, 20, 30])\n",
    "            batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "            dropout = trial.suggest_float(\"dropout\", low = 0.1, high = 0.5)\n",
    "            \n",
    "            self.model = Sequential()\n",
    "            self.model.add(Embedding(input_dim= self.vocab_len, output_dim= self.embed_vector_len, input_shape = (self.maxlen_per_sent,), trainable=False, embeddings_initializer = initializers.Constant(self.emb_matrix)))\n",
    "            self.model.add(LSTM(units))\n",
    "            self.model.add(Dropout(dropout))\n",
    "\n",
    "            self.model.add(Dense(units2))\n",
    "            self.model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "            self.model.compile(optimizer='adam',\n",
    "                            loss='binary_crossentropy',\n",
    "                            metrics=['accuracy'])\n",
    "\n",
    "            self.model.fit(self.X_train, self.y_train, epochs= epochs, batch_size= batch_size, verbose=1)  \n",
    "            _, accuracy = self.model.evaluate(self.X_test, self.y_test, verbose=0)\n",
    "\n",
    "            return accuracy\n",
    "\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(objective, n_trials=10)\n",
    "\n",
    "        self.best_trial = study.best_trial\n",
    "        self.best_params = self.best_trial.params\n",
    "        self.best_accuracy = self.best_trial.value\n",
    "\n",
    "        print(\"Best hyperparameters:\", self.best_params)\n",
    "        print(\"Best accuracy:\", self.best_accuracy)\n",
    "\n",
    "\n",
    "    def predict(self, verbose = False):\n",
    "\n",
    "        loss, accuracy = self.model.evaluate(self.X_test, self.y_test)\n",
    "        print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = self.model.predict(self.X_test)\n",
    "\n",
    "        y_hat = [1 if i> 0.5 else 0 for i in predictions]\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Classification Report:\")\n",
    "            print(classification_report(self.y_test, y_hat))\n",
    "\n",
    "            print(\"Confusion Matrix:\")\n",
    "            print(confusion_matrix(self.y_test, y_hat))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dupes removed\n",
      "Tokenizing..\n",
      "Finished Tokenizing\n",
      "Initialising word2vec\n",
      "lemm/stemm\n",
      "Embedding...\n",
      "Padding\n",
      "Finished padding\n",
      "Undersampling..\n",
      "Counter({0: 196, 1: 139})\n",
      "Counter({0: 139, 1: 139})\n"
     ]
    }
   ],
   "source": [
    "test = Train(subset = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-31 22:28:49,582] A new study created in memory with name: no-name-7f6696dc-9a06-4f88-89bb-fa311308bd27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zoe Lua\\anaconda3\\envs\\spam\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:81: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 118ms/step - accuracy: 0.5802 - loss: 0.6578\n",
      "Epoch 2/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - accuracy: 0.8185 - loss: 0.5009\n",
      "Epoch 3/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 123ms/step - accuracy: 0.8070 - loss: 0.3820\n",
      "Epoch 4/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 141ms/step - accuracy: 0.9141 - loss: 0.2457\n",
      "Epoch 5/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 127ms/step - accuracy: 0.9739 - loss: 0.1103\n",
      "Epoch 6/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 122ms/step - accuracy: 0.9889 - loss: 0.0634\n",
      "Epoch 7/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 140ms/step - accuracy: 0.9788 - loss: 0.0587\n",
      "Epoch 8/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step - accuracy: 0.9874 - loss: 0.0478\n",
      "Epoch 9/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step - accuracy: 0.9973 - loss: 0.0225\n",
      "Epoch 10/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step - accuracy: 1.0000 - loss: 0.0211\n",
      "Epoch 11/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 112ms/step - accuracy: 1.0000 - loss: 0.0132\n",
      "Epoch 12/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 112ms/step - accuracy: 1.0000 - loss: 0.0097\n",
      "Epoch 13/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step - accuracy: 1.0000 - loss: 0.0045\n",
      "Epoch 14/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - accuracy: 1.0000 - loss: 0.0050\n",
      "Epoch 15/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 122ms/step - accuracy: 1.0000 - loss: 0.0037\n",
      "Epoch 16/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 124ms/step - accuracy: 1.0000 - loss: 0.0030\n",
      "Epoch 17/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step - accuracy: 1.0000 - loss: 0.0025\n",
      "Epoch 18/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 125ms/step - accuracy: 1.0000 - loss: 0.0033\n",
      "Epoch 19/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step - accuracy: 1.0000 - loss: 0.0028\n",
      "Epoch 20/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 162ms/step - accuracy: 1.0000 - loss: 0.0023\n",
      "Epoch 21/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 143ms/step - accuracy: 1.0000 - loss: 0.0018\n",
      "Epoch 22/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 108ms/step - accuracy: 1.0000 - loss: 0.0014\n",
      "Epoch 23/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 108ms/step - accuracy: 1.0000 - loss: 0.0013\n",
      "Epoch 24/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 134ms/step - accuracy: 1.0000 - loss: 0.0013\n",
      "Epoch 25/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 145ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 26/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 108ms/step - accuracy: 1.0000 - loss: 0.0014\n",
      "Epoch 27/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 104ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 28/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 113ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 29/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 112ms/step - accuracy: 1.0000 - loss: 7.6018e-04\n",
      "Epoch 30/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 122ms/step - accuracy: 1.0000 - loss: 8.3130e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-31 22:29:30,910] Trial 0 finished with value: 0.903030276298523 and parameters: {'units': 32, 'epochs': 30, 'batch_size': 32, 'dropout': 0.145162878487423}. Best is trial 0 with value: 0.903030276298523.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 142ms/step - accuracy: 0.6487 - loss: 0.6668\n",
      "Epoch 2/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 168ms/step - accuracy: 0.7656 - loss: 0.5520\n",
      "Epoch 3/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 130ms/step - accuracy: 0.7997 - loss: 0.4838\n",
      "Epoch 4/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 122ms/step - accuracy: 0.8197 - loss: 0.3872\n",
      "Epoch 5/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 122ms/step - accuracy: 0.8817 - loss: 0.3088\n",
      "Epoch 6/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 122ms/step - accuracy: 0.9572 - loss: 0.1926\n",
      "Epoch 7/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 124ms/step - accuracy: 0.9708 - loss: 0.1288\n",
      "Epoch 8/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 125ms/step - accuracy: 0.9785 - loss: 0.0851\n",
      "Epoch 9/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 131ms/step - accuracy: 0.9827 - loss: 0.0663\n",
      "Epoch 10/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 125ms/step - accuracy: 0.9878 - loss: 0.0606\n",
      "Epoch 11/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 125ms/step - accuracy: 0.9963 - loss: 0.0462\n",
      "Epoch 12/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 125ms/step - accuracy: 1.0000 - loss: 0.0330\n",
      "Epoch 13/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 124ms/step - accuracy: 1.0000 - loss: 0.0271\n",
      "Epoch 14/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 125ms/step - accuracy: 0.9988 - loss: 0.0227\n",
      "Epoch 15/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 126ms/step - accuracy: 1.0000 - loss: 0.0204\n",
      "Epoch 16/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 123ms/step - accuracy: 0.9915 - loss: 0.0359\n",
      "Epoch 17/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 124ms/step - accuracy: 1.0000 - loss: 0.0154\n",
      "Epoch 18/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 124ms/step - accuracy: 1.0000 - loss: 0.0144\n",
      "Epoch 19/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 124ms/step - accuracy: 1.0000 - loss: 0.0124\n",
      "Epoch 20/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 125ms/step - accuracy: 1.0000 - loss: 0.0082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-31 22:29:50,436] Trial 1 finished with value: 0.8909090757369995 and parameters: {'units': 32, 'epochs': 20, 'batch_size': 64, 'dropout': 0.33064798001600804}. Best is trial 0 with value: 0.903030276298523.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 135ms/step - accuracy: 0.5439 - loss: 0.6776\n",
      "Epoch 2/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.7972 - loss: 0.6227\n",
      "Epoch 3/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8099 - loss: 0.5637\n",
      "Epoch 4/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8175 - loss: 0.5160\n",
      "Epoch 5/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8091 - loss: 0.4992\n",
      "Epoch 6/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8323 - loss: 0.4225\n",
      "Epoch 7/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8398 - loss: 0.3726\n",
      "Epoch 8/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8680 - loss: 0.3416\n",
      "Epoch 9/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8865 - loss: 0.2749\n",
      "Epoch 10/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.9326 - loss: 0.2248\n",
      "Epoch 11/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.9689 - loss: 0.1572\n",
      "Epoch 12/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9878 - loss: 0.1057\n",
      "Epoch 13/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.9861 - loss: 0.1144\n",
      "Epoch 14/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9850 - loss: 0.0644\n",
      "Epoch 15/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 171ms/step - accuracy: 0.9803 - loss: 0.0689\n",
      "Epoch 16/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 141ms/step - accuracy: 0.9972 - loss: 0.0390\n",
      "Epoch 17/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 141ms/step - accuracy: 0.9972 - loss: 0.0304\n",
      "Epoch 18/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 139ms/step - accuracy: 0.9972 - loss: 0.0257\n",
      "Epoch 19/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 137ms/step - accuracy: 0.9972 - loss: 0.0206\n",
      "Epoch 20/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.9953 - loss: 0.0212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-31 22:30:04,806] Trial 2 finished with value: 0.8787878751754761 and parameters: {'units': 32, 'epochs': 20, 'batch_size': 128, 'dropout': 0.13676090564396914}. Best is trial 0 with value: 0.903030276298523.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 218ms/step - accuracy: 0.7073 - loss: 0.6368\n",
      "Epoch 2/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 198ms/step - accuracy: 0.7614 - loss: 0.4814\n",
      "Epoch 3/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 183ms/step - accuracy: 0.8443 - loss: 0.3691\n",
      "Epoch 4/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 165ms/step - accuracy: 0.9069 - loss: 0.2194\n",
      "Epoch 5/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 173ms/step - accuracy: 0.9696 - loss: 0.1191\n",
      "Epoch 6/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 237ms/step - accuracy: 0.9702 - loss: 0.0688\n",
      "Epoch 7/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 184ms/step - accuracy: 0.9685 - loss: 0.0899\n",
      "Epoch 8/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 233ms/step - accuracy: 0.9958 - loss: 0.0459\n",
      "Epoch 9/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 179ms/step - accuracy: 0.9942 - loss: 0.0225\n",
      "Epoch 10/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 169ms/step - accuracy: 1.0000 - loss: 0.0085\n",
      "Epoch 11/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 180ms/step - accuracy: 0.9955 - loss: 0.0069\n",
      "Epoch 12/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 188ms/step - accuracy: 0.9993 - loss: 0.0055\n",
      "Epoch 13/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 181ms/step - accuracy: 1.0000 - loss: 0.0100\n",
      "Epoch 14/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 166ms/step - accuracy: 1.0000 - loss: 0.0106\n",
      "Epoch 15/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 173ms/step - accuracy: 1.0000 - loss: 0.0033\n",
      "Epoch 16/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 170ms/step - accuracy: 1.0000 - loss: 0.0020\n",
      "Epoch 17/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 164ms/step - accuracy: 1.0000 - loss: 0.0014\n",
      "Epoch 18/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 169ms/step - accuracy: 1.0000 - loss: 0.0018\n",
      "Epoch 19/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 164ms/step - accuracy: 1.0000 - loss: 9.4058e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 168ms/step - accuracy: 1.0000 - loss: 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-31 22:30:45,336] Trial 3 finished with value: 0.8969696760177612 and parameters: {'units': 128, 'epochs': 20, 'batch_size': 32, 'dropout': 0.48896846326570154}. Best is trial 0 with value: 0.903030276298523.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 146ms/step - accuracy: 0.7305 - loss: 0.6297\n",
      "Epoch 2/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 144ms/step - accuracy: 0.7634 - loss: 0.5063\n",
      "Epoch 3/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step - accuracy: 0.7960 - loss: 0.3923\n",
      "Epoch 4/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 153ms/step - accuracy: 0.8521 - loss: 0.2930\n",
      "Epoch 5/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 163ms/step - accuracy: 0.9579 - loss: 0.1137\n",
      "Epoch 6/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 150ms/step - accuracy: 0.9754 - loss: 0.0595\n",
      "Epoch 7/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 143ms/step - accuracy: 0.9896 - loss: 0.0345\n",
      "Epoch 8/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 141ms/step - accuracy: 0.9984 - loss: 0.0185\n",
      "Epoch 9/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 141ms/step - accuracy: 0.9989 - loss: 0.0096\n",
      "Epoch 10/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 148ms/step - accuracy: 0.9965 - loss: 0.0077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-31 22:31:08,034] Trial 4 finished with value: 0.8666666746139526 and parameters: {'units': 64, 'epochs': 10, 'batch_size': 32, 'dropout': 0.21506017509820344}. Best is trial 0 with value: 0.903030276298523.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 752ms/step - accuracy: 0.6418 - loss: 0.6495\n",
      "Epoch 2/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 728ms/step - accuracy: 0.7603 - loss: 0.5047\n",
      "Epoch 3/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 715ms/step - accuracy: 0.8110 - loss: 0.3852\n",
      "Epoch 4/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 682ms/step - accuracy: 0.8868 - loss: 0.3061\n",
      "Epoch 5/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 660ms/step - accuracy: 0.9145 - loss: 0.1920\n",
      "Epoch 6/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 662ms/step - accuracy: 0.9621 - loss: 0.1255\n",
      "Epoch 7/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 737ms/step - accuracy: 0.9785 - loss: 0.0884\n",
      "Epoch 8/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 672ms/step - accuracy: 0.9920 - loss: 0.0414\n",
      "Epoch 9/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 662ms/step - accuracy: 0.9813 - loss: 0.0302\n",
      "Epoch 10/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 665ms/step - accuracy: 0.9933 - loss: 0.0525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-31 22:31:49,755] Trial 5 finished with value: 0.8545454740524292 and parameters: {'units': 256, 'epochs': 10, 'batch_size': 64, 'dropout': 0.4494049270982179}. Best is trial 0 with value: 0.903030276298523.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 186ms/step - accuracy: 0.6390 - loss: 0.6614\n",
      "Epoch 2/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step - accuracy: 0.7379 - loss: 0.5725\n",
      "Epoch 3/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 168ms/step - accuracy: 0.7895 - loss: 0.4874\n",
      "Epoch 4/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 185ms/step - accuracy: 0.7923 - loss: 0.4074\n",
      "Epoch 5/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step - accuracy: 0.8338 - loss: 0.3585\n",
      "Epoch 6/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 153ms/step - accuracy: 0.9347 - loss: 0.1977\n",
      "Epoch 7/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 153ms/step - accuracy: 0.9761 - loss: 0.0959\n",
      "Epoch 8/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step - accuracy: 0.9825 - loss: 0.0610\n",
      "Epoch 9/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step - accuracy: 0.9933 - loss: 0.0405\n",
      "Epoch 10/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step - accuracy: 0.9934 - loss: 0.0309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-31 22:32:03,714] Trial 6 finished with value: 0.8969696760177612 and parameters: {'units': 64, 'epochs': 10, 'batch_size': 64, 'dropout': 0.4497249394189913}. Best is trial 0 with value: 0.903030276298523.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 366ms/step - accuracy: 0.6100 - loss: 0.6621\n",
      "Epoch 2/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 361ms/step - accuracy: 0.7519 - loss: 0.5281\n",
      "Epoch 3/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 385ms/step - accuracy: 0.7775 - loss: 0.4326\n",
      "Epoch 4/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 356ms/step - accuracy: 0.8546 - loss: 0.3472\n",
      "Epoch 5/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 355ms/step - accuracy: 0.8962 - loss: 0.2508\n",
      "Epoch 6/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 343ms/step - accuracy: 0.9734 - loss: 0.1097\n",
      "Epoch 7/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 339ms/step - accuracy: 0.9827 - loss: 0.0812\n",
      "Epoch 8/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 347ms/step - accuracy: 0.9167 - loss: 0.1978\n",
      "Epoch 9/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 401ms/step - accuracy: 0.9721 - loss: 0.0919\n",
      "Epoch 10/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 357ms/step - accuracy: 0.9878 - loss: 0.0483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-31 22:32:28,640] Trial 7 finished with value: 0.8545454740524292 and parameters: {'units': 128, 'epochs': 10, 'batch_size': 64, 'dropout': 0.21853321855234267}. Best is trial 0 with value: 0.903030276298523.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 748ms/step - accuracy: 0.5668 - loss: 0.6488\n",
      "Epoch 2/10\n"
     ]
    }
   ],
   "source": [
    "test.lstm_op()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenTrial(number=1, state=1, values=[0.8969696760177612], datetime_start=datetime.datetime(2024, 3, 31, 22, 19, 58, 558387), datetime_complete=datetime.datetime(2024, 3, 31, 22, 20, 33, 713916), params={'units': 32, 'epochs': 30, 'batch_size': 32, 'dropout': 0.1826917779294587}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'units': CategoricalDistribution(choices=(32, 64, 128, 256)), 'epochs': CategoricalDistribution(choices=(10, 20, 30)), 'batch_size': CategoricalDistribution(choices=(32, 64, 128)), 'dropout': FloatDistribution(high=0.5, log=False, low=0.1, step=None)}, trial_id=1, value=None)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
