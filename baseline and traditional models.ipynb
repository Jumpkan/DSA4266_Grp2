{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import re\n",
    "from TCSP import read_stopwords_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English Translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_pickle('../data/full_df_en_processed.pkl').reset_index()#.drop_duplicates('clean_msg')\n",
    "temp=pd.read_pickle('../data/full_df.pkl').reset_index()\n",
    "data=data.merge(temp[['doc_id','pretranslation']],how='left',on='doc_id').drop_duplicates('pretranslation').drop_duplicates('clean_msg')\n",
    "# data=data[['class','processed']].drop_duplicates().rename(columns={'processed':'clean_msg'})\n",
    "\n",
    "data=data[data['clean_msg']!=''].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_word=re.compile(\"[\\u4e00-\\u9FFF]\")\n",
    "data['chinese']=data['pretranslation'].apply(lambda x: ''.join([word for word in x if cn_word.match(word)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lang']=data['chinese'].apply(lambda x: 'c' if len(x)>2 else 'e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>class</th>\n",
       "      <th>translated</th>\n",
       "      <th>clean_msg</th>\n",
       "      <th>pretranslation</th>\n",
       "      <th>chinese</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lang</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>23583</td>\n",
       "      <td>23583</td>\n",
       "      <td>23583</td>\n",
       "      <td>23583</td>\n",
       "      <td>23583</td>\n",
       "      <td>23583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>15662</td>\n",
       "      <td>15662</td>\n",
       "      <td>15662</td>\n",
       "      <td>15662</td>\n",
       "      <td>15662</td>\n",
       "      <td>15662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      doc_id  class  translated  clean_msg  pretranslation  chinese\n",
       "lang                                                               \n",
       "c      23583  23583       23583      23583           23583    23583\n",
       "e      15662  15662       15662      15662           15662    15662"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('lang').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_regex = re.compile('[^a-zA-Z ]')\n",
    "# d = enchant.Dict(\"en_US\") \n",
    "# STOPWORDS = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def enPreprocess(string):\n",
    "#     string=string.replace('\\n',' ') #remove newline char\n",
    "#     string=en_regex.sub('',string) #removes non alphabets\n",
    "#     string=string.split()\n",
    "#     string=[word.lower() for word in string if len(word)>1]\n",
    "#     string=' '.join([word for word in string if d.check(word) and (word not in STOPWORDS)]) # split the string into list and check each word if it is in the english dictionary and longer than 1 alphabet\n",
    "#     return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['clean_msg']=data['translated'].apply(lambda x: enPreprocess(x))\n",
    "# data.to_pickle('lowercase words.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = data.drop(columns='class')\n",
    "y = data['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=.2,random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,test_size=.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "ham     19566\n",
       "spam     5550\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=pd.concat([y_train[y_train=='ham'].sample(5528),y_train[y_train=='spam']]).sample(frac=1) #under sampling\n",
    "# y_train=pd.concat([y_train[y_train=='ham'],y_train[y_train=='spam'].sample(24461,replace=True)]).sample(frac=1) #over sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.loc[y_train.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_e=X_train[X_train['lang']=='e']['clean_msg']\n",
    "X_val_e=X_val[X_val['lang']=='e']['clean_msg']\n",
    "X_test_e=X_test[X_test['lang']=='e']['clean_msg']\n",
    "y_train_e=y_train.loc[X_train_e.index]\n",
    "y_val_e=y_val.loc[X_val_e.index]\n",
    "y_test_e=y_test.loc[X_test_e.index]\n",
    "\n",
    "X_train_c=X_train[X_train['lang']=='c']['chinese']\n",
    "X_val_c=X_val[X_val['lang']=='c']['chinese']\n",
    "X_test_c=X_test[X_test['lang']=='c']['chinese']\n",
    "y_train_c=y_train.loc[X_train_c.index]\n",
    "y_val_c=y_val.loc[X_val_c.index]\n",
    "y_test_c=y_test.loc[X_test_c.index]\n",
    "\n",
    "X_train=X_train['clean_msg']\n",
    "X_val=X_val['clean_msg']\n",
    "X_test=X_test['clean_msg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words='english')\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_val_dtm = vect.transform(X_val)\n",
    "X_test_dtm= vect.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidvect=TfidfTransformer(smooth_idf=1)\n",
    "# X_train_dtm = tfidvect.fit_transform(X_train_dtm)\n",
    "# X_val_dtm = tfidvect.transform(X_val)\n",
    "# X_test_dtm= tfidvect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translated dataset Multinomial Naive Bayes \n",
      "acc 0.9305732484076433\n",
      "f1 0.8425992779783393\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(X_val_dtm)\n",
    "\n",
    "print('translated dataset Multinomial Naive Bayes ')\n",
    "print('acc',metrics.accuracy_score(y_val, y_pred_class))\n",
    "print('f1',f1_score(y_val.to_list(), y_pred_class,pos_label=\"spam\"))\n",
    "# metrics.confusion_matrix(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translated dataset log regression \n",
      "acc 0.9487261146496815\n",
      "validation f1 0.8924515698062792\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(solver='liblinear')\n",
    "logreg.fit(X_train_dtm, y_train)\n",
    "y_pred_class = logreg.predict(X_val_dtm)\n",
    "y_pred_prob = logreg.predict_proba(X_val_dtm)[:, 1]\n",
    "\n",
    "print('translated dataset log regression ')\n",
    "print('acc',metrics.accuracy_score(y_val, y_pred_class))\n",
    "print('validation f1',f1_score(y_val.to_list(), y_pred_class,pos_label=\"spam\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshhold optimise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[]\n",
    "b=[]\n",
    "for i in np.arange(0.7,.85,0.001):\n",
    "    a.append(i)\n",
    "    c=pd.Series(y_pred_prob).apply(lambda x: 'spam' if x>i else 'ham')\n",
    "    b.append(f1_score(y_val.to_list(),c,pos_label=\"spam\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshhold value</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.744</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.745</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.746</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.747</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.748</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.749</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.787</td>\n",
       "      <td>0.916577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.743</td>\n",
       "      <td>0.916343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.786</td>\n",
       "      <td>0.916309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.785</td>\n",
       "      <td>0.916309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    threshhold value  f1_score\n",
       "44             0.744  0.916667\n",
       "45             0.745  0.916667\n",
       "46             0.746  0.916667\n",
       "47             0.747  0.916667\n",
       "48             0.748  0.916667\n",
       "49             0.749  0.916667\n",
       "87             0.787  0.916577\n",
       "43             0.743  0.916343\n",
       "86             0.786  0.916309\n",
       "85             0.785  0.916309"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresh_hold_table=pd.DataFrame({'threshhold value':a,'f1_score':b}).sort_values('f1_score',ascending=False).head(10)\n",
    "thresh_hold_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test f1 0.9240398293029872\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = logreg.predict_proba(X_test_dtm)[:, 1]\n",
    "opt_predicted=pd.Series(y_pred_prob).apply(lambda x: 'spam' if x>thresh_hold_table.iloc[0,0] else 'ham')\n",
    "print('test f1',f1_score(y_test.to_list(), opt_predicted,pos_label=\"spam\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=pd.concat([data[data['class']=='ham'].sample(8666),data[data['class']=='spam']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>weights</th>\n",
       "      <th>word</th>\n",
       "      <th>ham_count</th>\n",
       "      <th>spam_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25599</th>\n",
       "      <td>25599</td>\n",
       "      <td>-2.424083</td>\n",
       "      <td>thanks</td>\n",
       "      <td>263.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25871</th>\n",
       "      <td>25871</td>\n",
       "      <td>-2.258781</td>\n",
       "      <td>title</td>\n",
       "      <td>69.0</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14905</th>\n",
       "      <td>14905</td>\n",
       "      <td>-1.573450</td>\n",
       "      <td>list</td>\n",
       "      <td>781.0</td>\n",
       "      <td>213.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18814</th>\n",
       "      <td>18814</td>\n",
       "      <td>-1.417257</td>\n",
       "      <td>plan</td>\n",
       "      <td>295.0</td>\n",
       "      <td>154.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2788</th>\n",
       "      <td>2788</td>\n",
       "      <td>-1.409560</td>\n",
       "      <td>board</td>\n",
       "      <td>954.0</td>\n",
       "      <td>105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21790</th>\n",
       "      <td>21790</td>\n",
       "      <td>-1.376202</td>\n",
       "      <td>robot</td>\n",
       "      <td>294.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>1160</td>\n",
       "      <td>-1.361579</td>\n",
       "      <td>anybody</td>\n",
       "      <td>152.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21658</th>\n",
       "      <td>21658</td>\n",
       "      <td>-1.346509</td>\n",
       "      <td>ribbon</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26876</th>\n",
       "      <td>26876</td>\n",
       "      <td>-1.340602</td>\n",
       "      <td>university</td>\n",
       "      <td>47.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28263</th>\n",
       "      <td>28263</td>\n",
       "      <td>-1.327338</td>\n",
       "      <td>wrote</td>\n",
       "      <td>989.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18948</th>\n",
       "      <td>18948</td>\n",
       "      <td>-1.258351</td>\n",
       "      <td>pm</td>\n",
       "      <td>295.0</td>\n",
       "      <td>211.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19623</th>\n",
       "      <td>19623</td>\n",
       "      <td>-1.197419</td>\n",
       "      <td>problem</td>\n",
       "      <td>926.0</td>\n",
       "      <td>147.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19654</th>\n",
       "      <td>19654</td>\n",
       "      <td>-1.191490</td>\n",
       "      <td>produced</td>\n",
       "      <td>34.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27015</th>\n",
       "      <td>27015</td>\n",
       "      <td>-1.187651</td>\n",
       "      <td>unsubscribe</td>\n",
       "      <td>164.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13707</th>\n",
       "      <td>13707</td>\n",
       "      <td>-1.166726</td>\n",
       "      <td>interval</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4551</th>\n",
       "      <td>4551</td>\n",
       "      <td>-1.146243</td>\n",
       "      <td>class</td>\n",
       "      <td>163.0</td>\n",
       "      <td>106.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14248</th>\n",
       "      <td>14248</td>\n",
       "      <td>-1.141699</td>\n",
       "      <td>kind</td>\n",
       "      <td>197.0</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13470</th>\n",
       "      <td>13470</td>\n",
       "      <td>-1.074914</td>\n",
       "      <td>installed</td>\n",
       "      <td>107.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3322</th>\n",
       "      <td>3322</td>\n",
       "      <td>-1.068887</td>\n",
       "      <td>build</td>\n",
       "      <td>269.0</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21780</th>\n",
       "      <td>21780</td>\n",
       "      <td>-1.023426</td>\n",
       "      <td>rob</td>\n",
       "      <td>1384.0</td>\n",
       "      <td>252.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22630</th>\n",
       "      <td>22630</td>\n",
       "      <td>-1.002553</td>\n",
       "      <td>sensor</td>\n",
       "      <td>205.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19189</th>\n",
       "      <td>19189</td>\n",
       "      <td>-1.000832</td>\n",
       "      <td>posted</td>\n",
       "      <td>90.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16476</th>\n",
       "      <td>16476</td>\n",
       "      <td>-0.967062</td>\n",
       "      <td>movie</td>\n",
       "      <td>27.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>1265</td>\n",
       "      <td>-0.936182</td>\n",
       "      <td>appreciated</td>\n",
       "      <td>186.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15314</th>\n",
       "      <td>15314</td>\n",
       "      <td>-0.933805</td>\n",
       "      <td>mails</td>\n",
       "      <td>63.0</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21316</th>\n",
       "      <td>21316</td>\n",
       "      <td>-0.926039</td>\n",
       "      <td>research</td>\n",
       "      <td>205.0</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28165</th>\n",
       "      <td>28165</td>\n",
       "      <td>-0.924569</td>\n",
       "      <td>working</td>\n",
       "      <td>281.0</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15995</th>\n",
       "      <td>15995</td>\n",
       "      <td>-0.904932</td>\n",
       "      <td>mike</td>\n",
       "      <td>28.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23553</th>\n",
       "      <td>23553</td>\n",
       "      <td>0.923185</td>\n",
       "      <td>soft</td>\n",
       "      <td>484.0</td>\n",
       "      <td>550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.932944</td>\n",
       "      <td>ab</td>\n",
       "      <td>2248.0</td>\n",
       "      <td>1788.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19530</th>\n",
       "      <td>19530</td>\n",
       "      <td>1.011571</td>\n",
       "      <td>prices</td>\n",
       "      <td>22.0</td>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14634</th>\n",
       "      <td>14634</td>\n",
       "      <td>1.038776</td>\n",
       "      <td>legal</td>\n",
       "      <td>43.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17392</th>\n",
       "      <td>17392</td>\n",
       "      <td>1.138486</td>\n",
       "      <td>online</td>\n",
       "      <td>109.0</td>\n",
       "      <td>204.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4621</th>\n",
       "      <td>4621</td>\n",
       "      <td>1.328271</td>\n",
       "      <td>click</td>\n",
       "      <td>66.0</td>\n",
       "      <td>173.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27814</th>\n",
       "      <td>27814</td>\n",
       "      <td>1.705400</td>\n",
       "      <td>website</td>\n",
       "      <td>111.0</td>\n",
       "      <td>761.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index   weights         word  ham_count  spam_count\n",
       "25599  25599 -2.424083       thanks      263.0        23.0\n",
       "25871  25871 -2.258781        title       69.0        69.0\n",
       "14905  14905 -1.573450         list      781.0       213.0\n",
       "18814  18814 -1.417257         plan      295.0       154.0\n",
       "2788    2788 -1.409560        board      954.0       105.0\n",
       "21790  21790 -1.376202        robot      294.0         5.0\n",
       "1160    1160 -1.361579      anybody      152.0        23.0\n",
       "21658  21658 -1.346509       ribbon       11.0         5.0\n",
       "26876  26876 -1.340602   university       47.0        13.0\n",
       "28263  28263 -1.327338        wrote      989.0        12.0\n",
       "18948  18948 -1.258351           pm      295.0       211.0\n",
       "19623  19623 -1.197419      problem      926.0       147.0\n",
       "19654  19654 -1.191490     produced       34.0         8.0\n",
       "27015  27015 -1.187651  unsubscribe      164.0        35.0\n",
       "13707  13707 -1.166726     interval      133.0         0.0\n",
       "4551    4551 -1.146243        class      163.0       106.0\n",
       "14248  14248 -1.141699         kind      197.0       104.0\n",
       "13470  13470 -1.074914    installed      107.0         0.0\n",
       "3322    3322 -1.068887        build      269.0        77.0\n",
       "21780  21780 -1.023426          rob     1384.0       252.0\n",
       "22630  22630 -1.002553       sensor      205.0         2.0\n",
       "19189  19189 -1.000832       posted       90.0         4.0\n",
       "16476  16476 -0.967062        movie       27.0        22.0\n",
       "1265    1265 -0.936182  appreciated      186.0        12.0\n",
       "15314  15314 -0.933805        mails       63.0        88.0\n",
       "21316  21316 -0.926039     research      205.0        59.0\n",
       "28165  28165 -0.924569      working      281.0        61.0\n",
       "15995  15995 -0.904932         mike       28.0         4.0\n",
       "23553  23553  0.923185         soft      484.0       550.0\n",
       "2          2  0.932944           ab     2248.0      1788.0\n",
       "19530  19530  1.011571       prices       22.0       159.0\n",
       "14634  14634  1.038776        legal       43.0        80.0\n",
       "17392  17392  1.138486       online      109.0       204.0\n",
       "4621    4621  1.328271        click       66.0       173.0\n",
       "27814  27814  1.705400      website      111.0       761.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_count=[]\n",
    "spam_count=[]\n",
    "word_list=[]\n",
    "temp=pd.DataFrame({'weights':logreg.coef_[0]}).reset_index().merge(pd.DataFrame({'index':vect.vocabulary_.values(),'word':vect.vocabulary_.keys()}),how='left',on='index')\n",
    "for index,(word,weights) in temp[temp['weights'].apply(lambda x:abs(x)>.9)][['word','weights']].iterrows():\n",
    "    counter=sample[sample['pretranslation'].apply(lambda x: word in x)]['class'].value_counts()\n",
    "    try:\n",
    "        ham_count.append(counter['ham'])\n",
    "    except:\n",
    "        ham_count.append(0)\n",
    "    try:\n",
    "        spam_count.append(counter['spam'])\n",
    "    except:\n",
    "        spam_count.append(0)\n",
    "    word_list.append(word)\n",
    "\n",
    "word_ham_spam_counter=pd.DataFrame({'word':word_list,'ham_count':ham_count,'spam_count':spam_count})\n",
    "\n",
    "temp=temp.merge(word_ham_spam_counter,how='left',on='word')\n",
    "temp[temp['ham_count']>5].sort_values('weights',ascending=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 model for each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words='english')\n",
    "X_train_e_dtm = vect.fit_transform(X_train_e)\n",
    "X_test_e_dtm= vect.transform(X_test_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_c=X_train_c.apply(lambda x: ''.join([word for word in x if cn_word.match(word)]))\n",
    "X_test_c=X_test_c.apply(lambda x: ''.join([word for word in x if cn_word.match(word)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'b', 'c', 'e', 'f', 'g', 'h', 'j', 'l', 'n', 'p', 'r', 'u', 'v', 'w', '介', '代', '令', '似', '位', '余', '作', '例', '便', '候', '值', '假', '僅', '儘', '儻', '先', '免', '全', '兩', '具', '兼', '出', '切', '前', '加', '反', '受', '句', '叮', '否', '呼', '哧', '唯', '唷', '問', '啪', '啷', '喔', '單', '喻', '噠', '固', '基', '外', '天', '夫', '奈', '妨', '妳', '始', '孰', '家', '少', '尚', '巧', '幸', '庶', '徒', '循', '怕', '恰', '悉', '惟', '慢', '成', '截', '抑', '拘', '接', '換', '料', '方', '旁', '旦', '曰', '期', '果', '根', '極', '樣', '次', '止', '正', '步', '死', '毋', '沒', '況', '消', '漫', '烏', '然', '特', '猶', '獨', '甚', '登', '直', '相', '省', '真', '眨', '眼', '知', '確', '種', '竟', '算', '簡', '結', '綜', '緊', '總', '繼', '罷', '肯', '舊', '般', '莫', '萬', '處', '裡', '見', '言', '設', '許', '話', '誠', '譬', '豈', '賊', '賴', '越', '身', '轉', '辦', '述', '逐', '通', '進', '道', '達', '遵', '部', '鄙', '針', '鑒', '開', '間', '關', '限', '難', '雲', '面', '願', '類', '餘', '首', '體', '齊'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(tokenizer=lambda txt:[*txt],stop_words=read_stopwords_list())\n",
    "X_train_c_dtm = vect.fit_transform(X_train_c)\n",
    "X_test_c_dtm= vect.transform(X_test_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_e_dtm, y_train_e)\n",
    "y_pred_e_class = nb.predict(X_test_e_dtm)\n",
    "\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_c_dtm, y_train_c)\n",
    "y_pred_c_class = nb.predict(X_test_c_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 model combined Multinomial Naive Bayes \n",
      "acc 0.9445789272518792\n",
      "f1 0.8807238826432684\n"
     ]
    }
   ],
   "source": [
    "print('2 model combined Multinomial Naive Bayes ')\n",
    "y_pred_combined=y_pred_e_class.tolist()+y_pred_c_class.tolist()\n",
    "y_actual_combined=pd.concat([y_test_e,y_test_c]).to_list()\n",
    "print('acc',metrics.accuracy_score(y_actual_combined, y_pred_combined))\n",
    "print('f1',f1_score(y_pred_combined,y_actual_combined,pos_label=\"spam\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "logreg.fit(X_train_e_dtm, y_train_e)\n",
    "y_pred_e_class = logreg.predict(X_test_e_dtm)\n",
    "# y_pred_prob = logreg.predict_proba(X_test_dtm)[:, 1]\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "logreg.fit(X_train_c_dtm, y_train_c)\n",
    "y_pred_c_class = logreg.predict(X_test_c_dtm)\n",
    "# y_pred_prob = logreg.predict_proba(X_test_dtm)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 model combined log regression \n",
      "acc 0.9649636896419926\n",
      "f1 0.924346629986245\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('2 model combined log regression ')\n",
    "y_pred_combined=y_pred_e_class.tolist()+y_pred_c_class.tolist()\n",
    "y_actual_combined=pd.concat([y_test_e,y_test_c]).to_list()\n",
    "print('acc',metrics.accuracy_score(y_actual_combined, y_pred_combined))\n",
    "print('f1',f1_score(y_pred_combined,y_actual_combined,pos_label=\"spam\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>weights</th>\n",
       "      <th>word</th>\n",
       "      <th>ham_count</th>\n",
       "      <th>spam_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4455</th>\n",
       "      <td>4455</td>\n",
       "      <td>-1.076214</td>\n",
       "      <td>题</td>\n",
       "      <td>1809.0</td>\n",
       "      <td>963.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3983</th>\n",
       "      <td>3983</td>\n",
       "      <td>-1.037339</td>\n",
       "      <td>较</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>855.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>466</td>\n",
       "      <td>-0.932342</td>\n",
       "      <td>历</td>\n",
       "      <td>683.0</td>\n",
       "      <td>514.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>833</td>\n",
       "      <td>-0.818275</td>\n",
       "      <td>太</td>\n",
       "      <td>1425.0</td>\n",
       "      <td>424.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4431</th>\n",
       "      <td>4431</td>\n",
       "      <td>0.869561</td>\n",
       "      <td>页</td>\n",
       "      <td>61.0</td>\n",
       "      <td>557.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>261</td>\n",
       "      <td>0.947042</td>\n",
       "      <td>免</td>\n",
       "      <td>239.0</td>\n",
       "      <td>1540.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3142</th>\n",
       "      <td>3142</td>\n",
       "      <td>0.980435</td>\n",
       "      <td>网</td>\n",
       "      <td>598.0</td>\n",
       "      <td>3106.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>728</td>\n",
       "      <td>0.985060</td>\n",
       "      <td>图</td>\n",
       "      <td>244.0</td>\n",
       "      <td>644.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3764</th>\n",
       "      <td>3764</td>\n",
       "      <td>1.031492</td>\n",
       "      <td>详</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1289.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4107</th>\n",
       "      <td>4107</td>\n",
       "      <td>1.128372</td>\n",
       "      <td>邮</td>\n",
       "      <td>140.0</td>\n",
       "      <td>2291.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index   weights word  ham_count  spam_count\n",
       "4455   4455 -1.076214    题     1809.0       963.0\n",
       "3983   3983 -1.037339    较     1001.0       855.0\n",
       "466     466 -0.932342    历      683.0       514.0\n",
       "833     833 -0.818275    太     1425.0       424.0\n",
       "4431   4431  0.869561    页       61.0       557.0\n",
       "261     261  0.947042    免      239.0      1540.0\n",
       "3142   3142  0.980435    网      598.0      3106.0\n",
       "728     728  0.985060    图      244.0       644.0\n",
       "3764   3764  1.031492    详      100.0      1289.0\n",
       "4107   4107  1.128372    邮      140.0      2291.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_count=[]\n",
    "spam_count=[]\n",
    "word_list=[]\n",
    "temp=pd.DataFrame({'weights':logreg.coef_[0]}).reset_index().merge(pd.DataFrame({'index':vect.vocabulary_.values(),'word':vect.vocabulary_.keys()}),how='left',on='index')\n",
    "for index,(word,weights) in temp[temp['weights'].apply(lambda x:abs(x)>.8)][['word','weights']].iterrows():\n",
    "    counter=sample[sample['pretranslation'].apply(lambda x: word in x)]['class'].value_counts()\n",
    "    try:\n",
    "        ham_count.append(counter['ham'])\n",
    "    except:\n",
    "        ham_count.append(0)\n",
    "    try:\n",
    "        spam_count.append(counter['spam'])\n",
    "    except:\n",
    "        spam_count.append(0)\n",
    "    word_list.append(word)\n",
    "\n",
    "word_ham_spam_counter=pd.DataFrame({'word':word_list,'ham_count':ham_count,'spam_count':spam_count})\n",
    "\n",
    "temp=temp.merge(word_ham_spam_counter,how='left',on='word')\n",
    "temp[temp['ham_count']>5].sort_values('weights',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>class</th>\n",
       "      <th>translated</th>\n",
       "      <th>clean_msg</th>\n",
       "      <th>pretranslation</th>\n",
       "      <th>chinese</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>trec06c/data/067/247</td>\n",
       "      <td>ham</td>\n",
       "      <td>[The following text is reprinted from the Girl...</td>\n",
       "      <td>following text reprinted girl discussion forum...</td>\n",
       "      <td>【 以下文字转载自 Girl 讨论区 】 发信人: psycho (风子【Nash/Selt...</td>\n",
       "      <td>以下文字转载自讨论区发信人风子信区标题赠美眉北大硕士白领美女真诚征婚发信人风子信区标题赠美眉...</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12327</th>\n",
       "      <td>trec06c/data/114/056</td>\n",
       "      <td>ham</td>\n",
       "      <td>It seems that you really have no experience. I...</td>\n",
       "      <td>seems really experience sure take time review ...</td>\n",
       "      <td>看来你的确没阅历没经验。 看人不准可以多审查些时间，可以请长辈或者朋友帮你看看。 朋友介绍的...</td>\n",
       "      <td>看来你的确没阅历没经验看人不准可以多审查些时间可以请长辈或者朋友帮你看看朋友介绍的一个男生认...</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16295</th>\n",
       "      <td>trec06c/data/148/261</td>\n",
       "      <td>ham</td>\n",
       "      <td>It was just after midnight when I came out of ...</td>\n",
       "      <td>midnight came cash box maybe last dance music ...</td>\n",
       "      <td>从钱柜出来，刚过零点。也许是最后那段舞曲太过疯狂，所以每个人的脸上都还残留着余兴未尽的暧昧。...</td>\n",
       "      <td>从钱柜出来刚过零点也许是最后那段舞曲太过疯狂所以每个人的脸上都还残留着余兴未尽的暧昧寒暄道别...</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9586</th>\n",
       "      <td>trec06c/data/087/260</td>\n",
       "      <td>ham</td>\n",
       "      <td>The probability of happiness may be lower. But...</td>\n",
       "      <td>probability happiness may lower learn see peop...</td>\n",
       "      <td>幸福的概率可能会低一点 但是如果从此学会看人和独立，也许幸福的概率会高一些 一切都在于自己 ...</td>\n",
       "      <td>幸福的概率可能会低一点但是如果从此学会看人和独立也许幸福的概率会高一些一切都在于自己我是因为...</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9659</th>\n",
       "      <td>trec06c/data/088/239</td>\n",
       "      <td>ham</td>\n",
       "      <td>It's obvious that you are the one with the pro...</td>\n",
       "      <td>obvious one problem want someone else want fin...</td>\n",
       "      <td>明明就是你有问题，你不要的人家，还不乐意人家自己寻找快乐么？ 看不出凭哪点你mm就该终生不嫁...</td>\n",
       "      <td>明明就是你有问题你不要的人家还不乐意人家自己寻找快乐么看不出凭哪点你就该终生不嫁一世苦等你回...</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23155</th>\n",
       "      <td>trec06c/data/212/237</td>\n",
       "      <td>spam</td>\n",
       "      <td>This is a letter in HTML format! -------------...</td>\n",
       "      <td>letter format mail system free download lifeti...</td>\n",
       "      <td>这是一封HTML格式信件！ -------------VolleyMail邮件系统-----...</td>\n",
       "      <td>这是一封格式信件邮件系统免费下载终身可用您好感谢您能在百忙之中抽出时间阅读此信函首先对冒昧地...</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23209</th>\n",
       "      <td>trec06c/data/213/200</td>\n",
       "      <td>spam</td>\n",
       "      <td>Socorro, the friend of Socorro and meditates w...</td>\n",
       "      <td>friend meditates crank case toothache related ...</td>\n",
       "      <td>Socorro, the friend of Socorro and meditates w...</td>\n",
       "      <td>模具估计大师寻求和作模具估价系统主要是根据塑料成品尺寸估算出塑料模具规格以及价格可进行模具估...</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23253</th>\n",
       "      <td>trec06c/data/214/020</td>\n",
       "      <td>spam</td>\n",
       "      <td>Countless businessmen have benefited from it. ...</td>\n",
       "      <td>countless businessmen benefited tool many ente...</td>\n",
       "      <td>无数商人从中获益 　　众多企业必备利刃 您有新产品却不知如何推广？您建了网站却没几个人访问？...</td>\n",
       "      <td>无数商人从中获益众多企业必备利刃您有新产品却不知如何推广您建了网站却没几个人访问您做了搜索引...</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32853</th>\n",
       "      <td>trec06p/data/070/200</td>\n",
       "      <td>spam</td>\n",
       "      <td>How to obtain high-quality overseas customers ...</td>\n",
       "      <td>obtain overseas customers orders obtain overse...</td>\n",
       "      <td>如何获取海外优质客户与订单 　 如何获取海外优质客户与订单 及国际商务谈判实战技巧强化训练 ...</td>\n",
       "      <td>如何获取海外优质客户与订单如何获取海外优质客户与订单及国际商务谈判实战技巧强化训练上海深圳准...</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38940</th>\n",
       "      <td>trec06p/data/124/020</td>\n",
       "      <td>spam</td>\n",
       "      <td>Supplier Management and Procurement Cost Reduc...</td>\n",
       "      <td>supplier management procurement cost reduction...</td>\n",
       "      <td>供应商管理及采购成本降低技巧 3月18-19日 采购外包管理及供应商业绩改进实务高级研修班 ...</td>\n",
       "      <td>供应商管理及采购成本降低技巧月日采购外包管理及供应商业绩改进实务高级研修班时间年月日深圳金百...</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>403 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     doc_id class  \\\n",
       "7608   trec06c/data/067/247   ham   \n",
       "12327  trec06c/data/114/056   ham   \n",
       "16295  trec06c/data/148/261   ham   \n",
       "9586   trec06c/data/087/260   ham   \n",
       "9659   trec06c/data/088/239   ham   \n",
       "...                     ...   ...   \n",
       "23155  trec06c/data/212/237  spam   \n",
       "23209  trec06c/data/213/200  spam   \n",
       "23253  trec06c/data/214/020  spam   \n",
       "32853  trec06p/data/070/200  spam   \n",
       "38940  trec06p/data/124/020  spam   \n",
       "\n",
       "                                              translated  \\\n",
       "7608   [The following text is reprinted from the Girl...   \n",
       "12327  It seems that you really have no experience. I...   \n",
       "16295  It was just after midnight when I came out of ...   \n",
       "9586   The probability of happiness may be lower. But...   \n",
       "9659   It's obvious that you are the one with the pro...   \n",
       "...                                                  ...   \n",
       "23155  This is a letter in HTML format! -------------...   \n",
       "23209  Socorro, the friend of Socorro and meditates w...   \n",
       "23253  Countless businessmen have benefited from it. ...   \n",
       "32853  How to obtain high-quality overseas customers ...   \n",
       "38940  Supplier Management and Procurement Cost Reduc...   \n",
       "\n",
       "                                               clean_msg  \\\n",
       "7608   following text reprinted girl discussion forum...   \n",
       "12327  seems really experience sure take time review ...   \n",
       "16295  midnight came cash box maybe last dance music ...   \n",
       "9586   probability happiness may lower learn see peop...   \n",
       "9659   obvious one problem want someone else want fin...   \n",
       "...                                                  ...   \n",
       "23155  letter format mail system free download lifeti...   \n",
       "23209  friend meditates crank case toothache related ...   \n",
       "23253  countless businessmen benefited tool many ente...   \n",
       "32853  obtain overseas customers orders obtain overse...   \n",
       "38940  supplier management procurement cost reduction...   \n",
       "\n",
       "                                          pretranslation  \\\n",
       "7608   【 以下文字转载自 Girl 讨论区 】 发信人: psycho (风子【Nash/Selt...   \n",
       "12327  看来你的确没阅历没经验。 看人不准可以多审查些时间，可以请长辈或者朋友帮你看看。 朋友介绍的...   \n",
       "16295  从钱柜出来，刚过零点。也许是最后那段舞曲太过疯狂，所以每个人的脸上都还残留着余兴未尽的暧昧。...   \n",
       "9586   幸福的概率可能会低一点 但是如果从此学会看人和独立，也许幸福的概率会高一些 一切都在于自己 ...   \n",
       "9659   明明就是你有问题，你不要的人家，还不乐意人家自己寻找快乐么？ 看不出凭哪点你mm就该终生不嫁...   \n",
       "...                                                  ...   \n",
       "23155  这是一封HTML格式信件！ -------------VolleyMail邮件系统-----...   \n",
       "23209  Socorro, the friend of Socorro and meditates w...   \n",
       "23253  无数商人从中获益 　　众多企业必备利刃 您有新产品却不知如何推广？您建了网站却没几个人访问？...   \n",
       "32853  如何获取海外优质客户与订单 　 如何获取海外优质客户与订单 及国际商务谈判实战技巧强化训练 ...   \n",
       "38940  供应商管理及采购成本降低技巧 3月18-19日 采购外包管理及供应商业绩改进实务高级研修班 ...   \n",
       "\n",
       "                                                 chinese lang  \n",
       "7608   以下文字转载自讨论区发信人风子信区标题赠美眉北大硕士白领美女真诚征婚发信人风子信区标题赠美眉...    c  \n",
       "12327  看来你的确没阅历没经验看人不准可以多审查些时间可以请长辈或者朋友帮你看看朋友介绍的一个男生认...    c  \n",
       "16295  从钱柜出来刚过零点也许是最后那段舞曲太过疯狂所以每个人的脸上都还残留着余兴未尽的暧昧寒暄道别...    c  \n",
       "9586   幸福的概率可能会低一点但是如果从此学会看人和独立也许幸福的概率会高一些一切都在于自己我是因为...    c  \n",
       "9659   明明就是你有问题你不要的人家还不乐意人家自己寻找快乐么看不出凭哪点你就该终生不嫁一世苦等你回...    c  \n",
       "...                                                  ...  ...  \n",
       "23155  这是一封格式信件邮件系统免费下载终身可用您好感谢您能在百忙之中抽出时间阅读此信函首先对冒昧地...    c  \n",
       "23209  模具估计大师寻求和作模具估价系统主要是根据塑料成品尺寸估算出塑料模具规格以及价格可进行模具估...    c  \n",
       "23253  无数商人从中获益众多企业必备利刃您有新产品却不知如何推广您建了网站却没几个人访问您做了搜索引...    c  \n",
       "32853  如何获取海外优质客户与订单如何获取海外优质客户与订单及国际商务谈判实战技巧强化训练上海深圳准...    c  \n",
       "38940  供应商管理及采购成本降低技巧月日采购外包管理及供应商业绩改进实务高级研修班时间年月日深圳金百...    c  \n",
       "\n",
       "[403 rows x 7 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[sample['pretranslation'].apply(lambda x: '寻' in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'如何获取海外优质客户与订单如何获取海外优质客户与订单及国际商务谈判实战技巧强化训练上海深圳准时开课主办单位华鹰企业管理咨询公司时间地点年月日上海兆安酒店时间地点年月日深圳金融培训中心费用元人包括培训费资料费两天午餐证书费以及上下午茶点等学员对象成长型出口企业的总经理海外营销部长国际贸易部经理区域市场经理主管海外业务员驻外代表以及预备外销员和其他对国际贸易感兴趣的人士课程背景中国很多非常有竞争力的企业因为不懂得如何开拓国际市场而失去了迅速做大做强的机会部分已经出口的企业也因为不懂得如何开拓国际市场而不得不依靠外贸企业间接出口但结果是产品出口了自己并没有享受到高额的利润并游离在国际市场的门外没有自己的海外客户始终受制于外贸企业有的企业已经直接出口产品但却没有找到最有利润的市场和客户仅仅学会了出口操作而没有达到出口的真正目的获取高额销售利润与此形成鲜明对比的是全球买家越来越青睐中国制造的产品纷纷开始从中国采购或者加大从中国采购的力度实践证明主动找上门来的买家比自己主动找去的买家成交率高倍以上平均首次成交所需时间只有后者的但中国的企业却不知道如何抓住这些机会如何能够让自己轻松地被海外客户找到因而坐失商机出口营销实战系列培训课程着重帮助解决中国出口商开拓国际市场的两个核心问题快速获取国际市场与买家信息和高效出口推广策略不仅准确地定位买家而且更能让买家轻松找到和优先联络自己您的七项收益准确定位目标市场和发现高利润市场迅速地找到您全球的潜在买家和合作伙伴发现竞争对手难以发现的客户独享高利润出口订单极大丰富客户数据库不断优化客户结构提高整体客户质量提高国际市场调查技能轻松获取高价值的市场信息掌握一套获取市场信息和客户情报的系统方法知己知彼百战不殆结识同行拓展人脉积累资源培训核心内容一外销启动前的准备人才方面的准备中国各类企业国际营销部门组织架构的设定及管理方式优劣对比硬件方面的准备软件方面的准备资料方面的准备其它方面的准备二掌握产品知识应该包含哪些关键内容产品知识测试题清楚自己产品的名称清楚自己产品的技术知识及卖点清楚自己产品成本构成及报价清楚相关联产品与行业的知识及名称三如何迅速了解行业宏观环境及掌握竞争对手信息行业国际市场宏观环境所包含的要素及获取办法购买现成的行业国际市场宏观环境报告的途径利用互联网手段查询制作简易行业国际市场宏观环境利用互联网查询和分析国内最主要的竞争对手利用互联网查询和分析国际市场最主要的竞争对手明确我们的竞争对手锁定和分析我们的竞争对手四如何开拓海外市场和寻找海外客户商机经常出没的地方选用哪种方式寻找这些商机更加适合自己如何抓住这些商机获取商机的其他一些途径买家分类和所遵循的标准关键买家信息所应包含的内容和获取的途径获得买家信息主要的途径和方法用什么办法来提高买家信息的准确性和完整性利用互联网寻找全球买家的一些方法五海外目标市场定向调研和管理海外目标市场的调研手段方法及关键所应包含的内容有哪些六海外市场定点调研及对海外客户评估国际区域市场划分原则和客户调研海外买家实力和信用简易评估方法客户信用调查辅助机构和手段利用互联网评估客户评估客户是否适合自己七中国企业自主品牌国际营销风险种类及控制方法八分析企业自身的优势劣势及制定各海外区域市场相应的进占战略分析对本企业的作用分析当中重点关注的要素如何来做分析九根据分析结果制订年度销售规划与政策并分解之各区域及个人十根据调研结果制定目标海外市场产品线策略十一根据调研结果制定目标海外市场价格策略十二根据调研结果制定目标海外市场渠道策略客户渠道的设定及管理渠道规划的基本原则各个时期所采用的渠道策略十三根据调研结果制定目标海外市场促销策略十四根据调研结果制定目标海外市场售后服务策略十五月度季度年度销售统计和分析十六海外营销部内部运作体系和管理海外营销部分权手册和审批权限海外营销部作业流程和相关规定外销业务员的奖励和处罚规定十七同海外客户沟通及获取信任的几个关键技巧同客户沟通的内容和基本要素沟通中的人性基础和重要的沟通策略获取海外客户信任的途径利用企业网站赢得客户信任通过电话沟通取得客户信任外销员个人专业素养取得客户信任客户信任的企业文化包括哪些要素利用企业形象取得客户信任企业形象有哪些作用客户欣赏怎样的企业形象有效的形象传播企业形象的塑造因素出口企业形象操作出口企业形象和本上销售企业形象的差异十八参加展览的标准和取得成功的关键因素选择展览的评估展览成功展览的标准和关键的成功因素展览组织的细节问题如何挑选展位展前如何邀请客户参展现场需要注意问题特装设计技巧怎样充分挖掘展览价值十九临门一脚争取客户最后落单展后如何争取成交机会如何开始接触海外客户商业信函的写作技巧联络海外客户各种方式的选择二十管理海外客户的询盘海外询盘的各种形式辨别和获取高质量的询盘辨别询盘的真假回复询盘的原则抓住真实有效的询盘机会分类管理好以往的询盘二十一获取订单的战略战术应用突破客户落单的最后几个障碍如何获得和对待海外大客户大客户有哪些特点如何获得大客户的青睐开发大客户当中常见的问题如何留住大客户二十二接待客户来访的注意事项如何体现品质保障能力规范运作所体现出来的公司实力二十三中国外销企业当前所面临的问题和挑战战略联盟的形成前提条件如何运作略联盟来实现企业的高速增长美的企业战略联盟的成功案例分析主讲导师徐老师国际贸易实战型专家高级讲师首批丹麦哥本哈根工程学院专业公费留学生北方交通大学工商管理硕士曾任中国机械进出口公司驻巴基斯坦首席代表丹麦通讯公司驻欧洲区市场助理八年国外生活工作经历原任广东美的企业集团广东美的厨房电器海外营销总监广东华帝海外事业部总经理年月因成功完成交钥匙工程合作项目受到巴基斯坦总统穆沙夫接见年作为广东经济代表团成员之一随同当时的广东省委书记李长春访问欧洲其中随同的正式代表有广东集团董事长李东生广东美的企业集团董事长何享健广东格兰仕企业集团董事长梁庆德金羚电器集团董事长陈立民广东省交通集团董事长游国经等广东前强企业董事长多次接受大经贸在越南等地专访印度尼西亚所建的品牌旗舰店在胡主席参观企业时得到高度赞扬现所操盘的企业海外年销售额过万美元江西财经大学国际学院客座教授广州华鹰企业管理咨询有限公司国际营销管理首席顾问自行设置和开发的课程中小企业如何顺利实现走出国门系列外贸出口营销管理实战类课程培训获得由广东省劳动和社会保障厅颁发的第五届广东省职业技能培训和技工教育教学成果奖组委会联络处联系电话联系传真联系人吴小姐陈先生报名回执我单位共人报名参加年月日在上海深圳举办的如何获取海外优质客户与订单及国际商务谈判实战技巧强化训练单位名称培训联系人联系电话联系传真移动电话电子邮箱参加人数人费用总计元参会人所任职务移动电话参会人所任职务移动电话参会人所任职务移动电话付款方式请选择打钩现金支票转帐'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[32853,'chinese']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translated dataset Random Forest\n",
      "acc 0.9592356687898089\n",
      "f1 0.9118457300275482\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Train the Random Forest classifier on the training data\n",
    "rf.fit(X_train_dtm, y_train)\n",
    "\n",
    "# Predict classes on the validation set\n",
    "y_pred_class = rf.predict(X_val_dtm)\n",
    "\n",
    "# Evaluate the Random Forest classifier\n",
    "print('translated dataset Random Forest')\n",
    "print('acc', accuracy_score(y_val, y_pred_class))\n",
    "print('f1', f1_score(y_val.to_list(), y_pred_class, pos_label=\"spam\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translated dataset XGBoost\n",
      "acc 0.9512738853503184\n",
      "f1 0.8982035928143713\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Define label mapping\n",
    "label_map = {'ham': 0, 'spam': 1}\n",
    "\n",
    "# Convert categorical labels to numerical labels\n",
    "y_train_mapped = y_train.map(label_map)\n",
    "y_val_mapped = y_val.map(label_map)\n",
    "\n",
    "# Create an XGBoost classifier\n",
    "xgb_classifier = xgb.XGBClassifier()\n",
    "\n",
    "# Train the XGBoost classifier on the training data\n",
    "xgb_classifier.fit(X_train_dtm, y_train_mapped)\n",
    "\n",
    "# Predict classes on the validation set\n",
    "y_pred_class = xgb_classifier.predict(X_val_dtm)\n",
    "\n",
    "# Evaluate the XGBoost classifier\n",
    "print('translated dataset XGBoost')\n",
    "print('acc', accuracy_score(y_val_mapped, y_pred_class))\n",
    "print('f1', f1_score(y_val_mapped.to_list(), y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
