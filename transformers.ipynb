{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())\n",
    "print(torch.cuda.current_device())\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizerFast, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "path = 'Data/splits'\n",
    "model_path = 'google-bert/bert-base-uncased'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path,tokenized=False):\n",
    "    if tokenized:\n",
    "        full_ds = load_dataset('arrow',data_files={\n",
    "            'train':path+'/tokenized/train_ds/train_ds.arrow',\n",
    "            'test':path+'/tokenized/test_ds/test_ds.arrow',\n",
    "            'val':path+'/tokenized/val_ds/val_ds.arrow'\n",
    "        })\n",
    "    else:\n",
    "        full_ds = load_dataset('arrow',data_files={\n",
    "            'train':path+'/train_ds/train_ds.arrow',\n",
    "            'test':path+'/test_ds/test_ds.arrow',\n",
    "            'validation':path+'/val_ds/val_ds.arrow'\n",
    "        })\n",
    "    return full_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Val-Test split, run only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(id,example,df):\n",
    "    text = example['processed']\n",
    "    label = example['class']\n",
    "    tokens = tokenizer.encode(text)\n",
    "    out = []\n",
    "    if len(tokens) <= 512:\n",
    "        out.append([tokens,label,id])\n",
    "        return out\n",
    "    else:\n",
    "        cls_token = tokens[0]\n",
    "        sep_token = tokens[-1]\n",
    "        tokens = tokens[1:-1] # remove CLS and SEP tokens\n",
    "        chunks = [tokens[i:i+500] for i in range(0,len(tokens),500)]\n",
    "        for c in chunks: # add back CLS and SEP tokens\n",
    "            c.insert(0,cls_token)\n",
    "            c.append(sep_token)\n",
    "            out.append([c,label,id])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    new_df = {'text':[],'label':[]}\n",
    "    ids = []\n",
    "    mapping = {\"ham\":0,\"spam\":1}\n",
    "    for i,row in tqdm(df.iterrows()):\n",
    "        output = preprocess_function(i,row,df)\n",
    "        for chunk,label,idx in output:\n",
    "            if len(chunk) > 512: print(\"ERROR\")\n",
    "            new_df['processed'].append(chunk)\n",
    "            new_df['class'].append(mapping[label])\n",
    "            ids.append(idx)\n",
    "    final_df = pd.DataFrame(new_df,index=ids)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"Data/full_df.pkl\")\n",
    "df = df.drop_duplicates(subset=['processed'])\n",
    "X = df['processed']\n",
    "y = df['class']\n",
    "X_tr, X_test, y_tr, y_test = train_test_split(X,y,test_size=0.2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tr,y_tr,test_size=0.2)\n",
    "train_df = pd.DataFrame({'processed':X_train,'class':y_train})\n",
    "val_df = pd.DataFrame({'processed':X_val,'class':y_val})\n",
    "test_df = pd.DataFrame({'processed':X_test,'class':y_test})\n",
    "train_df = preprocess(train_df)\n",
    "val_df = preprocess(val_df)\n",
    "test_df = preprocess(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_pandas(train_df,split='train')\n",
    "val_ds = Dataset.from_pandas(val_df,split='validation')\n",
    "test_ds = Dataset.from_pandas(test_df,split='test')\n",
    "train_ds.save_to_disk(path+'/train_ds')\n",
    "test_ds.save_to_disk(path+'/test_ds')\n",
    "val_ds.save_to_disk(path+'/val_ds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_data(path)\n",
    "train_ds = ds['train']\n",
    "test_ds = ds['test']\n",
    "val_ds = ds['validation']\n",
    "train_ds = train_ds.remove_columns(['__index_level_0__'])\n",
    "test_ds = test_ds.remove_columns(['__index_level_0__'])\n",
    "val_ds = val_ds.remove_columns(['__index_level_0__'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding of tokens and getting attention maps via BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_and_tokenize_function(example):\n",
    "    tokens = example['processed']\n",
    "    text = tokenizer.decode(tokens,skip_special_tokens=True)\n",
    "    return tokenizer(text,padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_ds = train_ds.map(decode_and_tokenize_function)\n",
    "tokenized_test_ds = test_ds.map(decode_and_tokenize_function)\n",
    "tokenized_val_ds = val_ds.map(decode_and_tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_ds.save_to_disk(path+'/tokenized/train_ds')\n",
    "tokenized_val_ds.save_to_disk(path+'/tokenized/val_ds')\n",
    "tokenized_test_ds.save_to_disk(path+'/tokenized/test_ds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Evaluation metrics, Data Collator and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds = load_data(path,tokenized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "accuracy = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "id2label = {\n",
    "    0:\"ham\",\n",
    "    1:\"spam\",\n",
    "}\n",
    "\n",
    "label2id = {\n",
    "    \"ham\":0,\n",
    "    \"spam\":1,\n",
    "}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model_checkpoints\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"val\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
