{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3080\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-13 15:55:30.732024: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-13 15:55:31.423195: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-13 15:55:32.770884: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name())\n",
    "    print(torch.cuda.current_device())\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizerFast, RobertaTokenizerFast, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer, pipeline\n",
    "import evaluate\n",
    "path = '/app/Data/'\n",
    "#model_path = 'google-bert/bert-base-uncased'\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "model_path = 'FacebookAI/roberta-base'\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrow_file_name = \"data-00000-of-00001\"\n",
    "def load_data(path,tokenized=False):\n",
    "    if tokenized:\n",
    "        full_ds = load_dataset('arrow',data_files={\n",
    "            'train_r':path+f'/tokenized/train_ds_r/{arrow_file_name}.arrow',\n",
    "            'train_e':path+f'/tokenized/train_ds_e/{arrow_file_name}.arrow',\n",
    "            'test':path+f'/tokenized/test_ds/{arrow_file_name}.arrow',\n",
    "            'val':path+f'/tokenized/val_ds/{arrow_file_name}.arrow'\n",
    "        })\n",
    "    else:\n",
    "        full_ds = load_dataset('arrow',data_files={\n",
    "            'train_r':path+f'/train_ds_r/{arrow_file_name}.arrow',\n",
    "            'train_e':path+f'/train_ds_e/{arrow_file_name}.arrow',\n",
    "            'test':path+f'/test_ds/{arrow_file_name}.arrow',\n",
    "            'validation':path+f'/val_ds/{arrow_file_name}.arrow'\n",
    "        })\n",
    "    return full_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Evaluation metrics, Data Collator and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds = load_data(path,tokenized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "accuracy = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "id2label = {\n",
    "    0:\"ham\",\n",
    "    1:\"spam\",\n",
    "}\n",
    "\n",
    "label2id = {\n",
    "    \"ham\":0,\n",
    "    \"spam\":1,\n",
    "}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "eval_class_weights = torch.tensor([0.5, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, eval_class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # eval_class_weights should be a tensor of shape [num_labels].\n",
    "        self.eval_class_weights = eval_class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Call the original compute_loss\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        labels = inputs.get('labels')\n",
    "        if self.eval_class_weights is not None and self.args.do_eval:\n",
    "            # Only modify loss computation during evaluation\n",
    "            eval_class_weights_device = self.eval_class_weights.to(model.device)\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(weight=eval_class_weights_device)\n",
    "            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        else:\n",
    "            # Default behavior\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "to_torch_compile = True # Change to True only if on linux\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/app/model_checkpoints\",\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=2e-5,\n",
    "    optim=\"adamw_bnb_8bit\", # Supposedly better than AdamW while using less space\n",
    "    gradient_accumulation_steps=2, # Increases Effective Batch Size for smoother gradient descent\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    push_to_hub=False,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    logging_steps=100,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    save_total_limit=5,\n",
    "    load_best_model_at_end=True,\n",
    "    torch_compile = to_torch_compile\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train_r\"],\n",
    "    eval_dataset=tokenized_ds[\"val\"],\n",
    "    eval_class_weights=eval_class_weights,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6928' max='6928' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6928/6928 1:28:30, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.278200</td>\n",
       "      <td>0.182699</td>\n",
       "      <td>0.926129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.171000</td>\n",
       "      <td>0.141325</td>\n",
       "      <td>0.960073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.139000</td>\n",
       "      <td>0.207691</td>\n",
       "      <td>0.952747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.109400</td>\n",
       "      <td>0.110401</td>\n",
       "      <td>0.974847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.130100</td>\n",
       "      <td>0.154370</td>\n",
       "      <td>0.963614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.117500</td>\n",
       "      <td>0.097782</td>\n",
       "      <td>0.975214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.096500</td>\n",
       "      <td>0.117731</td>\n",
       "      <td>0.974969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.078600</td>\n",
       "      <td>0.132617</td>\n",
       "      <td>0.975092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.088300</td>\n",
       "      <td>0.084679</td>\n",
       "      <td>0.980952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.129337</td>\n",
       "      <td>0.978022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.058900</td>\n",
       "      <td>0.083657</td>\n",
       "      <td>0.982784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.093300</td>\n",
       "      <td>0.070338</td>\n",
       "      <td>0.984982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.070600</td>\n",
       "      <td>0.102002</td>\n",
       "      <td>0.981807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.096100</td>\n",
       "      <td>0.074571</td>\n",
       "      <td>0.983028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.058600</td>\n",
       "      <td>0.101426</td>\n",
       "      <td>0.981197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.069000</td>\n",
       "      <td>0.076578</td>\n",
       "      <td>0.983761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.061100</td>\n",
       "      <td>0.096189</td>\n",
       "      <td>0.983272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.029800</td>\n",
       "      <td>0.099861</td>\n",
       "      <td>0.983883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.027800</td>\n",
       "      <td>0.091290</td>\n",
       "      <td>0.983761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.036100</td>\n",
       "      <td>0.085797</td>\n",
       "      <td>0.985470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.028900</td>\n",
       "      <td>0.121476</td>\n",
       "      <td>0.983150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.045100</td>\n",
       "      <td>0.096978</td>\n",
       "      <td>0.984737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.045100</td>\n",
       "      <td>0.075149</td>\n",
       "      <td>0.986081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.098200</td>\n",
       "      <td>0.985348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.028100</td>\n",
       "      <td>0.128788</td>\n",
       "      <td>0.983150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.030200</td>\n",
       "      <td>0.094601</td>\n",
       "      <td>0.985348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.050700</td>\n",
       "      <td>0.073879</td>\n",
       "      <td>0.985714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.032200</td>\n",
       "      <td>0.095493</td>\n",
       "      <td>0.984982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>0.091039</td>\n",
       "      <td>0.984127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>0.104437</td>\n",
       "      <td>0.985714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.062800</td>\n",
       "      <td>0.068723</td>\n",
       "      <td>0.984615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.037600</td>\n",
       "      <td>0.084252</td>\n",
       "      <td>0.984615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.043600</td>\n",
       "      <td>0.093988</td>\n",
       "      <td>0.984615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.038900</td>\n",
       "      <td>0.072362</td>\n",
       "      <td>0.985836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>0.097777</td>\n",
       "      <td>0.984493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>0.090125</td>\n",
       "      <td>0.986325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.105733</td>\n",
       "      <td>0.986081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.092432</td>\n",
       "      <td>0.986447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.107856</td>\n",
       "      <td>0.986325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>0.091288</td>\n",
       "      <td>0.986447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>0.087145</td>\n",
       "      <td>0.986935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>0.094024</td>\n",
       "      <td>0.985104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>0.084449</td>\n",
       "      <td>0.987179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>0.097218</td>\n",
       "      <td>0.985836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.081424</td>\n",
       "      <td>0.987424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.104022</td>\n",
       "      <td>0.985836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.019100</td>\n",
       "      <td>0.094561</td>\n",
       "      <td>0.987302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>0.089568</td>\n",
       "      <td>0.986447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.015700</td>\n",
       "      <td>0.103827</td>\n",
       "      <td>0.987057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>0.087540</td>\n",
       "      <td>0.986813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>0.087620</td>\n",
       "      <td>0.986203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.097865</td>\n",
       "      <td>0.987668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.107734</td>\n",
       "      <td>0.986813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>0.098569</td>\n",
       "      <td>0.987302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.098217</td>\n",
       "      <td>0.987057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.110679</td>\n",
       "      <td>0.987057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.093395</td>\n",
       "      <td>0.987912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.096603</td>\n",
       "      <td>0.987057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>0.091907</td>\n",
       "      <td>0.988034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.095059</td>\n",
       "      <td>0.988278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.096760</td>\n",
       "      <td>0.987668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.093251</td>\n",
       "      <td>0.988156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.089777</td>\n",
       "      <td>0.988278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.095628</td>\n",
       "      <td>0.988523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.097233</td>\n",
       "      <td>0.988523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.096943</td>\n",
       "      <td>0.988645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.103642</td>\n",
       "      <td>0.988400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.099717</td>\n",
       "      <td>0.988523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.100478</td>\n",
       "      <td>0.988400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6928, training_loss=0.03994914622034089, metrics={'train_runtime': 5352.8309, 'train_samples_per_second': 20.708, 'train_steps_per_second': 1.294, 'total_flos': 2.68121676520512e+16, 'train_loss': 0.03994914622034089, 'epoch': 4.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"/app/models/test_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "to_torch_compile = True # Change to True only if on linux\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/app/model_checkpoints\",\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=2e-5,\n",
    "    optim=\"adamw_bnb_8bit\", # Supposedly better than AdamW while using less space\n",
    "    gradient_accumulation_steps=2, # Increases Effective Batch Size for smoother gradient descent\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    push_to_hub=False,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    logging_steps=100,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    save_total_limit=5,\n",
    "    load_best_model_at_end=True,\n",
    "    torch_compile = to_torch_compile\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train_e\"],\n",
    "    eval_dataset=tokenized_ds[\"val\"],\n",
    "    eval_class_weights=eval_class_weights,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3876' max='3876' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3876/3876 49:45, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.325600</td>\n",
       "      <td>0.256904</td>\n",
       "      <td>0.913065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.199200</td>\n",
       "      <td>0.185128</td>\n",
       "      <td>0.956410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.161200</td>\n",
       "      <td>0.178501</td>\n",
       "      <td>0.956532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.128700</td>\n",
       "      <td>0.134404</td>\n",
       "      <td>0.964713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.124100</td>\n",
       "      <td>0.142743</td>\n",
       "      <td>0.965079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.106600</td>\n",
       "      <td>0.138370</td>\n",
       "      <td>0.967033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.140800</td>\n",
       "      <td>0.169818</td>\n",
       "      <td>0.943101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.143600</td>\n",
       "      <td>0.100224</td>\n",
       "      <td>0.968498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.103900</td>\n",
       "      <td>0.131701</td>\n",
       "      <td>0.974115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>0.101501</td>\n",
       "      <td>0.976557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.092100</td>\n",
       "      <td>0.119066</td>\n",
       "      <td>0.978144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.067100</td>\n",
       "      <td>0.111972</td>\n",
       "      <td>0.978022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.067100</td>\n",
       "      <td>0.096196</td>\n",
       "      <td>0.980952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.074700</td>\n",
       "      <td>0.082656</td>\n",
       "      <td>0.980342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.076300</td>\n",
       "      <td>0.110390</td>\n",
       "      <td>0.978022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.087112</td>\n",
       "      <td>0.982051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.053200</td>\n",
       "      <td>0.101082</td>\n",
       "      <td>0.978388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.072700</td>\n",
       "      <td>0.091428</td>\n",
       "      <td>0.980220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.083241</td>\n",
       "      <td>0.979976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.046200</td>\n",
       "      <td>0.075050</td>\n",
       "      <td>0.984127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.032300</td>\n",
       "      <td>0.080813</td>\n",
       "      <td>0.982295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.038700</td>\n",
       "      <td>0.090908</td>\n",
       "      <td>0.983394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.041800</td>\n",
       "      <td>0.105136</td>\n",
       "      <td>0.981441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.040700</td>\n",
       "      <td>0.099269</td>\n",
       "      <td>0.980708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.021800</td>\n",
       "      <td>0.093954</td>\n",
       "      <td>0.982051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.034400</td>\n",
       "      <td>0.080173</td>\n",
       "      <td>0.984982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.076965</td>\n",
       "      <td>0.985470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>0.981563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>0.121243</td>\n",
       "      <td>0.981929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.092372</td>\n",
       "      <td>0.984737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>0.092078</td>\n",
       "      <td>0.984127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.012700</td>\n",
       "      <td>0.113714</td>\n",
       "      <td>0.983394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>0.114257</td>\n",
       "      <td>0.982906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.026700</td>\n",
       "      <td>0.101961</td>\n",
       "      <td>0.984005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>0.085500</td>\n",
       "      <td>0.985592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>0.090223</td>\n",
       "      <td>0.985348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.095892</td>\n",
       "      <td>0.984493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.091099</td>\n",
       "      <td>0.985592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"/app/models/test_e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds = load_data(path,tokenized=True)\n",
    "tokenized_test_ds = tokenized_ds['test']\n",
    "classifier = pipeline('text-classification',model='/app/models/test_e', device=torch.cuda.current_device())\n",
    "def decode_tokens(example):\n",
    "    tokens = example['text']\n",
    "    # label_map = {0:\"ham\",1:\"spam\"}\n",
    "    text = tokenizer.decode(tokens,skip_special_tokens=True)\n",
    "    # label = label_map[example['label']]\n",
    "    return {'text':text}\n",
    "\n",
    "tokenized_test_ds = tokenized_test_ds.map(decode_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import evaluator\n",
    "task_evaluator = evaluator('text-classification')\n",
    "results = task_evaluator.compute(\n",
    "    model_or_pipeline=classifier,\n",
    "    data=tokenized_test_ds,\n",
    "    metric=evaluate.combine(['accuracy','recall','precision','f1']),\n",
    "    label_mapping=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9881040892193309,\n",
       " 'recall': 0.9762845849802372,\n",
       " 'precision': 0.9762845849802372,\n",
       " 'f1': 0.9762845849802372,\n",
       " 'total_time_in_seconds': 46.33630791600001,\n",
       " 'samples_per_second': 87.08074038429608,\n",
       " 'latency_in_seconds': 0.011483595518215615}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9866171003717472, 'recall': 0.9901185770750988, 'precision': 0.9579349904397706, 'f1': 0.9737609329446064, 'total_time_in_seconds': 39.899076257999695, 'samples_per_second': 101.13016085656844, 'latency_in_seconds': 0.009888246904089144}\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline('text-classification',model='/app/models/test_r', device=torch.cuda.current_device())\n",
    "from evaluate import evaluator\n",
    "task_evaluator = evaluator('text-classification')\n",
    "results = task_evaluator.compute(\n",
    "    model_or_pipeline=classifier,\n",
    "    data=tokenized_test_ds,\n",
    "    metric=evaluate.combine(['accuracy','recall','precision','f1']),\n",
    "    label_mapping=label2id,\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9866171003717472,\n",
       " 'recall': 0.9901185770750988,\n",
       " 'precision': 0.9579349904397706,\n",
       " 'f1': 0.9737609329446064,\n",
       " 'total_time_in_seconds': 39.899076257999695,\n",
       " 'samples_per_second': 101.13016085656844,\n",
       " 'latency_in_seconds': 0.009888246904089144}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
