{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 2060\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())\n",
    "print(torch.cuda.current_device())\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizerFast, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "path = 'Data/splits'\n",
    "model_path = 'google-bert/bert-base-uncased'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Val-Test split, run only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(id,example,df):\n",
    "    text = example['processed']\n",
    "    label = example['class']\n",
    "    tokens = tokenizer.encode(text)\n",
    "    out = []\n",
    "    if len(tokens) <= 512:\n",
    "        out.append([tokens,label,id])\n",
    "        return out\n",
    "    else:\n",
    "        cls_token = tokens[0]\n",
    "        sep_token = tokens[-1]\n",
    "        tokens = tokens[1:-1] # remove CLS and SEP tokens\n",
    "        chunks = [tokens[i:i+510] for i in range(0,len(tokens),510)]\n",
    "        for c in chunks: # add back CLS and SEP tokens\n",
    "            c.insert(0,cls_token)\n",
    "            c.append(sep_token)\n",
    "            out.append([c,label,id])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    new_df = {'processed':[],'class':[]}\n",
    "    ids = []\n",
    "    mapping = {\"ham\":0,\"spam\":1}\n",
    "    for i,row in tqdm(df.iterrows()):\n",
    "        output = preprocess_function(i,row,df)\n",
    "        for chunk,label,idx in output:\n",
    "            new_df['processed'].append(chunk)\n",
    "            new_df['class'].append(mapping[label])\n",
    "            ids.append(idx)\n",
    "    final_df = pd.DataFrame(new_df,index=ids)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26078it [02:07, 204.78it/s]\n",
      "6520it [00:35, 183.66it/s]\n",
      "8150it [00:38, 210.52it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle(\"Data/full_df.pkl\")\n",
    "df = df.drop_duplicates(subset=['processed'])\n",
    "X = df['processed']\n",
    "y = df['class']\n",
    "X_tr, X_test, y_tr, y_test = train_test_split(X,y,test_size=0.2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tr,y_tr,test_size=0.2)\n",
    "train_df = pd.DataFrame({'processed':X_train,'class':y_train})\n",
    "val_df = pd.DataFrame({'processed':X_val,'class':y_val})\n",
    "test_df = pd.DataFrame({'processed':X_test,'class':y_test})\n",
    "train_df = preprocess(train_df)\n",
    "val_df = preprocess(val_df)\n",
    "test_df = preprocess(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 39081/39081 [00:00<00:00, 479530.98 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 11308/11308 [00:00<00:00, 559557.23 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 9382/9382 [00:00<00:00, 255789.81 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds = Dataset.from_pandas(train_df,split='train')\n",
    "val_ds = Dataset.from_pandas(val_df,split='validation')\n",
    "test_ds = Dataset.from_pandas(test_df,split='test')\n",
    "train_ds.save_to_disk(path+'/train_ds')\n",
    "test_ds.save_to_disk(path+'/test_ds')\n",
    "val_ds.save_to_disk(path+'/val_ds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path,tokenized=False):\n",
    "    if tokenized:\n",
    "        full_ds = load_dataset('arrow',data_files={\n",
    "            'train':path+'/tokenized/train_ds/train_ds.arrow',\n",
    "            'test':path+'/tokenized/test_ds/test_ds.arrow',\n",
    "            'val':path+'/tokenized/val_ds/val_ds.arrow'\n",
    "        })\n",
    "    else:\n",
    "        full_ds = load_dataset('arrow',data_files={\n",
    "            'train':path+'/train_ds/train_ds.arrow',\n",
    "            'test':path+'/test_ds/test_ds.arrow',\n",
    "            'validation':path+'/val_ds/val_ds.arrow'\n",
    "        })\n",
    "    return full_ds\n",
    "\n",
    "ds = load_data(path)\n",
    "train_ds = ds['train']\n",
    "test_ds = ds['test']\n",
    "val_ds = ds['validation']\n",
    "train_ds = train_ds.remove_columns(['__index_level_0__'])\n",
    "test_ds = test_ds.remove_columns(['__index_level_0__'])\n",
    "val_ds = val_ds.remove_columns(['__index_level_0__'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding of tokens and getting attention maps via BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_and_tokenize_function(example):\n",
    "    tokens = example['processed']\n",
    "    text = tokenizer.decode(tokens)\n",
    "    return tokenizer(text,padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 39081/39081 [02:40<00:00, 244.20 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_ds = train_ds.map(decode_and_tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/11308 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 11308/11308 [00:41<00:00, 273.37 examples/s]\n",
      "Map: 100%|██████████| 9382/9382 [00:35<00:00, 263.04 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_test_ds = test_ds.map(decode_and_tokenize_function)\n",
    "tokenized_val_ds = val_ds.map(decode_and_tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 39081/39081 [00:00<00:00, 103684.48 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 9382/9382 [00:00<00:00, 321347.75 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 11308/11308 [00:00<00:00, 153947.89 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_ds.save_to_disk(path+'/tokenized/train_ds')\n",
    "tokenized_val_ds.save_to_disk(path+'/tokenized/val_ds')\n",
    "tokenized_test_ds.save_to_disk(path+'/tokenized/test_ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 39081 examples [00:00, 205496.79 examples/s]\n",
      "Generating test split: 11308 examples [00:00, 125610.62 examples/s]\n",
      "Generating val split: 9382 examples [00:00, 134823.12 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_ds = load_data(path,tokenized=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Evaluation metrics, Data Collator and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 839kB/s]\n",
      "c:\\Users\\Darren Choo\\Documents\\DSA4266_Grp2\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Darren Choo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "accuracy = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "id2label = {\n",
    "    0:\"ham\",\n",
    "    1:\"spam\",\n",
    "}\n",
    "\n",
    "label2id = {\n",
    "    \"ham\":0,\n",
    "    \"spam\":1,\n",
    "}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model_checkpoints\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"val\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
