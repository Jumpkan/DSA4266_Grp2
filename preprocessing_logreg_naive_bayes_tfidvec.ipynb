{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import enchant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>contains_img</th>\n",
       "      <th>email_from</th>\n",
       "      <th>email_to</th>\n",
       "      <th>message</th>\n",
       "      <th>subject</th>\n",
       "      <th>subpath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>False</td>\n",
       "      <td>\"Stella Lowry\" &lt;rookcuduq@yahoo.com&gt;</td>\n",
       "      <td>\"Brian\" &lt;bernice@groucho.cs.psu.edu&gt;</td>\n",
       "      <td>\\n                                 ...</td>\n",
       "      <td>re[12]:</td>\n",
       "      <td>../data/000/001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>False</td>\n",
       "      <td>\"Walter\" &lt;trwmpca@downtowncumberland.com&gt;</td>\n",
       "      <td>&lt;arline@groucho.cs.psu.edu&gt;</td>\n",
       "      <td>Academic Qualifications available from prestig...</td>\n",
       "      <td>Take a moment to explore this.</td>\n",
       "      <td>../data/000/002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>False</td>\n",
       "      <td>Scott Schwartz &lt;schwartz@groucho.cs.psu.edu&gt;</td>\n",
       "      <td>9fans &lt;plan9-fans@cs.psu.edu&gt;</td>\n",
       "      <td>Greetings all.  This is to verify your subscri...</td>\n",
       "      <td>Greetings</td>\n",
       "      <td>../data/000/003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>False</td>\n",
       "      <td>\"Mr Jailyn Koepke\" &lt;kiflsbizc@attheworld.com&gt;</td>\n",
       "      <td>melvin@groucho.cs.psu.edu</td>\n",
       "      <td>try chauncey may conferred the luscious not co...</td>\n",
       "      <td>LOANS @ 3.17% (27 term)</td>\n",
       "      <td>../data/000/004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ham</td>\n",
       "      <td>False</td>\n",
       "      <td>Scott Schwartz &lt;schwartz@groucho.cs.psu.edu&gt;</td>\n",
       "      <td>9fans &lt;plan9-fans@cs.psu.edu&gt;</td>\n",
       "      <td>It's quiet.  Too quiet.  Well, how about a str...</td>\n",
       "      <td>who wants to start?</td>\n",
       "      <td>../data/000/005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37817</th>\n",
       "      <td>spam</td>\n",
       "      <td>False</td>\n",
       "      <td>\"Christy Armstrong\" &lt;contact@10-million-hits.com&gt;</td>\n",
       "      <td>webmastr@kukui.ifa.hawaii.edu</td>\n",
       "      <td>Great News Expec ted!\\n\\nInfinex Ventures Inc....</td>\n",
       "      <td>RE: Watcher daily news</td>\n",
       "      <td>../data/126/017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37818</th>\n",
       "      <td>spam</td>\n",
       "      <td>False</td>\n",
       "      <td>\"Bette Ortiz\" &lt;confirm@1000land.com&gt;</td>\n",
       "      <td>webmastr@KUKUI.IFA.HAWAII.EDU</td>\n",
       "      <td>The OIL sector is going crazy. This is our wee...</td>\n",
       "      <td>RE: Number 1 market Picks</td>\n",
       "      <td>../data/126/018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37819</th>\n",
       "      <td>spam</td>\n",
       "      <td>False</td>\n",
       "      <td>\"Bettye Rowland\" &lt;baitommyf@0451.com&gt;</td>\n",
       "      <td>webmastr@KUKUI.IFA.HAWAII.EDU</td>\n",
       "      <td>http://vdtobj.docscan.info/?23759301\\n\\nSuffer...</td>\n",
       "      <td>All products for your health!</td>\n",
       "      <td>../data/126/019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37820</th>\n",
       "      <td>spam</td>\n",
       "      <td>False</td>\n",
       "      <td>\"Daisy\" &lt;nluwwj@avk-pl.nl&gt;</td>\n",
       "      <td>&lt;webmastr@KUKUI.IFA.HAWAII.EDU&gt;</td>\n",
       "      <td>U N I V E R S I T Y  D I P L O M A S\\n\\nDo you...</td>\n",
       "      <td>Its here, now.</td>\n",
       "      <td>../data/126/020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37821</th>\n",
       "      <td>spam</td>\n",
       "      <td>False</td>\n",
       "      <td>\"Piper\" &lt;decvkvwzkqhyj@omm.com&gt;</td>\n",
       "      <td>webmastr@KUKUI.IFA.HAWAII.EDU</td>\n",
       "      <td>but moat , coverall be cytochemistry be planel...</td>\n",
       "      <td>This diet plan is over the roof</td>\n",
       "      <td>../data/126/021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36837 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      class contains_img                                         email_from  \\\n",
       "1      spam        False               \"Stella Lowry\" <rookcuduq@yahoo.com>   \n",
       "2      spam        False          \"Walter\" <trwmpca@downtowncumberland.com>   \n",
       "3       ham        False       Scott Schwartz <schwartz@groucho.cs.psu.edu>   \n",
       "4      spam        False      \"Mr Jailyn Koepke\" <kiflsbizc@attheworld.com>   \n",
       "5       ham        False       Scott Schwartz <schwartz@groucho.cs.psu.edu>   \n",
       "...     ...          ...                                                ...   \n",
       "37817  spam        False  \"Christy Armstrong\" <contact@10-million-hits.com>   \n",
       "37818  spam        False               \"Bette Ortiz\" <confirm@1000land.com>   \n",
       "37819  spam        False              \"Bettye Rowland\" <baitommyf@0451.com>   \n",
       "37820  spam        False                         \"Daisy\" <nluwwj@avk-pl.nl>   \n",
       "37821  spam        False                    \"Piper\" <decvkvwzkqhyj@omm.com>   \n",
       "\n",
       "                                   email_to  \\\n",
       "1      \"Brian\" <bernice@groucho.cs.psu.edu>   \n",
       "2               <arline@groucho.cs.psu.edu>   \n",
       "3             9fans <plan9-fans@cs.psu.edu>   \n",
       "4                 melvin@groucho.cs.psu.edu   \n",
       "5             9fans <plan9-fans@cs.psu.edu>   \n",
       "...                                     ...   \n",
       "37817         webmastr@kukui.ifa.hawaii.edu   \n",
       "37818         webmastr@KUKUI.IFA.HAWAII.EDU   \n",
       "37819         webmastr@KUKUI.IFA.HAWAII.EDU   \n",
       "37820       <webmastr@KUKUI.IFA.HAWAII.EDU>   \n",
       "37821         webmastr@KUKUI.IFA.HAWAII.EDU   \n",
       "\n",
       "                                                 message  \\\n",
       "1                 \\n                                 ...   \n",
       "2      Academic Qualifications available from prestig...   \n",
       "3      Greetings all.  This is to verify your subscri...   \n",
       "4      try chauncey may conferred the luscious not co...   \n",
       "5      It's quiet.  Too quiet.  Well, how about a str...   \n",
       "...                                                  ...   \n",
       "37817  Great News Expec ted!\\n\\nInfinex Ventures Inc....   \n",
       "37818  The OIL sector is going crazy. This is our wee...   \n",
       "37819  http://vdtobj.docscan.info/?23759301\\n\\nSuffer...   \n",
       "37820  U N I V E R S I T Y  D I P L O M A S\\n\\nDo you...   \n",
       "37821  but moat , coverall be cytochemistry be planel...   \n",
       "\n",
       "                               subject          subpath  \n",
       "1                              re[12]:  ../data/000/001  \n",
       "2       Take a moment to explore this.  ../data/000/002  \n",
       "3                            Greetings  ../data/000/003  \n",
       "4              LOANS @ 3.17% (27 term)  ../data/000/004  \n",
       "5                  who wants to start?  ../data/000/005  \n",
       "...                                ...              ...  \n",
       "37817           RE: Watcher daily news  ../data/126/017  \n",
       "37818        RE: Number 1 market Picks  ../data/126/018  \n",
       "37819    All products for your health!  ../data/126/019  \n",
       "37820                   Its here, now.  ../data/126/020  \n",
       "37821  This diet plan is over the roof  ../data/126/021  \n",
       "\n",
       "[36837 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_pickle('en_emails_raw.pkl')\n",
    "# data=data[['class','message']].drop_duplicates()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['message'].apply(lambda x: 'zipping my lean muscle mass' in x)\n",
    "# temp=data[data['message'].apply(lambda x: 'zipping my lean muscle mass' in x)]\n",
    "# temp.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_regex = re.compile('[^a-zA-Z ]')\n",
    "d = enchant.Dict(\"en_US\") \n",
    "STOPWORDS = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enPreprocess(string):\n",
    "    string=string.replace('\\n',' ') #remove newline char\n",
    "    string=en_regex.sub('',string) #removes non alphabets\n",
    "    string=string.split()\n",
    "    string=[word.lower() for word in string if len(word)>1]\n",
    "    string=' '.join([word for word in string if d.check(word) and (word not in STOPWORDS)]) # split the string into list and check each word if it is in the english dictionary and longer than 1 alphabet\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_msg']=data['message'].apply(lambda x: enPreprocess(x))\n",
    "data.to_pickle('lowercase words.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('price', 12433), ('adobe', 10354), ('info', 8632), ('company', 8444), ('gold', 7831), ('professional', 7224), ('windows', 6447), ('us', 6095), ('office', 5812), ('campaign', 5395), ('website', 5178), ('stock', 5011), ('save', 4988), ('add', 4987), ('china', 4957), ('one', 4805), ('hi', 4796), ('reviews', 4743), ('retail', 4684), ('ms', 4655), ('get', 4615), ('cart', 4574), ('rating', 4544), ('please', 4404), ('pro', 4274), ('corp', 3979), ('may', 3963), ('effects', 3847), ('energy', 3759), ('product', 3719), ('news', 3649), ('new', 3618), ('weight', 3607), ('like', 3560), ('inc', 3425), ('offer', 3380), ('day', 3319), ('see', 3232), ('body', 3171), ('even', 3160), ('opt', 3074), ('also', 2986), ('back', 2976), ('best', 2893), ('development', 2880), ('email', 2874), ('oil', 2870), ('great', 2818), ('side', 2806), ('today', 2794)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "words = data[data['class']=='spam']['clean_msg'].apply(lambda x: [word.lower() for word in x.split()])\n",
    "ham_words = Counter()\n",
    "\n",
    "for msg in words:\n",
    "    ham_words.update(msg)\n",
    "    \n",
    "print(ham_words.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_pickle('lowercase words.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clean_msg</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>john office prepared poster announcing awards planning poster display entered building easel informed left overnight would likely stolen mind please recommend location poster mounted wall posters students others receiving external organizations location enter building suggestions please director foundation government grants university hall fax john office prepared poster announcing awards planning poster display entered building easel informed left overnight would likely stolen mind please recommend location poster mounted wall posters students others receiving external organizations location enter building suggestions please director foundation government grants university hall fax</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>please remove address mailing list dr warren jr associate dean academic programs university college engineering hall box fl email forwarding service brought let become free minutes</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>please remove list also department biology institute technology avenue phone fax email</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>please remove list also stop blind coping every removal request sent ms coordinator international admissions university south sc phone fax web address email forwarding service brought let become free minutes</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>please remove list interested messages</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>luxury watches buy frank muller omega tag full gold men fast delivery lowest prices world worldwide shipping visit shop</th>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great news ted ventures inc price already started climb one well last marketing campaign well overview aggressive energetic boasts dynamic diversified portfolio operations across north eye international expansion grounded natural resource exploration also offers investors access exciting new developments sector booming international real estate market market based experience tenacious research techniques razor sharp analytical skills allow us leverage opportunities emerging markets developing technologies identifying opportunities earliest stages allows us accelerate business development fully realize true potential maximizing overall profitability turn enhancing shareholder value current press release announces joint venture option agreement extension may ventures inc board directors please announce company granted extension days fulfill contractual obligations joint venture option agreement dated island yew group mining claims yew claims located island region long history mining dating back several high grade copper gold mined area geology yew claims found</th>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suffering pain depression heartburn well help verified collected one licensed online store great choice wonderful give relief operative support fast shipping secure processing complete confidentiality store verified approved visa</th>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>back legal back legal best weight loss aid world ever known organic herb grows china used humans many centuries currently used safely millions people stock right company product stock lose pounds fighting without hungry spending hours gym adverse side effects proprietary combination caffeine aspirin optimally designed maximize weight loss zero side effects pounds literally fall body least pounds per week swear info opt campaign website see</th>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>special offer adobe video collection adobe premiere professional adobe effects professional adobe audition adobe encore info ms windows pro ms office pro info adobe ms windows pro ms office pro adobe acrobat professional info bestsellers office professional edition rating reviews retail price save price add cart windows professional rating reviews retail price save price add cart adobe cs rating reviews retail price save price add cart special offer adobe video premiere effects audition adobe encore info ms windows proms office pro info adobe ms windows proms office acrobat professional info bestsellers office professional edition rating reviews retail price save price add cart windows professional rating reviews retail price save price add cart adobe cs rating reviews retail price save price add cart</th>\n",
       "      <td>734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15421 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    class\n",
       "clean_msg                                                \n",
       "john office prepared poster announcing awards p...      1\n",
       "please remove address mailing list dr warren jr...      1\n",
       "please remove list also department biology inst...      1\n",
       "please remove list also stop blind coping every...      1\n",
       "please remove list interested messages                  1\n",
       "...                                                   ...\n",
       "luxury watches buy frank muller omega tag full ...    204\n",
       "great news ted ventures inc price already start...    228\n",
       "suffering pain depression heartburn well help v...    256\n",
       "back legal back legal best weight loss aid worl...    346\n",
       "special offer adobe video collection adobe prem...    734\n",
       "\n",
       "[15421 rows x 1 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('clean_msg').count().sort_values('class')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[['clean_msg','class']]#.drop_duplicates()\n",
    "data=data[data['clean_msg']!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = data['clean_msg']\n",
    "y = data['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1,test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words='english')\n",
    "vect.fit(X_train)\n",
    "\n",
    "# learn training data vocabulary, then use it to create a document-term matrix\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "\n",
    "# equivalently: combine fit and transform into a single step\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "\n",
    "X_test_dtm= vect.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english dataset Multinomial Naive Bayes \n",
      "0.9621\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "\n",
    "print('english dataset Multinomial Naive Bayes ')\n",
    "print(round(metrics.accuracy_score(y_test, y_pred_class),4))\n",
    "\n",
    "# metrics.confusion_matrix(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english dataset tfidtransfomer \n",
      "0.9164\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_transformer.fit(X_train_dtm)\n",
    "tfidf_transformer.transform(X_train_dtm)\n",
    "pipe = Pipeline([('bow', CountVectorizer()), \n",
    "                 ('tfid', TfidfTransformer()),  \n",
    "                 ('model', MultinomialNB())])\n",
    "pipe.fit(X_train, y_train)\n",
    "print('english dataset tfidtransfomer ')\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(round(metrics.accuracy_score(y_test, y_pred),4))\n",
    "# metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english dataset log regression \n",
      "0.9731\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# train the model using X_train_dtm\n",
    "logreg.fit(X_train_dtm, y_train)\n",
    "y_pred_class = logreg.predict(X_test_dtm)\n",
    "\n",
    "# calculate predicted probabilities for X_test_dtm (well calibrated)\n",
    "y_pred_prob = logreg.predict_proba(X_test_dtm)[:, 1]\n",
    "print('english dataset log regression ')\n",
    "print(round(metrics.accuracy_score(y_test, y_pred_class),4))\n",
    "# print(metrics.confusion_matrix(y_test, y_pred_class))\n",
    "# print(metrics.roc_auc_score(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>讲的是孔子后人的故事。一个老领导回到家乡，跟儿子感情不和，跟贪财的孙子孔为本和睦。\\n老领导...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>尊敬的贵公司(财务/经理)负责人您好！  \\n        我是深圳金海实业有限公司（广州...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>贵公司负责人(经理/财务）您好： \\n    深圳市华龙公司受多家公司委托向外低点代开部分增...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>这是一封HTML格式信件！\\n\\n-----------------------------...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>\\nTO：贵公司经理、财务\\n\\n　　\\n  　您好！　　　　　　\\n　  深圳市春洋贸易有...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64587</th>\n",
       "      <td>ham</td>\n",
       "      <td>清华毕业生在美国失踪续：母子已被证实死亡 \\n-----------------------...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64588</th>\n",
       "      <td>ham</td>\n",
       "      <td>她长得很小巧，五官清秀，笑起来很可爱，有点调皮，常捣蛋，是个半大不小的孩子。\\n  她平时作...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64593</th>\n",
       "      <td>ham</td>\n",
       "      <td>你似乎在转换话题\\n首先，成绩好，推研地把握就大，这个基本上是废话\\n每个人都知道\\n但是推...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64594</th>\n",
       "      <td>spam</td>\n",
       "      <td>ccert，您好：\\n    大多数人靠打工拿工资，用自己的血汗去成就老板的事业，用自己的辛...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64609</th>\n",
       "      <td>spam</td>\n",
       "      <td>你需要成人用品吗？http://www.91911.net\\n就要你爱神成人用品商城会满足你...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25306 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      class                                            message\n",
       "1       ham  讲的是孔子后人的故事。一个老领导回到家乡，跟儿子感情不和，跟贪财的孙子孔为本和睦。\\n老领导...\n",
       "2      spam  尊敬的贵公司(财务/经理)负责人您好！  \\n        我是深圳金海实业有限公司（广州...\n",
       "3      spam  贵公司负责人(经理/财务）您好： \\n    深圳市华龙公司受多家公司委托向外低点代开部分增...\n",
       "4      spam  这是一封HTML格式信件！\\n\\n-----------------------------...\n",
       "5      spam  \\nTO：贵公司经理、财务\\n\\n　　\\n  　您好！　　　　　　\\n　  深圳市春洋贸易有...\n",
       "...     ...                                                ...\n",
       "64587   ham  清华毕业生在美国失踪续：母子已被证实死亡 \\n-----------------------...\n",
       "64588   ham  她长得很小巧，五官清秀，笑起来很可爱，有点调皮，常捣蛋，是个半大不小的孩子。\\n  她平时作...\n",
       "64593   ham  你似乎在转换话题\\n首先，成绩好，推研地把握就大，这个基本上是废话\\n每个人都知道\\n但是推...\n",
       "64594  spam  ccert，您好：\\n    大多数人靠打工拿工资，用自己的血汗去成就老板的事业，用自己的辛...\n",
       "64609  spam  你需要成人用品吗？http://www.91911.net\\n就要你爱神成人用品商城会满足你...\n",
       "\n",
       "[25306 rows x 2 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_pickle('ch_emails_raw.pkl')\n",
    "data=data[['class','message']].drop_duplicates()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.groupby('clean_msg').count().sort_values('class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "punc = \"\"\"！？｡。＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.- / : 　, !@#$%^&*()_-=[]{ }\\|/.,><;:'=+?\" \"' \"\"\"\n",
    "def cnPreprocess(string):\n",
    "    string=string.lower().replace('\\n',' ') #remove newline char\n",
    "    string=[i for i in string]\n",
    "    # string=[word.lower() for word in string if len(word)>1]\n",
    "    string=''.join([word for word in string if word not in 'abcdefghijklmnopqrstuvwxyz '+punc+'0123456789']) # split the string into list and check each word if it is in the english dictionary and longer than 1 alphabet\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_msg']=data['message'].apply(lambda x: cnPreprocess(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[data['clean_msg']!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = data['clean_msg']\n",
    "y = data['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1,test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\forgo\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(tokenizer=lambda txt:[*txt])\n",
    "vect.fit(X_train)\n",
    "\n",
    "# learn training data vocabulary, then use it to create a document-term matrix\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "\n",
    "# equivalently: combine fit and transform into a single step\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "\n",
    "X_test_dtm= vect.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chinese dataset Multinomial Naive Bayes \n",
      "0.9173\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "\n",
    "print('chinese dataset Multinomial Naive Bayes ')\n",
    "print(round(metrics.accuracy_score(y_test, y_pred_class),4))\n",
    "\n",
    "# metrics.confusion_matrix(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chinese dataset tfidtransfomer \n",
      "0.8348\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_transformer.fit(X_train_dtm)\n",
    "tfidf_transformer.transform(X_train_dtm)\n",
    "pipe = Pipeline([('bow', CountVectorizer()), \n",
    "                 ('tfid', TfidfTransformer()),  \n",
    "                 ('model', MultinomialNB())])\n",
    "pipe.fit(X_train, y_train)\n",
    "print('chinese dataset tfidtransfomer ')\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(round(metrics.accuracy_score(y_test, y_pred),4))\n",
    "# metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chinese dataset log regression \n",
      "0.9841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\forgo\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# train the model using X_train_dtm\n",
    "logreg.fit(X_train_dtm, y_train)\n",
    "y_pred_class = logreg.predict(X_test_dtm)\n",
    "\n",
    "# calculate predicted probabilities for X_test_dtm (well calibrated)\n",
    "y_pred_prob = logreg.predict_proba(X_test_dtm)[:, 1]\n",
    "print('chinese dataset log regression ')\n",
    "print(round(metrics.accuracy_score(y_test, y_pred_class),4))\n",
    "# print(metrics.confusion_matrix(y_test, y_pred_class))\n",
    "# print(metrics.roc_auc_score(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
